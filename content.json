{"meta":{"title":"Essviv","subtitle":"Dare to be different","description":null,"author":"Essviv","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-03-08T08:38:11.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"孙义威 名称 信息 学历 研究生/中国科学院大学 工作年限 3年 手机号码 18867102100 Email patrick_sun@qq.com 技术博客 这里 Github 这里 工作经历1. 流量平台(2014.08 - 2017.01)项目简介流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 我的贡献流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 技术细节流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 （这里可以加一张架构演变示意图） 2. 基础设施服务化（2016.04 - 2016.12）项目简介流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 我的贡献流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 短信服务 号段服务 存储服务 充值服务 营销模板服务 （这里少一张架构图） 技术细节流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 3. 流量自营平台（2015.07 - 2016.12)项目简介流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 我的贡献流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 技术细节流量平台主要是为了服务各省公司日常的流量运营活动，我在此项目负责了哪些工作，分别在哪些地方做得出色/和别人不一样/成长快，这个项目中，我最困难的问题是什么，我采取了什么措施，最后结果如何。这个项目中，我最自豪的技术细节是什么，为什么，实施前和实施后的数据对比如何，同事和领导对此的反应如何。 其他项目(在校期间) 嫦娥三号制图与仿真系统(2013.3 - 2013.5)，中国科学院， 北京 高分辨率遥感卫星影像几何精校正(2011.9 - 2012.9)，中国科学院， 北京 影像和IMU组合精密导航定位系统(2011.7 - 2012.7)，中国科学院， 北京 SAR影像目标解译(2010.7 - 2011.7)，同济大学， 上海 技术文章完整的技术文章可在GitHub上找到，这里仅列出一些例子 AQS框架源码解析 代理模式 Netty服务端的启动 CountDownLatch, Semaphore, CyclicBarrier源码解析 技能清单 JAVA基础扎实，熟悉io, 多线程, 集合等基础框架 理解缓存、消息队列的原理，并掌握常见框架的使用(redis,memcached, RabbitMQ, ActiveMQ) 深入了解spring框架，阅读过框架源码，对IoC, AOP及相应的实现机制有深刻的理解 注重团队合作，能够快速地适应团队要求，对代码质量有很高的追求 荣誉与证书 2008年： CET-6 2010年： 国家奖学金 2016年： 2017年： 杭研之星"}],"posts":[{"title":"Nodejs - globals","slug":"前端/nodejs/Nodejs.globals","date":"2017-03-21T00:58:00.000Z","updated":"2017-03-21T03:07:47.000Z","comments":true,"path":"2017/03/21/前端/nodejs/Nodejs.globals/","link":"","permalink":"http://yoursite.com/2017/03/21/前端/nodejs/Nodejs.globals/","excerpt":"","text":"全局对象这些全局对象在所有模块中均可以访问. 这里有的对象并不是全局对象，而模块域的对象，当遇到这种情况时，会特别指出. 这里列出的对象是node.js特有的对象. 在js语言中还有许多语言本身定义的全局对象，这些全局对象也可以被访问. _dirname 返回: String 该属性返回当前模块的路径名称，它的值与path.dirname(__filename)的值相等. 事实上，该属性是模块私有的属性，而不是全局属性. 例如： 在/Users/mjr目录下运行node example.js会得到以下结果 1234console.log(__dirname);// Prints: /Users/mjrconsole.log(path.dirname(__filename));// Prints: /Users/mjr __filename 返回: String 该属性返回当前模块的文件名称，返回的名称是经过解析后的绝对路径. 对于主程序而言，该属性的值不一定与命令行中使用的文件名相等. 可以通过__dirname属性获取当前模块的路径名称. 事实上，该属性是模块私有的属性，而不是全局属性. 例如， 在/Users/mjr目录下运行node example.js会得到以下结果： 1234console.log(__filename);// Prints: /Users/mjr/example.jsconsole.log(__dirname);// Prints: /Users/mjr 假设有两个模块a和b， 其中a依赖于b， 它们的文件结构如下： /Users/mjr/app/a.js /Users/mjr/app/node_modules/b/b.js 当在b.js中访问__filename时，会得到/Users/mjr/app/nodemodules/b/b.js; 当在a.js中访问\\_filename时，会得到/Users/mjr/app/a.js. clearImmediate(immediaObject)clearImmediate()方法的讨论可参见”timers”章节 clearInterval(intervalObject)clearInterval()方法的讨论可参见”timers”章节 clearTimeout(timeoutObject)clearTimeout()方法的讨论可参见”timers”章节 console 返回: Object 该对象可用于将信息输出到stdin和stderr对象中，具体可参见”console”章节. exports该属性指向module.exports，是访问module.exports的”快捷方式”， 可以参见”module文档”来了解什么时候应该使用exports， 什么时候应该使用module.exports. 事实上，exports属性是模块私有的对象，而不是全局属性. global 返回: Object 全局的命名空间对象 在浏览器环境中，顶级的作用域就是全局作用域. 这意味着，如果在浏览器环境中，在全局作用域中调用var something会在全局作用域中定义相应的变量. 但是在node.js中并不一样. 顶级的作用域不再是全局作用域. 如果在模块中定义var something操作，那么该变量的作用域是整个模块，而不是全局作用域. module 返回: Object 该属性是对当前模块的引用. module.exports属性被用来定义要导出模块的变量和方法，其它模块可以通过require()方法进行引用. 事实上, module属性是模块私有的对象，它并不是全局属性. 关于module属性的更多内容，可以参见”module”章节. process 返回: Object 该属性表示当前的进程对象. 具体可以参见”process”章节. require() 类型: Function 该方法用于引入其它的模块. 具体的内容的可以参见”module”章节. 事实上，require属性的作用域仅限于模块内部，它并不是全局属性. require.cache 类型: Object 当在模块内部引入了其它模块时，这些模块会被缓存到这个属性中. 如果删除了该属性中的某个key值， 那么当再次载这个模块时，会重新载入该模块. 备注： 重新载入并不适用于插件，重新载入插件会抛出Error. require.resolve()使用require()方法的查找机制来查找某个模块的具体位置 ，但不会真实地加载该模块，只会输出该模块的路径名称. setImmediate(callback[, …args])setImmediate()方法的具体讨论见”timers”章节 setInterval(callback, delay[, …args])setInterval()方法的具体讨论见”timers”章节 setTimeout(callback, delay[, …args])setTimeout()方法的具体讨论见”timers”章节 参考文献 官方文献","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"globals","slug":"globals","permalink":"http://yoursite.com/tags/globals/"}]},{"title":"nodejs - Path","slug":"前端/nodejs/Nodejs.path","date":"2017-03-20T12:25:00.000Z","updated":"2017-03-20T15:40:38.000Z","comments":true,"path":"2017/03/20/前端/nodejs/Nodejs.path/","link":"","permalink":"http://yoursite.com/2017/03/20/前端/nodejs/Nodejs.path/","excerpt":"","text":"Pathpath模块提供了操作文件和目录名称的工具函数. 它可以通过以下方法来访问： 1const path = require(\"path\"); windows vs. POSIXpath模块默认执行的操作取决于应用程序所运行的操作系统. 具体来讲，当运行在windows系统中时，path模块会使用windows风格的路径名称. 例如： 当使用path.basename()方法时,在windows系统和posix系统中，c:\\temp\\myfile.html路径会得到不同的结果： 1234567//on POSIXpath.basename('C:\\\\temp\\\\myfile.html');// Returns: 'C:\\\\temp\\\\myfile.html'//on windowspath.basename('C:\\\\temp\\\\myfile.html');// Returns: 'myfile.html' 如果想得到一致的运行结果，对于windows风格的路径可以使用path.win32: 12path.win32.basename('C:\\\\temp\\\\myfile.html');// Returns: 'myfile.html' 而对于posix风格的文件路径，可以使用path.posix： 12path.posix.basename('/tmp/myfile.html');// Returns: 'myfile.html' path.basename(path[, ext]) path: String ext: String 可选的文件后缀名 返回: String path.basename()方法返回路径的最后一个部分，其功能与unix系统中的basename命令类似，例如： 12345path.basename('/foo/bar/baz/asdf/quux.html')// Returns: 'quux.html'path.basename('/foo/bar/baz/asdf/quux.html', '.html')// Returns: 'quux' 如果path参数不是string类型，或者ext不是string类型，那么会抛出TypeError异常 path.delimiter 返回: String 该属性记录了平台特定的文件分隔符. windows平台： ； POSIX: ： 例如： 12345678910111213//on posixconsole.log(process.env.PATH)// Prints: '/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin'process.env.PATH.split(path.delimiter)// Returns: ['/usr/bin', '/bin', '/usr/sbin', '/sbin', '/usr/local/bin']//on windowsconsole.log(process.env.PATH)// Prints: 'C:\\Windows\\system32;C:\\Windows;C:\\Program Files\\node\\'process.env.PATH.split(path.delimiter)// Returns: ['C:\\\\Windows\\\\system32', 'C:\\\\Windows', 'C:\\\\Program Files\\\\node\\\\'] path.dirname(path) path: String 返回: String path.dirname()方法返回path参数指定路径的目录名称，其功能与unix系统中dirname的功能类似，例如： 12path.dirname('/foo/bar/baz/asdf/quux')// Returns: '/foo/bar/baz/asdf' 如果path参数不是字符串类型，那么将抛出TypeError异常. path.extname(path) path: String 返回：String path.extname()方法返回path参数指定的路径名的扩展部分，从最后一次出现“.”开始到路径的最后. 如果在路径的最后一个部分中没有出现”.”， 或者”.”出现在了basename的第一个字母(path.basename()返回的值称为basename)，那么将返回了空字符串. 例如： 1234567891011121314path.extname('index.html')// Returns: '.html'path.extname('index.coffee.md')// Returns: '.md'path.extname('index.')// Returns: '.'path.extname('index')// Returns: ''path.extname('.index')// Returns: '' 如果path参数不是字符串类型，那么将抛出TypeError异常. path.format(pathObject) pathObject: Object dir: String root: String base: String name: String ext: String 返回: String path.format()方法将对象解析成路径名称， 这个方法和path.parse()方法的作用相反. pathObject对象的多个属性之间有优先顺序： 当提供了pathObject.dir属性之后， pathObject.root属性将会被忽略 当提供了pathObject.base属性之后，pathObject.name和pathObject.ext属性将会被忽略 例如： 1234567891011121314151617181920212223242526272829303132333435// 在POSIX系统中// If `dir`, `root` and `base` are provided,// `$&#123;dir&#125;$&#123;path.sep&#125;$&#123;base&#125;`// will be returned. `root` is ignored.path.format(&#123; root: '/ignored', dir: '/home/user/dir', base: 'file.txt'&#125;);// Returns: '/home/user/dir/file.txt'// `root` will be used if `dir` is not specified.// If only `root` is provided or `dir` is equal to `root` then the// platform separator will not be included. `ext` will be ignored.path.format(&#123; root: '/', base: 'file.txt', ext: 'ignored'&#125;);// Returns: '/file.txt'// `name` + `ext` will be used if `base` is not specified.path.format(&#123; root: '/', name: 'file', ext: '.txt'&#125;);// Returns: '/file.txt'//在window系统中path.format(&#123; dir : \"C:\\\\path\\\\dir\", base : \"file.txt\"&#125;);// Returns: 'C:\\\\path\\\\dir\\\\file.txt' path.isAbsolute(path)该方法用于判断path参数指定的路径名是否是绝对路径. 如果给定的path参数为空字符串，那么该方法将返回false. 例如： 1234567891011121314//在POSIX系统中path.isAbsolute('/foo/bar') // truepath.isAbsolute('/baz/..') // truepath.isAbsolute('qux/') // falsepath.isAbsolute('.') // false//在windows系统中path.isAbsolute('//server') // truepath.isAbsolute('\\\\\\\\server') // truepath.isAbsolute('C:/foo/..') // truepath.isAbsolute('C:\\\\foo\\\\..') // truepath.isAbsolute('bar\\\\baz') // falsepath.isAbsolute('bar/baz') // falsepath.isAbsolute('.') // false 如果path参数不是字符串类型，那么将抛出TypeError异常. path.join([…paths]) …paths: String 路径片断序列 path.join()方法将所有的路径片断连接到一起，以path.sep作为分隔符，并返回归一化后的路径名称. 空的路径片断将会被忽略，如果连接后的路径名为空字符串，那么将返回当前工作路径. 例如： 12345path.join('/foo', 'bar', 'baz/asdf', 'quux', '..')// Returns: '/foo/bar/baz/asdf'path.join('foo', &#123;&#125;, 'bar')// throws TypeError: Arguments to path.join must be strings 如果路径片断参数中含有不是字符串类型的参数，那么将抛出TypeError异常. path.normalize(path) path: String path.normalize()方法将路径名进行归一化操作，去掉路径中的”.”和”..”.如果路径中连续含有多个路径分隔符，那么它们会被单个分隔符替换. 路径尾部的分隔符会被保留. 如果path参数为空字符串，那么该方法将返回当前工作路径. 例如： 1234567//on POSIXpath.normalize('/foo/bar//baz/asdf/quux/..')// Returns: '/foo/bar/baz/asdf'//on windowspath.normalize('C:\\\\temp\\\\\\\\foo\\\\bar\\\\..\\\\');// Returns: 'C:\\\\temp\\\\foo\\\\' 如果path参数不是字符串类型，那么将抛出TypeError异常. path.parse(path) path: String 返回： Object path.parse()方法会解析路径名称，并将其作为对象返回，该对象中包含路径的所有信息. 返回的对象中包含有以下信息： root: String dir: String base: String name: String ext: String 12345678910111213141516171819202122232425262728293031323334353637//on POSIXpath.parse('/home/user/dir/file.txt')// Returns:// &#123;// root : \"/\",// dir : \"/home/user/dir\",// base : \"file.txt\",// ext : \".txt\",// name : \"file\"// &#125;┌─────────────────────┬────────────┐│ dir │ base │├──────┬ ├──────┬─────┤│ root │ │ name │ ext │\" / home/user/dir / file .txt \"└──────┴──────────────┴──────┴─────┘(all spaces in the \"\" line should be ignored -- they are purely for formatting)//on windowspath.parse('C:\\\\path\\\\dir\\\\file.txt')// Returns:// &#123;// root : \"C:\\\\\",// dir : \"C:\\\\path\\\\dir\",// base : \"file.txt\",// ext : \".txt\",// name : \"file\"// &#125;┌─────────────────────┬────────────┐│ dir │ base │├──────┬ ├──────┬─────┤│ root │ │ name │ ext │\" C:\\ path\\dir \\ file .txt \"└──────┴──────────────┴──────┴─────┘(all spaces in the \"\" line should be ignored -- they are purely for formatting) 如果path参数不是字符串类型，那么将抛出TypeError异常. path.posix 返回: Object path.posix提供了访问POSIX系统特定实现的方法，通过这个对象，可以调用POSIX系统特定的实现. path.relative(fromPath, toPath) fromPath: String toPath: String 返回: String path.relative()方法返回从fromPath到toPath的相对路径， 可以认为是fromPath + returnValue = toPath. 如果fromPath和toPath解析到了同一个路径，那么将返回空字符串. 如果fromPath和toPath参数为空，那么将以当前路径来替代. 例如： 1234567//on POSIXpath.relative('/data/orandea/test/aaa', '/data/orandea/impl/bbb')// Returns: '../../impl/bbb'//on windowspath.relative('C:\\\\orandea\\\\test\\\\aaa', 'C:\\\\orandea\\\\impl\\\\bbb')// Returns: '..\\\\..\\\\impl\\\\bbb' 如果fromPath或toPath参数的类型不是字符串类型，那么将抛出TypeError异常 path.resolve([…paths]) …paths: String 路径名片断序列 返回： String 解析后的路径镇中 path.resolve()方法将一系列的路径名或路径片断解析成一个绝对路径. 给定的路径序列将会被从右至左一一处理，每个路径都被加到路径的前端，直到解析到一个绝对路径为止，空的路径片断将会被忽略. 例如，/foo, /bar, baz这样的序列，调用path.resolve()方法会得到/bar/baz. 如果路径序列已经被解析完， 仍然没有解析到绝对路径，那么将会相对于当前路径给出绝对路径.另外，路径将会被path.normalize()方法归一化后，并移除尾部的斜线后返回. 如果没有传递任何路径片断，那么path.resolve()将会返回当前工作路径的绝对路径. 如： 123456789path.resolve('/foo/bar', './baz')// Returns: '/foo/bar/baz'path.resolve('/foo/bar', '/tmp/file/')// Returns: '/tmp/file'path.resolve('wwwroot', 'static_files/png/', '../gif/image.gif')// if the current working directory is /home/myself/node,// this returns '/home/myself/node/wwwroot/static_files/gif/image.gif' 如果paths序列中含有不是字符串类型的路径，那么将会抛出TypeError异常. path.sep 返回: String 该属性返回系统特定的路径分隔符， 在POSIX系统中，该属性为/ 在windows系统中，该属性为\\ 1234567//on POSIX'foo/bar/baz'.split(path.sep)// Returns: ['foo', 'bar', 'baz']//on windows'foo\\\\bar\\\\baz'.split(path.sep)// Returns: ['foo', 'bar', 'baz'] path.win32 返回： Object path.win32属性提供了访问windows系统特定实现的方法，通过这个属性可以访问windows系统中特有的实现. 备注： 在windows系统中，不论是斜杠(/)还是反斜杠()都会被当成是路径分隔符，但是，只有反斜杠才会被当成是属性值返回(path.sep) 参考文献 官方文档","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"path","slug":"path","permalink":"http://yoursite.com/tags/path/"}]},{"title":"nodejs - filesystem","slug":"前端/nodejs/Nodejs.filesystem","date":"2017-03-17T07:21:00.000Z","updated":"2017-03-20T06:21:25.000Z","comments":true,"path":"2017/03/17/前端/nodejs/Nodejs.filesystem/","link":"","permalink":"http://yoursite.com/2017/03/17/前端/nodejs/Nodejs.filesystem/","excerpt":"","text":"文件系统node.js的文件IO模块提供了对标准POSIX函数的包装. 如果要使用这个模块，可以调用require(“fs”)方法. 该模块中所有的方法都有同步和异步两个版本. 异步版本的方法都会带有回调方法作为最后一个参数，当回调方法被回调时，传回的参数数量及类型取决于具体的方法，但第一个参数都是异常对象. 如果操作正常执行，那么该参数被设置为null或者undefined.如果使用同步版本的方法时发生了错误，那么将直接抛出异常，调用同步方法的应用可以通过try/catch机制来捕获异常. 以下是同一个方法的同步版本和异步版本的调用示例： 12345678910111213//异步版本const fs = require('fs');fs.unlink('/tmp/hello', (err) =&gt; &#123; if (err) throw err; console.log('successfully deleted /tmp/hello');&#125;);//同步版本const fs = require('fs');fs.unlinkSync('/tmp/hello');console.log('successfully deleted /tmp/hello') 另外，值得注意的是，在使用异步版本的方法时，方法的执行顺序是没有保证的，出现在后面的代码有可能会先被执行: 12345678fs.rename('/tmp/hello', '/tmp/world', (err) =&gt; &#123; if (err) throw err; console.log('renamed complete');&#125;);fs.stat('/tmp/world', (err, stats) =&gt; &#123; if (err) throw err; console.log(`stats: $&#123;JSON.stringify(stats)&#125;`);&#125;); 在上面的代码中，fs.stat()方法有可能会先于fs.rename()方法执行. 正确的方法应该是在fs.rename()方法的回调函数中操作: 1234567fs.rename('/tmp/hello', '/tmp/world', (err) =&gt; &#123; if (err) throw err; fs.stat('/tmp/world', (err, stats) =&gt; &#123; if (err) throw err; console.log(`stats: $&#123;JSON.stringify(stats)&#125;`); &#125;);&#125;); 在负载较高的应用中，强烈建议使用异步版本的方法. 同步版本的方法在执行完成之前，会导致整个进程阻塞，这意味着所有的连接都会被挂起. 在方法参数中，可以使用相对路径来指定文件名称，但是需要注意的是，如果使用相对路径的话，它是相对于process.cwd()而言的. 大部分异步版本的方法都允许省略回调参数，这种情况下node.js会提供默认的回调方法，这个方法只会在出现错误时重新抛出异常. 如果要追踪原始的异常堆栈，可以在环境变量中设置NODE_DEBUG = fs. Buffer APIfs模块中的方法同时支持使用Buffer类型或String类型的参数来指定文件名. 前者主要用于那些允许文件名不是utf-8格式的系统中. 对于大部分的场景而言，不需要使用Buffer类型的文件名，因为node.js会自动将文件名转换成utf-8格式的字符串类型. 备注： 在某些系统中(如NTFS和HFS+)，文件名始终都是utf-8格式的. 在这些系统中以非utf-8格式的buffer类型来指定文件名，系统将无法正常工作. fs.FSWatcher调用fs.watch()方法时返回的对象类型为fs.FSWatcher. 在调用watch()方法时提供的callback参数将会在监听在目录发生变化时被调用. change事件 eventType: String 目录发生变化的事件类型， rename和change filename: String | Buffer 发生变化的文件名 当监听的目录中发生变化时触发该事件. 根据操作系统的不同，filename参数有可能不会被提供. 如果提供了filename参数，那么它的类型将和调用fs.watch()时指定的encoding参数保持一致，如果encoding参数为buffer， 那么filename参数就是buffer类型，否则就是String类型. 123456// Example when handled through fs.watch listenerfs.watch('./tmp', &#123;encoding: 'buffer'&#125;, (eventType, filename) =&gt; &#123; if (filename) console.log(filename); // Prints: &lt;Buffer ...&gt;&#125;); error事件 error: watcher.close()停止目录监听操作. fs.ReadStreamReadStream是一个可读流. open事件 fd: Integer ReadStream中使用的文件描述符，整型 当ReadStream被打开时触发该事件. close事件当调用fs.close()方法关闭了ReadStream对象的底层文件描述符时触发该事件. readStream.bytesRead该属性标识当前readStream已经读取的字节数. readStream.path该属性标识了fs.createReadStream()方法的第一个参数，也就是当前readStream对象正在读取的路径名称. 如果path参数是以string类型传递的，那么该属性返回的也是个String类型，如果path是以Buffer类型传递的，那么该属性也返回Buffer类型. fs.Stats调用fs.stat(), fs.lstat()以及fs.fstat()方法以及它们的同步版本的方法时返回该类型的对象. 该类型的对象包含以下方法: stats.isFile() stats.isDirectory() stats.isBlockDevice() stats.isCharacterDevice() stats.isSymbolicLink()(该方法只在调用fs.lstat()方法有效) stats.isFIFO() stats.isSocket() 对于普通的文件而言，调用util.inspect(stats)将返回以下格式的字符串: 12345678910111213141516&#123; dev: 2114, ino: 48064969, mode: 33188, nlink: 1, uid: 85, gid: 100, rdev: 0, size: 527, blksize: 4096, blocks: 8, atime: Mon, 10 Oct 2011 23:24:11 GMT, mtime: Mon, 10 Oct 2011 23:24:11 GMT, ctime: Mon, 10 Oct 2011 23:24:11 GMT, birthtime: Mon, 10 Oct 2011 23:24:11 GMT&#125; 这里需要注意atime, mtime, ctime以及birthTime均是Date类型的对象，如果要比较这些对象，需要调用相应的方法. 通常情况下，可以使用getTime()方法来返回毫秒数(从1970 00:00:00 UTC算起)，然后直接比较毫秒数即可. 但是，也可以使用更复杂的方法来显示更多的信息，具体可以参阅JS文档-Date Stats中的时间值Stats对象中各种时间的语义如下： atime: “Access Time”. 该属性标识的是文件最后一次被访问的时间. 调用mknod(2), utimes(2)以及read()方法将修改这个属性值. mtime: ”Modified Time”. 该属性标识了文件内容最后一次被修改的时间，调用mknod(), utimes()以及write()方法将修改这个属性的值 ctime: “Change Time”. 该属性标识了文件状态最后一次被修改的时间(inode数据修改)，调用chmod()， chown()， link(), mknod(), rename(), unlink(), utimes(), read()以及write()方法将修改这个属性的值. birthname: “Birth Time”. 该属性标识了文件被创建的时间，只在文件创建时被设置一次，后续不再修改. 在一些不支持birthname属性的系统中，这个值要么被设置ctime, 要么被设置成1970-01-01T00:00Z. 在Node.js V0.12版本之前，windows系统中的ctime的值与birthtime一致，但在V0.12版本以及之后的版本中，ctime不再是创建时间. 在Unix系统中，ctime将始终都是文件状态被修改的时间, 而不是创建的时间. fs.WriteStreamWriteStream是一个可写流. open事件 fd: Integer 在WriteStream对象中使用的文件描述符，整型 当WriteStream对象打开时被触发. close事件当调用fs.close()方法来关闭WriteStream对象的底层文件描述符时触发该事件. writeStream.bytesWritten该属性记录了当前已经写入的字节数，注意这个属性不包括缓冲区中的数据. writeStream.path该属性记录了调用fs.createWriteStream()方法时的第一个参数，也就是当前WriteStream对象正在写入的文件路径. 如果path参数是以String类型传递给fs.createWriteStream()方法，那么该属性也是String类型，如果传入时是Buffer类型，那么该属性也返回Buffer类型. fs.access(path[,mode],callback) path: String | Buffer mode: \\ callback: Function 该方法用于测试进程是否有访问path路径的权限. mode参数指定了需要验证的权限项，以下是所有可用的mode参数，可以或者按位或操作来组合多个权限项： fs.constants.F_OK - 该权限项用于确定path路径是否对当前进程可见. 这个权限项通常可用来判断path路径是否存在，如果省略了mode参数，默认情况下验证的就是这个权限项. fs.constants.R_OK - 该权限项用于确定path路径对于当前进行是否可读 fs.constants.W_OK - 该权限项用于确定path路径对于当前进程是否可写 fs.constants.X_OK - 该权限项用于确定path路径对于当前进程是否可执行，该权限项在Windows系统中与fs.constants.F_OK一样 最后一个回调方法在调用时，如果校验的权限项不满足，则会在回调时带上错误参数. 以下的例子演示了校验当前进程是否可以对/etc/passwd文件进行读写:123fs.access('/etc/passwd', fs.constants.R_OK | fs.constants.W_OK, (err) =&gt; &#123; console.log(err ? 'no access!' : 'can read/write');&#125;); 但是,node.js并不推荐在调用fs.open(), fs.readFile()以及fs.writeFile()方法前先调用fs.access()方法来验证文件是否存在，因为这样做可能会导致”竞争条件”，在校验的过程中可能会有其它的进程对文件进行了修改，建议直接尝试对文件进行操作，并处理文件不存在的情况进行处理.123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//不推荐， 打开fs.access('myfile', (err) =&gt; &#123; if (!err) &#123; console.error('myfile already exists'); return; &#125; fs.open('myfile', 'wx', (err, fd) =&gt; &#123; if (err) throw err; writeMyData(fd); &#125;);&#125;);//推荐，打开fs.open('myfile', 'wx', (err, fd) =&gt; &#123; if (err) &#123; if (err.code === \"EEXIST\") &#123; console.error('myfile already exists'); return; &#125; else &#123; throw err; &#125; &#125; writeMyData(fd);&#125;);//不推荐fs.access('myfile', (err) =&gt; &#123; if (err) &#123; if (err.code === \"ENOENT\") &#123; console.error('myfile does not exist'); return; &#125; else &#123; throw err; &#125; &#125; fs.open('myfile', 'r', (err, fd) =&gt; &#123; if (err) throw err; readMyData(fd); &#125;);&#125;);//推荐， 读fs.open('myfile', 'r', (err, fd) =&gt; &#123; if (err) &#123; if (err.code === \"ENOENT\") &#123; console.error('myfile does not exist'); return; &#125; else &#123; throw err; &#125; &#125; readMyData(fd);&#125;); 以上的例子中，标识为“不推荐”的例子，都是先使用fs.access()校验文件是否存在，然后再使用文件进行操作，但是如果在两个操作之间，有其它的进程修改了文件，那么将导致“竞争条件”. 而”推荐”的方法都是直接使用文件，并在抛出异常时进行处理.总得来讲，只在不直接使用文件时，才推荐使用fs.access()方法来校验文件是否存在. fs.accessSync(path[,mode]) path: String | Buffer mode: fs.access()方法的同步版本，该方法在权限项校验失败时抛出异常，否则不做任何事情 fs.appendFile(file, data[,options], callback) file: String | Buffer | Number 文件名称或文件描述符 data: String | Buffer options: Object | String encoding: String | Null 默认情况为utf-8 mode: Integer 默认为0o666(十六进制的数字) flag: String 默认为’a’ callback: Function 往文件中异步追回数据，如果文件不存在，那么会新建文件. data参数可以是String类型或者是Buffer类型. 例如： 1234fs.appendFile('message.txt', 'data to append', (err) =&gt; &#123; if (err) throw err; console.log('The \"data to append\" was appended to file!');&#125;); 如果options参数是String类型，那么它指定的是encoding参数的值. 例如： 1fs.appendFile('message.txt', 'data to append', 'utf8', callback); 如果file指定的是文件描述符，那么它必须是打开的. 另外，如果file参数是文件描述符，那么它不会被自动关闭. fs.appendFileSync(file, data[, options]) file: String | Buffer | Number 文件名称或文件描述符 data: String | Buffer options: Object | String encoding: String | Null 默认情况为utf-8 mode: Integer 默认为0o666(十六进制的数字) flag: String 默认为’a’ fs.appendFile()方法的同步版本，返回undefined. fs.chmod(path, mode, callback) path: String | Buffer mode: Integer callback: Function chmod(2)的异步版本实现. 除了异常错误参数之外，回调函数不会有其它的参数. fs.chmodSync(path, mode) path: String | Buffer mode: Integer 同步版本的chmod(2). 返回undefined. fs.chown(path, uid, gid, callback) path: String | Buffer uid: Integer gid: Integer callback: Function 异步版本的chown(2). 除了异常错误参数之外，回调函数不会有其它的参数. fs.chownSync(path, uid, gid) path: String | Buffer uid: Integer gid: Integer 同步版本的chown(2)，返回undefined. fs.close(fd, callback) fd: Integer callback: Function close(2)方法的异步实现，除了异常错误参数之外，回调函数不会有其它的参数. fs.closeSync(fd) fd: Integer 同步版本的close(2)实现，返回undefined fs.constants该对象存储了文件操作中常用的常量信息，具体定义的常量值可见“FS模块中的常量值”一节 fs.createReadStream(path[, options]) path: String | Buffer options: Object | String flags: String encoding: String fd: Integer mode: Integer autoClose: Boolean start: Integer(inclusive) end: Integer(inclusive) 该方法返回ReadStream类型的对象. 值得注意的是，在可读流对象中，highWaterMark值默认设置的是16kb， 但是这个方法中返回的highWaterMark参数是64Kb. 另外，options参数的默认值如下， 1234567&#123; flags: 'r', encoding: null, fd: null, mode: 0o666, autoClose: true&#125; 其中start和end参数用于读取部分的文件数据，默认情况下读取整个文件的数据内容. encoding参数可以是任意可用的Buffer编码方式. 如果指定了fd参数，但没有指定start参数，fs.createReadStream()方法将从文件的当前位置开始读取数据. 另外，如果指定了fd参数，ReadStream将忽略path参数，这意味着文件的open事件不会被触发. 备注：fd应该是阻塞的，非阻塞的fd参数应该传递给net.Socket 如果autoClose参数设置为false, 那么即使在处理遇到错误的时候，文件描述符也不会自动关闭. 关闭文件描述符是应用程序的责任，否则将导致文件描述符泄露. 如果autoClose参数设置为true, 那么当遇到error事件和end事件时，文件描述符都会被自动关闭. mode参数指定了在新建文件时使用的权限项. 以下的例子演示了从一个100字节的文件中读取最后10个字节的例子： 1fs.createReadStream('sample.txt', &#123;start: 90, end: 99&#125;); 如果options参数是字符串类型，那么它指定了encoding参数的值 fs.createWriteStream([options]) path: String | Buffer options: Object | String flags: String defaultEncoding: String fd: Integer mode: Integer autoClose: Boolean start: Integer(inclusive) 该方法返回WriteStream对象. options参数可以是对象类型，也可以是字符串类型. 它的默认值如下： 1234567&#123; flags: 'w', defaultEncoding: 'utf8', fd: null, mode: 0o666, autoClose: true&#125; options参数中可以包含start参数，该参数指定了写入文件的开始位置. 如果要编辑某个文件的内容，而不是替换全部内容，那么flags参数的值应该设置为”r+”. defaultEncoding参数可以是任何有效的Buffer编码方式. 如果autoClose参数设置为true, 那么在遇到error和end事件时，文件将被自动关闭. 如果设置为false, 那么应该程序就有责任自行关闭文件描述符，否则将导致文件描述符泄露，因为在这种情况下，即使遇到了error事件，文件描述符也不会被关闭. 和ReadStream一样，如果指定了fd参数，那么WriteStream将忽略path参数，这意味着不会触发open事件. 备注：fd应该是阻塞的，非阻塞的fd参数应该传递给net.Socket 如果options参数是字符串类型，那么它指定的是encoding参数的值. fs.existsSync(path) path: String | Buffer fs.exists()方法的同步版本实现. 当文件存在时返回true, 否则false. 备注： fs.exist()方法已经被废弃了，但是fs.existSync()方法并没有. 因为fs.exist()方法接受的回调方法参数与node.js中其它的回调方法不一样，但是fs.existSync()是个同步实现，它不需要接受回调参数. fs.fchmod(fd, mode, callback) fd: Integer mode: Integer callback: Function fchmod(2)方法的异步实现. 在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.fchmodSync(fd, mode) fd: Integer mode: Integer fs.fchmod(2)的同步版本实现，返回undefined fs.fchown(fd, uid, gid, callback) fd: Integer uid: Integer gid: Integer callback: Function fchown(2)方法的异步实现. 在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.fchownSync(fd, uid, gid) fd: Integer uid: Integer gid: Integer fchown(2)方法的同步实现，返回undefined fs.fdatasync(fd, callback) fd: Integer callback: Function fdatasync(2)方法的异步实现，在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.fdatasyncSync(fd) fd: Integer fdatasync(2)的同步实现, 返回undefined fs.\u001cfstat(fd, callback) fd: Integer callback: Function fstat(2)方法的异步实现，回调函数带有(err, stats)两个参数，其中stats参数是fs.Stats类的实例. fstat()方法等同于stat()方法，只是在指定文件的时候是通过fd文件描述符来指定的，而不是通过路径名来指定的. fs.fstatSync(fd) fd: Integer fstat(2)方法的同步实现, 返回fs.Stats的实例. fs.fsync(fd, callabck) fd: Integer callback: Function fsync(2)方法的异步实现. 在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.fsyncSync(fd) fd: Integer fsync(2)方法的同步实现， 返回undefined. fs.ftruncate(fd, len, callback) fd: Integer len: Integer, 默认为0 callback: Function ftruncate(2)方法的异步实现. 在回调函数中，除了可能存在的异常参数外，不会有其它参数. 如果指定的文件超过了len参数指定的字节数，那么只会保留文件中的前len字节的内容. 例如，以下的代码只会保留文件中的最开始的4个字节： 123456789101112console.log(fs.readFileSync('temp.txt', 'utf8'));// Prints: Node.js// get the file descriptor of the file to be truncatedconst fd = fs.openSync('temp.txt', 'r+');// truncate the file to first four bytesfs.ftruncate(fd, 4, (err) =&gt; &#123; assert.ifError(err); console.log(fs.readFileSync('temp.txt', 'utf8'));&#125;);// Prints: Node 如果原始的文件不足len参数指定的字节数，那么这个文件会被空字符串(‘\\0’)填充到len长度，例如： 文件的最后3个字节为\\0，以此来弥补truncate方法中指定的len字节数. 12345678910111213console.log(fs.readFileSync('temp.txt', 'utf-8'));// Prints: Node.js// get the file descriptor of the file to be truncatedconst fd = fs.openSync('temp.txt', 'r+');// truncate the file to 10 bytes, whereas the actual size is 7 bytesfs.ftruncate(fd, 10, (err) =&gt; &#123; assert.ifError(!err); console.log(fs.readFileSync('temp.txt'));&#125;);// Prints: &lt;Buffer 4e 6f 64 65 2e 6a 73 00 00 00&gt;// ('Node.js\\0\\0\\0' in UTF8) fs.ftruncateSync(fd, len) fd: Integer len: Integer, 默认为0 ftruncate(2)方法的同步版本实现， 返回undefined fs.futimes(fd, atime, mtime, callback) fd: Integer atime: Integer mtime: Integer callback: Function 修改fd文件描述符指定文件的时间戳. fs.futimesSync(fd, atime, mtime) fd: Integer atime: Integer mtime: Integer fs.futimes()方法的同步版本实现，返回undefined. fs.lchmod(path, mode, callback) path: String | Buffer mode: Integer callback: Function lchmod(2)方法异步版本实现，在回调函数中，除了可能存在的异常参数外，不会有其它参数. 另外，这个方法只在OSX系统中可用 fs.lchmodSync(path, mode) path: String | Buffer mode: Integer fs.lchmod()方法的异步版本实现，该方法返回undefined fs.lchown(path, uid, gid, callback) path: String | Buffer uid: Integer gid: Integer callback: Function lchown(2)的异步版本实现，在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.lchownSync(path, uid, gid) path: Integer uid: Integer gid: Integer lchown(2)的同步版本实现，返回undefined. 在V0.4.7中被废弃. fs.link(existingPath, newPath, callback) existingPath: String | Buffer newPath: String | Buffer callback: Function link(2)方法的异步实现，在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.linkSync(existingPath, newPath) existingPath: String | Buffer newPath: String | Buffer link(2)方法的同步版本实现， 返回undefined. fs.lstat(path, callback) path: String | Buffer callback: Function lstat(2)方法的异步实现，回调方法带有两个参数(err, stats)， 其中stats参数是fs.Stats类的实例，lstat()方法与stat()方法基本等同， 只在指定的文件为符号连接时存在差别. 当Path参数指定的文件为符号连接时，lstat()会获取符号连接本身的数据信息，而stat()方法会获取符号连接所指向的真实文件的数据信息. fs.lstatSync(path) path: String | Buffer lstat(2)的同步版本实现，返回fs.Stats实例 fs.mkdir(path[, mode], callback) path: String | Buffer mode: Integer， 默认值为0o777 callback: Function mkdir(2)方法的异步实现，在回调函数中，除了可能存在的异常参数外，不会有其它参数. fs.mkdirSync(path[, mode]) path: String | Buffer mode: Integer mkdir(2)的同步版本实现，返回undefined. fs.mkdtemp(prefix[, options], callback) prefix: String options: Object | String encoding: String, 默认为utfi callback: Function 该方法用于创建一个临时目录, 目录的文件名为在prefix参数后随机增加6个字符. 在创建完目录后，目录的文件名会作为callback回调函数的第二个参数回调. 可选的options参数可以是字符串，用来指定prefix参数的编码格式，也可以是个对象，该对象包含encoding属性. 12345fs.mkdtemp('/tmp/foo-', (err, folder) =&gt; &#123; if (err) throw err; console.log(folder); // Prints: /tmp/foo-itXde2&#125;); 备注： fs.mkdtemp()方法会直接在prefix参数后面加6个随机字符. 例如 ，如果想要在/tmp目录下创建个临时目录，那么指定prefix参数时必须加上平台相关的路径分隔符， 可以使用require(“path”).sep来指定 12345678910111213141516171819202122// The parent directory for the new temporary directoryconst tmpDir = '/tmp';// This method is *INCORRECT*:fs.mkdtemp(tmpDir, (err, folder) =&gt; &#123; if (err) throw err; console.log(folder); // Will print something similar to `/tmpabc123`. // Note that a new temporary directory is created // at the file system root rather than *within* // the /tmp directory.&#125;);// This method is *CORRECT*:const path = require('path');fs.mkdtemp(tmpDir + path.sep, (err, folder) =&gt; &#123; if (err) throw err; console.log(folder); // Will print something similar to `/tmp/abc123`. // A new temporary directory is created within // the /tmp directory.&#125;); fs.mkdtempSync(prefix[, options]) prefix: String options: String | Object encoding: String, 默认为utf-8 fs.mkdtemp()方法的同步版本实现，返回新创建的目录名. 可选的options参数可以是字符串，用来指定prefix参数的编码格式，也可以是个对象，该对象包含encoding属性. fs.open(path, flag[, mode], callback) path: String | Buffer flag: String | Number mode: Integer callback: Function 异步打开文件的方法， 可参考open(2)方法. flags参数可以是以下的选项： ‘r’ - 以只读的方式打开文件，如果文件不存在 ，则抛出异常 ‘r+’ - 以读写的方式打开文件，如果文件不存在 ，则抛出异常 ‘rs+’ - 以同步读写的方式打开文件，这个选项会建议操作系统禁用本地文件系统的缓存. 它在打开NFS系统中的文件时特殊有用，它避免了可能存在过期缓存的问题，但是，除非是有充足的理由，否则不建议使用这个选项，因为它会对性能产生严重的影响. 备注： 这个标识只是禁用了操作系统的本地缓存，但不会把fs.open()方法切换到同步方式，如果需要使用同步调用的方式，可以使用fs.openSync()方法 ’w’ - 以只写的方式打开文件，如果文件不存在 ，它会创建新的文件，已经存在的文件内容会被清空. ‘wx’ - 和’w’标识的作用一样，但在文件存在的情况下会抛出异常. ‘w+’ - 以读写的方式打开文件，如果文件不存在 ，它会创建新的文件，已经存在的文件内容会被清空. ‘wx+’ - 和’w+’标识的作用一样，但在文件存在的情况下会抛出异常 ‘a’ - 以追加的方式打开文件，如果文件不存在， 它会创建新的文件 ‘ax’ - 和’a’标识的作用一样，但在文件存在的情况下会抛出异常 ‘a+’ - 以读和追加的方式打开文件，如果文件不存在， 它会创建新的文件 ‘ax+’ - 和’a+’标识的作用一样，但在文件存在的情况下会抛出异常 ‘x’标识确保打开的文件是新建的，在POSIX系统中，path参数即使是一个指向不存在路径的符号连接，那么它也会被认为是已经存在了. ‘x’标识在网络文件系统中有可能无法正常工作. flags参数也可以是open(2)方法中定义的整型变量，这些常用的常量可以通过fs.constants来访问. 在windows系统中，flags参数被转化成对应的值, 例如： 0_WRONLY对应了FILE_GENERIC_WRITE， 0_EXCLIO_CREAT对应了CREATE_NEW. 在Linux系统下，如果是以追回模式打开文件，那么基于位置的写操作就无法正常工作，linux内核会忽略位置参数，始终在文件的末尾追回数据. 备注：对于fs.open()方法的一些标识而言，它们的作用可以与特定的操作系统相关. 例如，以下的例子中，在OSX和Linux系统中，尝试以’a+’模式打开目录会抛出错误. 相反，在Windows和BSD系统中，则可以正常地返回文件描述符. 123456789// OS X and Linuxfs.open('&lt;directory&gt;', 'a+', (err, fd) =&gt; &#123; // =&gt; [Error: EISDIR: illegal operation on a directory, open &lt;directory&gt;]&#125;);// Windows and FreeBSDfs.open('&lt;directory&gt;', 'a+', (err, fd) =&gt; &#123; // =&gt; null, &lt;fd&gt;&#125;); mode参数只在创建文件的时候起作用，它指定了新创建文件的权限项， 默认情况下为0o666, 可读写. callback回调函数有两个参数，分别是(err, fd) fs.openSync(path, flags[, mode]) path: String flags: String | Integer mode: Integer fs.open()方法的同步版本实现，返回相应的文件描述符. fs.read(fd, buffer, offset, length, position, callback) fd: Integer buffer: String | Buffer offset: Integer length: Integer position: Integer callback: Function fs.read()方法从指定的文件描述符中读取数据. buffer参数定义了读取到的数据的写入位置. offset参数定义了在写入数据时的偏移量. length参数定义了要读取的数据长度. potision参数定义了从文件中读取数据的起始位置. 如果potions参数为null, 那么将会从fd的当前位置开始读取. callback回调参数有三个参数，分别是(err, bytesRead, buffer) fs.readdir(path[,options], callback) path: String | Buffer options: String | Object encoding: String 默认为utf-8 callback: Function readdir(3)方法的异步实现, 该方法会读取目录的内容. callback回调函数有两个参数，分别是(err, files), 其中files参数是包含了目录中路径名的数组，但不包括”.”和”..” 可选的options参数可以是字符串类型，也可以是带有encoding属性的对象类型. 该参数指定了回传给callback回调函数的文件名称的编码类型，如果encoding参数为buffer, 那么返回的将是buffer类型，否则将是指定编码的字符串类型. fs.readdirSync(path[, options]) path: String | Buffer options: String | Object encoding: String , 默认为utf-8 readdir(3)方法的同步版本实现，返回该目录下所有的文件名，不包括”.”和”..” 可选的options参数可以是字符串类型，也可以是带有encoding属性的对象类型. 该参数指定了回传给callback回调函数的文件名称的编码类型，如果encoding参数为buffer, 那么返回的将是buffer类型，否则将是指定编码的字符串类型. fs.readFile(file[, options], callback) file: String | Buffer | Integer 文件名或者文件描述符 options: String | Object encoding: String | Null 默认为null flag: String 默认为’r’ callback: Function 异步地方式读取文件的全部内容. 例如： 1234fs.readFile('/etc/passwd', (err, data) =&gt; &#123; if (err) throw err; console.log(data);&#125;); 回调函数中有两个参数，分别是(err, data)， 其中data参数是读取的文件内容. 如果没有指定encoding参数，那么data参数就是没有编码过的buffer类型. 如果options参数是string类型，那么它指定了encoding参数的值. 例如： 1fs.readFile('/etc/passwd', 'utf8', callback); 另外，值得注意的是，提供的任何文件描述符必须支持读模式. 而且，如果是以文件描述符的方式来指定file参数，那么当文件读取结束的时候，文件不会被自动关闭. fs.readFileSync(file[, options]) file: String | Buffer | Integer 文件名或者文件描述符 options: String | Object encoding: String | Null 默认为null flag: String 默认为’r’ fs.readFile()的同步版本， 返回文件的内容. 如果options参数是string类型，那么它指定了encoding参数的值. fs.readlink(path[, options], callback) path: String | Buffer options: String | Object encoding: String 默认为utf-8 callback: Function readlink(2)方法的异步实现. 回调方法带有两个参数，分别是(err, linkString). 可选的options参数可以是字符串类型，也可以是带有encoding属性的对象类型. 该参数指定了回传给callback回调函数的文件名称的编码类型，如果encoding参数为buffer, 那么返回的将是buffer类型，否则将是指定编码的字符串类型. fs.readlinkSync(path[, options]) path: String | Buffer options: String | Object encoding: String 默认为utf-8 readlink(2)方法的同步版本实现, 返回符号连接字符串. 可选的options参数可以是字符串类型，也可以是带有encoding属性的对象类型. 该参数指定了回传给callback回调函数的文件名称的编码类型，如果encoding参数为buffer, 那么返回的将是buffer类型，否则将是指定编码的字符串类型. fs.readSync(fd, buffer, offset, length, position) fd: Integer buffer: String | Buffer offset: Integer length: Integer position: Integer fs.read()方法的同步版本实现，返回的是读取的字节数. fs.realpath(path[, options], callback) path: String | Buffer options: String | Object encoding: String 默认为utf8 callback: Function realpath(3)方法的异步实现. 回调函数带有两个参数， 分别为(err, resolvedPath), 可以使用process.cwd来解析相对路径. 目前只支持utf-8格式路径名. 可选的options参数可以是字符串类型，也可以是带有encoding属性的对象类型. 该参数指定了回传给callback回调函数的文件名称的编码类型，如果encoding参数为buffer, 那么返回的将是buffer类型，否则将是指定编码的字符串类型. fs.readpathSync(path[, options]) path: String | Buffer options: String | Object encoding: String 默认为utf8 realpath(3)方法的同步实现，返回解析的路径. 目前只支持utf-8格式路径名. 可选的options参数可以是字符串类型，也可以是带有encoding属性的对象类型. 该参数指定了回传给callback回调函数的文件名称的编码类型，如果encoding参数为buffer, 那么返回的将是buffer类型，否则将是指定编码的字符串类型. fs.rename(oldPath, newPath, callback) oldPath: String | Buffer newPath: String | Buffer callback: Function rename(2)方法的异步版本实现. 在回调函数的参数中，除了可能存在的错误异常参数外，不会有其它的参数. fs.renameSync(oldPath, newPath) oldPath: String | Buffer newPath: String | Buffer rename(3)的同步版本实现, 返回undefined. fs.rmdir(path, callback) path: String | Buffer callback: Function rmdir(2)方法的异步版本实现，在回调函数的参数中，除了可能存在的错误异常参数外，不会有其它的参数. fs.rmdirSync(path) path: String | Buffer rmdir(2)方法的同步版本实现，返回undefined. fs.stat(path, callback) path: String | Buffer callback: Function stat(2)方法的异步版本实现，回调函数带有两个参数， 分别是(err, stats), 其中stats参数为fs.Stats对象的实例. 如果发生了错误，那么err.code会是”常见系统错误码”中的一个. 不推荐在使用fs.open()，fs.readFile()以及fs.writeFile()方法之前使用fs.stat()方法来确定文件是否存在. 相反，推荐直接尝试 打开/读取/写入 文件，并处理文件不存在时可能存在的异常信息. 如果只是要确认文件是否存在，但后续不需要再对文件进行操作，则推荐使用fs.access()方法. fs.statSync(path) path: String | Buffer stat(2)方法的同步版本实现， 返回fs.Stats类的实例. fs.symlink(target, path[, type], callback) target: String | Buffer path: String | Buffer type: String callback: Function symlink(2)方法的异步版本实现. 在回调函数的参数中，除了可能存在的错误异常参数外，不会有其它的参数. type参数可能是dir, file或者是junction, 默认情况下为file. 备注: junction参数仅在windows系统中可用， 且要求junction为绝对路径. 当使用junction选项时，target参数会自动解析成绝对路径. 1fs.symlink('./foo', './new-port'); 以上的代码创建了一个新的符号连接，将new-port连接到foo. fs.symlinkSync(target, path[, type]) target: String | Buffer path: String | Buffer type: String symlink(2)方法的同步版本实现，返回undefined fs.truncate(path, len, callback) path: String | Buffer len: Integer 默认为0 callback: Function truncate(2)方法的异步版本实现. 在回调函数的参数中，除了可能存在的错误异常参数外，不会有其它的参数. 如果第一个参数是以文件描述符的方式提供，那么将会调用 fs.ftruncate()方法. fs.truncateSync(path, len) path: String | Buffer len: Integer 默认为0 truncate(2)的同步版本实现，返回undefined. 如果第一个参数是以文件描述符的方式提供，那么将会调用 fs.ftruncate()方法. fs.unlink(path, callabck) path: String | Buffer callback: Function unlink(2)方法的异步版本实现. 在回调函数的参数中，除了可能存在的错误异常参数外，不会有其它的参数. fs.unlinkSync(path) path: String | Buffer unlink(2)方法的同步版本实现，返回undefined. fs.unwatchFile(filename[, listener]) filename: String | Buffer listener: Function 停止对filename文件的监听. 如果指定了listener参数，那么只会移除该listener监听器. 否则，该文件上所有的监听器都会被移除. 如果在未监听的文件上调用fs.unwatchFile()方法，不会执行任何操作. 备注: fs.watch()方法的效率比fs.watchFile()和fs.unwatchFile()方法的效率高. 建议使用fs.watch()方法来代替fs.watchFile()和fs.unwatchFile()方法. fs.utimes(path, atime, mtime, callback) path: String | Buffer atime: Integer mtime: Integer callback: Function 该方法用于修改文件的时间戳. 备注： atime和mtime参数遵循以下的规则: 它们的值必须是unix时间戳，以秒为单位. 例如： Date.now()返回的是毫秒数，所以它必须除以1000后才能作为参数传入. 如果传入的值是数值形式的字符串，那么它会被自动转化成相应的数字 如果传入的是NaN和Infinity, 那么该参数会自动被设置为Date.now() / 1000 fs.utimesSync(path, atime, mtime) path: String | Buffer atime: Integer mtime: Integer fs.utimes()方法的同步版本实现，返回undefined. fs.watch(filename[, options][, listener]) filename: String | Buffer options: String | Object persistent: Boolean 标识文件只要被监听，进程就应该继续运行. 默认值为true recursive: Boolean 标识是否监听该目录下的子目录，还是只监听该目录本身. 该参数只在监听目录时有用，且只在特定的系统中(OSX和Linux)被支持.（ 参看”缺点”一节），默认值为false encoding: String 指定回传给listener监听方法的文件名的编码方式，默认为utf-8 listener: Function 该方法用于监听filename参数指定的目录，filename参数可以是文件也可以是目录，返回的对象为fs.FSWatcher类的实例. 方法的第二个参数是可选的，如果以字符串类型提供了options参数，那么该参数指定了encoding参数的值. 否则options参数为对象类型. listener回调方法带有两个参数，分别是(eventType, filename), 其中eventType是rename，或者是change中的一种； 而filename则表示触发事件的路径名称. 值得一提的是，在大部分平台中，rename事件会在新建和删除文件的时候被触发. 另外，listener回调方法也会被关联到fs.FSWatcher对象的change事件中，但是它和eventType值被设置为change的事件不是一回事. 缺点fs.watch方法并不是完全地跨平台，并且在某些情况下无法使用. 另外，recursive参数仅在OSX和Linux系统中可用. 1. 可用性该方法的实现依赖于底层操作系统提供的通知机制： 在linux系统中，该方法使用了inotify 在BSD系统中，该方法使用了kqueue 在OSX系统中，该方法在监听文件时使用了kqueue,而在监听文件夹时使用了FSEvents 在Windows系统中， 该方法取决于ReadDirectoryChangesW 在Aix系统中， 该方法取决于AHAFS 如果依赖的底层操作系统中的功能因为某些原因不可用，那么fs.watch()方法将不可用. 例如，在网络文件系统(NFS, SMB等)和主机文件系统中，监听文件或者目录的功能将不会很稳定，在有些情况下，甚至会不可用. 但应用程序仍可以使用fs.watchFile()，这个方法底层使用了stat polling机制， 但它运行的效率更低，稳定性也更差一些. 2. Inodes在Linux和OSX系统中， fs.watch()方法会解析路径到某个inode节点，并监视该节点. 但如果正在被监听的路径被删除后又重新创建了，那么它就会被分配一个新的inode节点. 此时，监听器会在delete操作后触发相应的事件，并继续监听原来的inode节点，而新创建的节点将不会触发任何事件. 3. 文件名参数只有在Linux和windows系统中， 才支持在回调函数中提供filename参数. 但即使是在支持的操作系统中，也不能保证每次都能提供filename参数. 因此，在使用filename参数时，不能假设回调函数中该参数的值会始终被提供，必须对该参数可能存在null的情况进行处理. 12345678fs.watch('somedir', (eventType, filename) =&gt; &#123; console.log(`event type is: $&#123;eventType&#125;`); if (filename) &#123; console.log(`filename provided: $&#123;filename&#125;`); &#125; else &#123; console.log('filename not provided'); &#125;&#125;); fs.watchFile(filename[, options], listener) filename: String | Buffer options: Object persistent: Boolean 该参数用于标识是否只要文件被监听，进程就应该一直运行, 默认为true interval: Integer listener: Function 监听filename指定的文件, 每次当文件被访问时，listener回调函数都会被触发. options参数是可选参数. 如果提供了该参数，那它必须是对象类型. options参数中可以包含有persistent属性和interval属性，其中persistent属性是用来标识是否只要文件被监听，进程就应该继续运行的标志，而interval参数指定了监听目标文件的频率，默认情况下两者的值分别是{persistent: true, interval: 5007}. listener回调函数接受两个参数，分别是当前的stat对象以及之前的stat对象, 它们都是fs.Stats类的实例. 例如： 1234fs.watchFile('message.text', (curr, prev) =&gt; &#123; console.log(`the current mtime is: $&#123;curr.mtime&#125;`); console.log(`the previous mtime was: $&#123;prev.mtime&#125;`);&#125;); 如果应用程序需要在文件被修改的时候得到通知，而不仅仅是访问的时候，那么只需要比较curr.mtime和prev.mtime即可. 备注： 当fs.watchFile()方法抛出ENOENT错误时，它会调用一次listener方法，但方法中的参数的全部属性都被0填充. 在Windows系统中，blksize以及blocks属性都会被设置为undefined. 如果文件在后续被创建，那么listener函数会再次被调用，函数的参数将会是新建的stat对象，这个是在v0.10版本中新增的功能. 另外，建议使用fs.watch()来代替fs.watchFile()和fs.unwatchFile()方法，前者的效率和稳定性都比后两者好. fs.write(fd, buffer, offset, length[, position], callback) fd: Integer buffer: Buffer | String offset: Integer length: Integer position: Integer callback: Function 该方法往指定的文件描述符中写入数据. offset和length参数指定了buffer要被写入文件的位置和长度. position参数指定了写入文件的初始位置 ，如果它不是number类型，那么数据将会被写入到文件描述符的当前位置. callback回调函数会带有三个参数，分别是(err, written, buffer)， 其中written参数指定了当前已经被写入的数据字节数. 备注： 在没有等待回调函数返回的情况下，多次调用fs.write()方法往同一个文件中写入数据是不安全的操作. 对于这种场景，建议使用fs.createWriteStream()来操作. 在linux操作系统中，如果文件是以追加的方式打开的，那么不支持在指定的位置上执行写操作，内核将会忽略提供的位置参数，始终在文件的末尾执行写入操作. fs.write(fd, data[, position][, encoding], callback) fd: Integer data: String | Buffer position: Integer encoding: String callback: Function 该方法将指定的数据写入到fd文件描述符中. 如果data参数的类型不是buffer类型，那么它将会被自动转化成string类型. position参数指定了写入文件的初始位置 ，如果它不是number类型，那么数据将会被写入到文件描述符的当前位置. encoding参数指定了编码格式. callback()方法将会有三个参数，分别是(err, written, string)， 其中written参数标识了被要求写入的字节数， 需要注意的是，这里的written参数返回的字节数与data参数的字符数并不相等. 具体可以查看Buffer.byteLength()方法 不像写入buffer类型的数据时，可以指定写入部分的数据，当写入的数据为字符串类型时，整个字符串都会被写入，而不能指定部分字符串. 这是因为字符串中的偏移量有可能与字节数的偏移量存在差异. 备注： 在没有等待回调函数返回的情况下，多次调用fs.write()方法往同一个文件中写入数据是不安全的操作. 对于这种场景，建议使用fs.createWriteStream()来操作. 在linux操作系统中，如果文件是以追加的方式打开的，那么不支持在指定的位置上执行写操作，内核将会忽略提供的位置参数，始终在文件的末尾执行写入操作. fs.writeFile(file, data[, options], callback) file: String | Buffer | Integer 文件名或者是文件描述符 data: String | Buffer options: Object | String encoding: String | Null 默认为utf-8 mode: Integer 默认为0o666 flag: String 默认为w callback: Function 该方法将数据异步地写到文件中，如果文件已经存在了，则替换其内容. data参数可以是string类型或者是buffer类型. 如果data是Buffer类型，那么将忽略encoding参数. 例如： 1234fs.writeFile('message.txt', 'Hello Node.js', (err) =&gt; &#123; if (err) throw err; console.log('It\\'s saved!');&#125;); 如果options参数为字符串，那么它指定的是encoding参数的值，如： 1fs.writeFile('message.txt', 'Hello Node.js', 'utf8', callback); 如果通过文件描述符指定文件的第一个参数，那么文件描述符必须支持写入数据. 备注： 在没有等待回调函数返回的情况下，多次调用fs.write()方法往同一个文件中写入数据是不安全的操作. 对于这种场景，建议使用fs.createWriteStream()来操作. 另外，如果是通过文件描述符来指定文件，那么文件在写入结束后，不会自动关闭. fs.writeFileSync(file, data[, options]) file: String | Buffer | Integer 文件名称或文件描述符 data: String | Buffer options: String | Object encoding: String 默认utf-8 mode: Integer 默认为0o666 flag: String 默认为’w’ fs.writeFile()方法的同步版本实现，返回undefined. fs.writeSync(fd, buffer, offset, length[, position]) fd: Integer buffer: String | Buffer offset: Integer length: Integer position: Integer fs.writeSync(fd, data[, position[, encoding]]) fd: Integer data: String | Buffer position: Integer encoding: String fs.write()方法的同步版本实现， 返回写入的字节数 FS常量以下是通过fs.constants变量定义的常量值. 备注：不是所有的变量在所有的操作系统中均可用 1. 文件访问权限常量以下的变量是在fs.access()中可用的权限常量： 常量值 描述 F_OK 标识文件对当前进程是否可见 R_OK 标识文件对当前进程是否可读 W_OK 标识文件对当前进程是否可写 X_OK 标识文件对当前进程是否可执行 2. 文件打开常量以下常量是在fs.open()中可用的常量 ： 常量值 描述 O_RDONLY 以只读方式打开文件 O_WRONLY 以只写方式打开文件 O_RDWR 以读写方式打开文件 O_CREAT 如果文件不存在 ，则创建文件 O_EXCL 如果设置了O_CREAT标识，但文件已经存在，此时打开文件应该返回错误 O_NOCTTY 如果文件名标识的是个终端设备，那么打开这个路径不会导致该终端成为当前进程的控制终端 O_TRUNC 如果文件名标识的是个普通文件，并且以写模式打开，那么该文件中的内容会被清除 O_APPEND 标识以追加的方式打开文件 O_DIRECTORY 如果文件名标识的不是个目录，那么打开该文件将失败 O_NOFOLLOW 如果文件名标识的是个符号连接，那么打开该文件将失败 O_SYNC 标识该文件是由同步IO的方式打开的 O_SYMLINK 如果文件名标识的是个符号连接，那么打开的是符号连接本身，而非它指向的目标文件 O_DIRECT 如果设置了这个标识 ，那么将最小化IO缓存的作用 O_NOBLOCK 标识文件以非阻塞模式打开文件 O_NOATIME 该标识仅在linux系统中有用，如果设置了这个标识，那么当读取文件的权限信息时，不会修改atime的属性值. 3. 文件类型常量以下的常量是在fs.Stats类中的mode属性使用的，用于确定文件的类型: 常量值 描述 S_IFMT 用于解析文件类型码的掩码 S_IFREG 普通文件 S_IFDIR 目录 S_IFCHR 面向字符的设备文件 S_IFBLK 面向块的设备文件 S_IFIFO FIFO的管道文件 S_IFLNK 符号连接 S_IFSOCK socket文件 4. 文件访问权限常量以下的常量是用于fs.Stats类中的mode属性使用的，用于确定文件的访问权限： 常量值 描述值 S_IRWXU 文件拥有者的读、写、执行权限 S_IRUSR 文件拥有者的读权限 S_IWUSR 文件拥有者的写权限 S_IXUSR 文件拥有者的执行权限 S_IRWXG 用户组拥有读、写、执行权限 S_IRGRP 用户组拥有读权限 S_IWGRP 用户组拥有写权限 S_IXGRP 用户组拥有执行权限 S_IRWXO 其它人拥有读、写、执行权限 S_IROTH 其它人拥有读权限 S_IWOTH 其它人拥有写权限 S_IXOTH 其它人拥有执行权限","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"filesystem","slug":"filesystem","permalink":"http://yoursite.com/tags/filesystem/"}]},{"title":"Node.js — Stream(3)","slug":"前端/nodejs/Nodejs.stream.3","date":"2017-03-15T08:30:00.000Z","updated":"2017-03-15T15:43:00.000Z","comments":true,"path":"2017/03/15/前端/nodejs/Nodejs.stream.3/","link":"","permalink":"http://yoursite.com/2017/03/15/前端/nodejs/Nodejs.stream.3/","excerpt":"","text":"实现Stream接口的APIstream模块基于js的原型继承的方式，对外提供了简洁的api，让应用程序可以很方便地实现Stream接口. 首先，开发者需要声明一个类，这个类继承自四个基本的stream接口(stream.Writable, stream.Readable, stream.Duplex, stream.Transform), 并确保调用了相应父类的构造方法: 1234567const Writable = require('stream').Writable;class MyWritable extends Writable &#123; constructor(options) &#123; super(options); &#125;&#125; 其次，新声明的类需要根据继承的接口，实现一个或多个以下的方法: 使用场景 要继承的类 要实现的方法 只读 Readable _read 只写 Writable _write, _writev 可读写 Duplex _read, _write, _writev 基于读到的内容写数据 Transfrom _transform, _flush 备注：在实现stream接口的时候，不要调用公有的接口，这些公有接口是供stream接口的使用者使用的(见”使用Stream接口的API”一节). 如果在实现接口的过程中使用了这些方法，将会导致接口的使用者在使用这些接口时出现不可预期的问题. 简化构造对于很多简单的场景而言，可以不依赖于继承来构造新的stream类， 这是通过直接构造四个基本类型的stream接口实例，并在构造函数中传入相应的方法来实现的. 如： 1234567const Writable = require('stream').Writable;const myWritable = new Writable(&#123; write(chunk, encoding, callback) &#123; // ... &#125;&#125;); 实现Writable接口Writable接口可用于扩展可写流对象. 自定义的可写流对象应该调用new stream.Writable()构造方法，并实现writable._write()方法, 可根据需要实现writable._writev()方法. Writable([options])构造器 options: Object highWaterMark: Number 可写流对象内部缓冲区的大小，默认为16384字节(16kb)，或16个对象 decodeStrings: Boolean 标识将字符串传递给_write()方法之前，是否进行解码操作，默认为true objectMode: Boolean 标识当前可写流是否处于“对象模式”, 这意味着stream.write(anyOjb)是否为有效地操作. 当设置了这个标识后，stream.write()方法就可以接收除了string和buffer类型之外的其它js类型. 默认情况下为false. write: Function stream._write()方法的实现 writev: Function stream._writev()方法的实现 例如： 12345678910111213141516171819202122232425262728293031const Writable = require('stream').Writable;class MyWritable extends Writable &#123; constructor(options) &#123; // Calls the stream.Writable() constructor super(options); &#125;&#125;//或者使用ES6风格的构造器const Writable = require('stream').Writable;const util = require('util');function MyWritable(options) &#123; if (!(this instanceof MyWritable)) return new MyWritable(options); Writable.call(this, options);&#125;util.inherits(MyWritable, Writable);//或者使用简化的构造器const Writable = require('stream').Writable;const myWritable = new Writable(&#123; write(chunk, encoding, callback) &#123; // ... &#125;, writev(chunks, callback) &#123; // ... &#125;&#125;); writable._write(chunk, encoding, callback) chunk: String | Buffer 写入的数据块. 如果没有设置decodeStrings参数，那么传入的永远都是个buffer encoding: String 如果chunk为字符串类型，那么这个参数标识了字符串的编码格式. 如果chunk为buffer类型，或者可写流处于“对象模式”,那么该参数将被忽略 callback: Function 在处理完数据块之后的回调函数，函数可能会带有错误参数. 所有自定义的Writable实现必须要实现writable._write()方法. 另外，不管这个方法对数据的处理结果是成功还是抛出错误. 都必须调用callback()回调方法，当处理的过程发生错误时，callback()回调函数的第一个参数为错误对象，否则为null. 注：Transform接口提供了特定格式的writable._write()实现. 注： 应用程序不能直接调用该方法. 这个方法是给子类继承时实现的，它只能在实现内部被调用. 另外，值得注意的是，在调用writable._write()方法和callback()回调方法之间，如果调用了writable().write()方法，那么这些数据将被存储在缓冲区中. 一旦调用了callback()方法，那么可写流对象将会触发drain事件. 如果实现的流对象可以一次性处理多个数据块，那么这个实现应该实现_writev()方法. 如果在构造函数的options参数中设置了decodeStrings标识，那么chunk参数就是字符串类型，而不是buffer类型，此时，encoding参数标识了当前字符串的编码格式. 这种方式可以针对某种编码格式进行一些优化处理. 如果decodeStrings标识被显式设置为false, 那么encoding参数将被忽略, chunk参数保持与传入writable.write()方法时一致. writable._write()方法名加上了下划线作为前缀，这是用来标识该方法是内部方法，仅供实现类内部使用，不能被外部的API使用者调用. writable._writev(chunks, callback) chunks: Array 写入的数据块数组. 每个数据块的格式为{chunk: …, encoding: …} callback: Function 数据块处理完成后的回调函数，可能会提供相应的错误参数 备注：这个方法仅供实现类内部使用，应用程序不能直接调用该方法. writable._writev()方法可以用来一次性处于多个数据块的场景. 如果实现了这个方法，缓冲区中所有的数据将会通过这个方法被调用. 该方法名加上了下划线作为前缀，这是用来标识该方法是内部方法，仅供实现类内部使用，不能被外部API的使用者调用. 写入数据时发生错误当在调用writable._write()方法或writable._writev()方法的过程产生错误时，建议使用callback回调函数，并将错误信息作为其第一个参数回传，这样可以触发error事件. 如果只是在方法中抛出error异常，方法的具体行为将取决于该可写流是如何被使用的，因此可能会导致不一致的行为. 使用callback回调函数的方式，可以保证错误信息总是能被正确地处理. 1234567891011const Writable = require('stream').Writable;const myWritable = new Writable(&#123; write(chunk, encoding, callback) &#123; if (chunk.toString().indexOf('a') &gt;= 0) &#123; callback(new Error('chunk is invalid')); &#125; else &#123; callback(); &#125; &#125;&#125;); 可写流实现示例以下的例子是一个非常简单的Writable接口的实现样例. 虽然这个实现没有实际作用，但它展示了实现Writable接口需要注意的点. 123456789101112131415const Writable = require('stream').Writable;class MyWritable extends Writable &#123; constructor(options) &#123; super(options); &#125; _write(chunk, encoding, callback) &#123; if (chunk.toString().indexOf('a') &gt;= 0) &#123; callback(new Error('chunk is invalid')); &#125; else &#123; callback(); &#125; &#125;&#125; 实现Readable接口所有实现可读流的对象都必须实现Readable接口. 自定义的可读流对象必须调用new stream.Readable([options])构造方法，并实现readable._read()方法. new stream.Readable([options]) options: Object highWaterMark: Number 设置可读流对象内部缓冲区的大小，默认情况下为16KB, 或者为16个对象 encoding: 如果设置了该参数，那么读取的数据将使用该编码格式进行解码，默认情况下为null objectMode: Boolean 标识该可读流是否处于“对象模式”. 处于“对象模式”的可读流返回的数据为单个对象，而不是buffer类型. 默认情况下为false. read： Function 实现stream._read()的方法 例如： 12345678910111213141516171819202122232425262728const Readable = require('stream').Readable;class MyReadable extends Readable &#123; constructor(options) &#123; // Calls the stream.Readable(options) constructor super(options); &#125;&#125;//或者使用ES6格式的构造器const Readable = require('stream').Readable;const util = require('util');function MyReadable(options) &#123; if (!(this instanceof MyReadable)) return new MyReadable(options); Readable.call(this, options);&#125;util.inherits(MyReadable, Readable);//或者使用简化的构造器const Readable = require('stream').Readable;const myReadable = new Readable(&#123; read(size) &#123; // ... &#125;&#125;); readable._read(size) size: Number 异步读取的数据块大小 备注： 这个方法仅供实现类内部使用，API的使用者不应该直接调用该方法. 所有实现了Readable接口的可读流都应该实现这个方法，以获取来自底层资源的数据. 当调用了readable._read()方法后，如果底层资源中有可读取的数据，那么实现应该不断调用 readable.push(dataChunk)方法将这些数据推到数据队列中以供应用程序读取，直到readable.push()方法返回false为止. 当_read()方法停止后，只有当它再次被调用的时候，才能继续将数据推到数据队列中. 备注： 一旦readable._read()方法被调用后，直到readable.push()方法被调用，_read()方法才能被再次调用. size参数是建议性的. 对于那些将“读取”操作当成是单独操作的实现来讲，可以按照size参数指定的大小来获取相应的数据，而对于另一些实现来讲，只要一有数据到达，它们就可以将数据读取出来，然后调用readable.push(chunk)方法将数据推入队列，而不用等到有size大小的数据时再去读取. readable._read()方法名前加了下划线作为前缀，这是用来标识该方法仅供实现类内部使用，API的使用者不应该直接调用该方法. readable.push(chunk[, encoding]) chunk: String | Buffer 需要推入读取队列的数据块 encoding: String 数据块的编码格式，必须是有效的编码格式，如utf-i, hex, ascii等. 返回： Boolean 当读取队列还可以推入数据时，返回true, 否则false 当chunk参数为String或者Buffer类型时，chunk数据将被加入到可读流的内部队列中以供应用程序读取. 如果chunk为null, 表示当前流已经到达尾部，那么无法再往内部队列中写入更多数据. 当Readable对象处于paused模式时，调用readable.push()方法推入到内部队列中的数据会一直存在队列中，直到应用程序在readable事件中调用了readable.read()方法才会被读取. 而当Readable对象处于flowing模式时，调用该方法推入的数据将会通过data事件被提供给应用程序读取. readable.push()方法被设计得尽可能灵活. 比如说，当某些底层资源提供了暂停/恢复机制以及相应的回调机制时，就可以通过自定义readable实现对这些资源的包装, 如下所示： 12345678910111213141516171819202122232425262728// source is an object with readStop() and readStart() methods,// and an `ondata` member that gets called when it has data, and// an `onend` member that gets called when the data is over.class SourceWrapper extends Readable &#123; constructor(options) &#123; super(options); this._source = getLowlevelSourceObject(); // Every time there's data, push it into the internal buffer. this._source.ondata = (chunk) =&gt; &#123; // if push() returns false, then stop reading from source if (!this.push(chunk)) this._source.readStop(); &#125;; // When the source ends, push the EOF-signaling `null` chunk this._source.onend = () =&gt; &#123; this.push(null); &#125;; &#125; // _read will be called when the stream wants to pull more data in // the advisory size argument is ignored in this case. _read(size) &#123; this._source.readStart(); &#125;&#125; 备注： readable.push()方法仅限于在readable接口的实现内部使用，并且只限于在readable._read()方法中使用. 读取时发生错误当readable._read()方法处理中发生错误时，建议触发error事件来发布错误，而不是直接在方法中抛出error异常. 直接抛弃error异常的后续处理取决于可读流被使用的方式，这有可能会导致程序的行为不一致，而通过触发error事件的方式来发布错误，则可以使错误有统一的处理方式. 1234567891011const Readable = require('stream').Readable;const myReadable = new Readable(&#123; read(size) &#123; if (checkSomeErrorCondition()) &#123; process.nextTick(() =&gt; this.emit('error', err)); return; &#125; // do some work &#125;&#125;); Readable接口的实现示例以下的例子是Readable接口示例实现，它会以逆序的方式推入1~1000000数字，然后结束. 1234567891011121314151617181920const Readable = require('stream').Readable;class Counter extends Readable &#123; constructor(opt) &#123; super(opt); this._max = 1000000; this._index = 1; &#125; _read() &#123; var i = this._index++; if (i &gt; this._max) this.push(null); else &#123; var str = '' + i; var buf = Buffer.from(str, 'ascii'); this.push(buf); &#125; &#125;&#125; Duplex接口的实现Duplex接口是同时实现了Readable接口和Writable接口的对象，它代表了可读写流，如TCP Socket对象等. 由于JS中不支持多重继承，所有实现可读写流的类都需要实现stream.Duplex接口，而不是同时继承stream.Readable接口和stream.Writable接口. 备注：stream.Duplex接口从原型上继承了stream.Readable接口，但是依赖于stream.Writable接口. 但是instanceof方法可以正确地处理两个基类，因为它重置了Writable接口的Symbol.hasInstance方法. 自定义Duplex接口，必须先调用new stream.Duplex([options])方法，然后重写readable._read()方法和writable._write()方法. new stream.Duplex([options]) options: Object 该对象将被同时传递给Reabable接口和Writable接口的构造函数 allowHalfOpen: Boolean 标识当前的流对象是否可以处于”半开”状态， 默认为true. 如果被设置为false, 意味着当读取数据侧被关闭时，写入数据侧也将被关闭，反之亦然. readableObjectMode: Boolean 用于设置读取数据侧的objectMode, 默认为false. 当objectMode被设置为true时，该参数不起作用. writableObjectMode: Boolean 用于设置写入数据侧的objectMode, 默认为false. 当objectMode被设置为true时，该参数不起作用. 例如： 123456789101112131415161718192021222324252627282930const Duplex = require('stream').Duplex;class MyDuplex extends Duplex &#123; constructor(options) &#123; super(options); &#125;&#125;//或者使用ES6风格的构造器const Duplex = require('stream').Duplex;const util = require('util');function MyDuplex(options) &#123; if (!(this instanceof MyDuplex)) return new MyDuplex(options); Duplex.call(this, options);&#125;util.inherits(MyDuplex, Duplex);//或者使用简化的风格const Duplex = require('stream').Duplex;const myDuplex = new Duplex(&#123; read(size) &#123; // ... &#125;, write(chunk, encoding, callback) &#123; // ... &#125;&#125;); Duplex接口的实现示例以下的例子展示了Duplex接口的实现，它包装了虚拟的底层资源对象，尽管该资源对象使用了与node.js流不兼容的API, 但它可以用于读写数据. 以下的例子是Duplex接口的简单实现，它通过Readable接口读取数据，然后再通过Writable接口缓冲数据. 1234567891011121314151617181920212223const Duplex = require('stream').Duplex;const kSource = Symbol('source');class MyDuplex extends Duplex &#123; constructor(source, options) &#123; super(options); this[kSource] = source; &#125; _write(chunk, encoding, callback) &#123; // The underlying source only deals with strings if (Buffer.isBuffer(chunk)) chunk = chunk.toString(); this[kSource].writeSomeData(chunk); callback(); &#125; _read(size) &#123; this[kSource].fetchSomeData(size, (data, encoding) =&gt; &#123; this.push(Buffer.from(data, encoding)); &#125;); &#125;&#125; Duplex接口最重要的一点是，虽然它同时实现了Writable接口和Readable接口，但是读取数据和写入数据是相互独立的. “对象模式”的Duplex对象对于Duplex接口而言，可以通过readableObjectMode和writableObjectMode两个参数分别设置读取侧和写入侧的objectMode参数. 在下面的例子中，创建了一个Transform对象(Duplex的实现)，它的写入侧处于“对象模式”, 它接受JS的数字类型，并在读取侧转化成16进制的字符串. 123456789101112131415161718192021222324252627const Transform = require('stream').Transform;// All Transform streams are also Duplex Streamsconst myTransform = new Transform(&#123; writableObjectMode: true, transform(chunk, encoding, callback) &#123; // Coerce the chunk to a number if necessary chunk |= 0; // Transform the chunk into something else. const data = chunk.toString(16); // Push the data onto the readable queue. callback(null, '0'.repeat(data.length % 2) + data); &#125;&#125;);myTransform.setEncoding('ascii');myTransform.on('data', (chunk) =&gt; console.log(chunk));myTransform.write(1);// Prints: 01myTransform.write(10);// Prints: 0amyTransform.write(100);// Prints: 64 Transform接口的实现Transform接口是Duplex接口的特例，它的输出是根据输入的数据来决定的. Transform接口的例子包括zlib流对象和crypto流对象，它们分别用于压缩数据和加密数据时使用. 备注： Transform接口并不要求输出的数据大小等同于输入的数据大小，也不要求数据块的数量是一样在的，甚至连数据读入和写出的时间也可以是不一样的. 例如， 哈希转换流只会在输入数据结束时，才会输出单个数据块， 而zlib流产生的输出数据要么远小于输入数据（压缩），要么远大于输入数据（解压缩）. 所有实现转换流的对象都必须实现stream.Transform接口. stream.Transform接口原型上继承自stream.Duplex接口，但它重写了writable._write()方法和readable._read()方法，自定义的Transform接口的实现必须实现tranform._tranform()方法，也可以按照需要实现transform._flush()方法. 备注： 如果在Transform接口的实现中，由于读取侧的数据没有被及时而导致写入侧被暂停时，那么必须十分小心. new stream.Transform([options]) options: Object 该对象将同时被传到Writable接口和Readable接口的构造函数中 transform: Function stream._transform()方法的实现函数 flush: Function stream._flush()方法的实现函数 例如： 123456789101112131415161718192021222324252627const Transform = require('stream').Transform;class MyTransform extends Transform &#123; constructor(options) &#123; super(options); &#125;&#125;//或者使用ES6风格的构造器const Transform = require('stream').Transform;const util = require('util');function MyTransform(options) &#123; if (!(this instanceof MyTransform)) return new MyTransform(options); Transform.call(this, options);&#125;util.inherits(MyTransform, Transform);//或者使用简化的构造器const Transform = require('stream').Transform;const myTransform = new Transform(&#123; transform(chunk, encoding, callback) &#123; // ... &#125;&#125;); finish事件和end事件finish事件和end事件是分别来自于stream.Writable接口和stream.Readable接口. finish事件是在stream.end()方法被调用后，所有的数据块都被stream._transform()方法处理后被触发， 而end事件是在transform._flush()方法的回调函数被调用后，所有的数据都已经输出时才被触发. transform._flush(callback) callback: Function 当剩余的数据被flush的时候，回调函数被调用，该函数被调用 时，有可能会带上Error参数. 备注： 该函数仅在实现类内部使用，不能在程序代码中直接调用该方法. 在某些场景下，转换操作必须在流的尾部增加一些数据，例如， 在zlib流进行压缩操作时，会存储一些内部状态以优化整个压缩过程. 但是在流数据到达尾部时，这些数据也需要被flush，这样整个压缩数据才能完整. 自定义的Transform实现也可以按照需要实现transform._flush()方法. 这个方法会在没有数据消费时被调用 ，但会在end事件被触发前调用. 在transform._flush()方法内部，readable.push()方法可能会被调用零次或多次, 而callback函数会在所有的数据都已经被flush后被调用. transform._flush()方法名增加了下划线作为前缀，这是用来标识该方法仅限在实现类内部使用，使用Transform API的程序不应该直接使用该方法. transform._transform(chunk, encoding, callback) chunk: Buffer | String 要被转换的数据块. 除非将decodeStrings标识设置为false, 否则将永远是buffer类型. encdoing: String 如果chunk是字符串类型，那么该参数标识了chunk的编码格式； 如果chunk是buffer类型，那么该参数被设置为“buffer”， 在这种情况下，忽略该参数 callback: Function 在完成chunk数据的转换操作后的回调函数，如果在转换的过程中出现错误 ，那么回调函数的第一个参数代表了Error类型 备注： transform._transform()方法仅限在实现类内部使用，使用Transform接口API的程序不能直接使用该方法. 所有Transform接口的实现类都必须实现transform._transform()方法，它接受写入侧的数据，并根据写入侧的数据转换成读取侧的数据，并通过readable.push()方法将转换后的数据提交给读取侧. 对于单次的输入数据块来讲，transform.push()方法可能会被调用零次或多次，这取决于具体的实现，甚至对于任何的输入数据都可以没有输出. callback回调方法只能在当前块数据被读取后才能被调用，如果在转化的过程中出现了错误，那么callback回调方法的第一个参数必须是Error对象，否则这个参数设置为null. 如果提供了callback函数的第二个参数，那么这个参数将会被传递给readable.push()方法，换句话说，下面的表达式是等价的： 12345678transform.prototype._transform = function (data, encoding, callback) &#123; this.push(data); callback();&#125;;transform.prototype._transform = function (data, encoding, callback) &#123; callback(null, data);&#125;; transform._transform()方法增加了下划线作为前缀，这是用来标识该方法仅限在实现类内部使用，使用Transform API的程序不能直接使用该方法. stream.PassThrough接口stream.PassThrough接口是Transform接口的实现，它不对数据做任何转换，只是简单地将输入数据作为输出. 这个接口的主要目的是为了测试，但是在一些场景下，也可以使用这个接口来作为新实现的流对象的基础. 额外说明与老版本node.js的兼容性在node.js的v0.10版本之前，Readable接口被设计得很简单，因此功能也相对简单： 当有可读数据时，data事件会被直接触发，而不是等待stream.read()方法被调用. 这对于那些需要做些额外的工作来决定如何处理数据的应用来讲，需要将读取到的数据暂存到缓存中，否则数据将丢失. stream.pause()方法是建议性的，而非强制性的. 这意味着，即使当前流对象处于paused模式，但需要继续监听data事件，因为还是会有数据被推过来 在node.js的V0.10版本的时候引入了Readable接口. 为了向后兼容，当增加了可读流的data事件监听器后，或者调用了resume()方法后，可读流才会被切换到flowing模式. 这样做的效果就是，即使没有使用新的stream.read()方法，或者设置readabale事件，数据块也不会丢失了. 虽然大部分程序都能够正常的工作，但是在特定的场景下，这也引入了一个问题： 没有增加data事件的监听器 stream.resume()方法也从来没有被调用 可读流对象也没有被pipe到任何可写流对象上 例如： 12345678910// WARNING! BROKEN!net.createServer((socket) =&gt; &#123; // we add an 'end' method, but never consume the data socket.on('end', () =&gt; &#123; // It will never get here. socket.end('The message was received but was not processed.\\n'); &#125;);&#125;).listen(1337); 在v0.10版本之前的node.js中,进来的数据会被忽略. 但是在v0.10以及之后的版本中，这个socket对象就会永远被暂停. 对于这种情况，解决方式就是调用stream.resume()方法. 1234567891011// Workaroundnet.createServer((socket) =&gt; &#123; socket.on('end', () =&gt; &#123; socket.end('The message was received but was not processed.\\n'); &#125;); // start the flow of data, discarding it. socket.resume();&#125;).listen(1337); 除了让可读流对象切换到flowing模式之外，还可以使用readable.wrap()方法对V0.10版本前的可读流对象进行包装. readable.read(0)在某些场景，需要刷新底层的可读流资源，但又不想真实地消费数据，在这些场景中，可以调用readable.read(0)方法，它会始终返回null. 如果内部的数据缓冲区在highWaterMark值以下，并且可读流对象没有正在被读取，那么调用readable.read(0)方法将会导致底层的readable._read()方法被调用. 虽然对于大部分的应用来讲并不需要使用这个方法，但是在node.js内部使用了这种机制，是在可读流对象的内部. readable.push(‘’)不建议使用readable.push()方法. 对于不处于“对象模式”的流来讲，将零字节的string和buffer推入其中会导致很有意思的现象. 由于调用了readable.push()方法，这个方法会中止读取操作，但是，由于传入的参数是个空字符串，没有数据被添加到缓冲区中，因此应用程序就读不到任何数据.","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"stream.Readable","slug":"stream-Readable","permalink":"http://yoursite.com/tags/stream-Readable/"},{"name":"stream.Duplex","slug":"stream-Duplex","permalink":"http://yoursite.com/tags/stream-Duplex/"},{"name":"stream.Transform","slug":"stream-Transform","permalink":"http://yoursite.com/tags/stream-Transform/"}]},{"title":"Node.js — Stream(2)","slug":"前端/nodejs/Nodejs.stream.2","date":"2017-03-14T01:30:00.000Z","updated":"2017-03-15T02:56:40.000Z","comments":true,"path":"2017/03/14/前端/nodejs/Nodejs.stream.2/","link":"","permalink":"http://yoursite.com/2017/03/14/前端/nodejs/Nodejs.stream.2/","excerpt":"","text":"可读流(Readable Stream)可读流是对于可读取数据的源目标的抽象. 它的实例包括： 客户端的http响应对象 服务端的http请求对象 fs文件流对象 zlib对象 crypto对象 TCP socket对象 子进程的stdout和stderr对象 process.stdin对象 所有的可读流对象实现了stream.Readable接口定义的方法. 两种模式可读流可工作在两种模式：flowing模式和paused模式. 当处于flowing工作模式的时候，当有数据到达底层系统时，这些数据自动被读取，应用程序可以通过EventEmitter接口的事件即时从流中获取这些数据. 而当处于paused模式的时候，则必须显式地调用stream.read()方法从可读流中获取数据. 所有的可读流对象在创建时都处于paused模式，后续可以通过以下几种方式转换成flowing模式: 增加data事件监听器 调用stream.resume()方法 调用stream.pipe()方法，将数据传递给Writable对象 另外，Readable对象可以通过以下两种方法从flowing模式转到paused模式: 当没有pipe目标时，可以通过调用stream.paused()方法 当有pipe目标时，必须移除data事件的监听器，然后调用unpipe()方法将所有的writable目标移除. 很重要的一点是，在设置消费数据或废弃数据的方法之前，Readable对象不会产生数据. 如果之前已经设置过消费数据的方法，但后续的操作把这个方法废弃了或移除了，那么Readable会尝试终止继续产生数据. 注意：为了向后兼容，移除data事件并不会自动将可读流对象转化成paused模式. 另外，如果可读流上有pipe目标，那么调用stream.pause()方法也无法保证可读流处于paused模式，因为pipe目标会可以在drain事件中继续请求数据. 另外，如果Readable对象在转化成flowing模式后，却没有相应的消费者可以处理数据，那么这些数据将会丢失. 这种情况会发生在没有data事件处理器时，在可读流上调用了stream.resume()方法，或者在调用了stream.resume()方法后，data事件处理器被移除了. 三种状态上述的两种模式是对Readable实现内部复杂状态的抽象概括. 具体来讲，在任何时刻，Readable对象处于以下三种状态之一： readable._readableState.flowing = null readable._readableState.flowing = true readable._readableState.flowing = false 当readable._readableState.flowing处于null状态时，表明当前readable没有设置消费方法，因此readable在这种状态下，不会产生数据. 如果增加了data事件的监听器，或者调用了readable.pipe()方法，或者调用了readable.resume()，会导致readable._readableState.flowing的值为true，这会导致Readable不断地触发数据事件. 如果调用了readable.pause()方法，或者调用了readable.unpipe()方法，或者收到了后端压力过大的反馈时，readable._readableState.flowing会被设置成false, 此时，readable会暂停产生数据事件，但并不会停止产生数据. 当这个标识为false时，数据会在readable的内部缓冲区中产生堆积. 选择一种方式可读流的API涉及node.js的多个版本，同时也提供了多种消费数据的方法. 总的来讲，开发者应该选择一种方式来消费数据，避免在同一个流对象中使用多种方式来消费. 使用readable.pipe()方法来消费数据是比较推荐的方法，也是最简单的方式. 开发者如果需要对数据的产生和转换进行更细致的控制，可以通过EventEmitter提供的事件以及readable.pause()/readable.resume()方法来实现. stream.Readableclose事件close事件是在流对象被关闭，或者流对象的底层资源被关闭时被触发.（如文件描述符,fd)这个事件意味着后续不再会有事件产生，也不会再有新的计算发生.但是，值得一提的是，并不是所有的Readable对象都会产生close事件. data事件 chunk: Buffer | String | any 数据块. 对于不是工作于“对象模式”的流而言，chunk的类型是string或者是Buffer. 而对于处于“对象模式”的流而言，chunk可以是任意有效的js类型，但是不能是null 当流对象把数据块移交给消费者的时候，会触发data事件. data事件在以下两种情况下会被触发： 流对象通过调用readable.resume(), readable.pipe()以及设置data事件的监听器进入flowing模式后，可能、随时被触发 调用readable.read()方法后会被触发 除非显式地调用了readable.paused()方法，否则在stream上设置data事件监听器会使可读流对象进入flowing模式. 后续当有数据处于可读状态时，会即时触发data事件. 如果通过readable.setEncoding()方法设置了可读流对象的编码方式，那么在data事件的监听器回调方法中，chunk数据块将会按照设置的编码方式设置成字符串类型，否则就是buffer类型. 1234const readable = getReadableStreamSomehow();readable.on('data', (chunk) =&gt; &#123; console.log(`Received $&#123;chunk.length&#125; bytes of data.`);&#125;); end事件当可读流中不再有数据产生和消费时，就会触发end事件. 注意：除非可读流中的数据已经被全部消费完，否则不会触发end事件. 可以将可读流对象切换到flowing模式，或者在paused模式下，不断地调用stream.read()方法，直到全部数据被消费完，以此来触发end事件. 1234567const readable = getReadableStreamSomehow();readable.on('data', (chunk) =&gt; &#123; console.log(`Received $&#123;chunk.length&#125; bytes of data.`);&#125;);readable.on('end', () =&gt; &#123; console.log('There will be no more data.');&#125;); error事件 ： 错误对象 任何时候，当可读流对象内部发生错误时都会触发error事件. 通常来讲，error事件的发生的情况有以下这些情况： 底层系统内部发生错误，导致可读流无法继续读取数据 当可读流尝试将无效的数据块推送给消费者时 error事件的监听器回调方法会带有一个error参数，这个参数中带有详细的错误信息. readable事件当可读流对象中有数据可以被读取的时候，就会触发readable事件. 在有些情况下， 设置readable事件的监听器会导致一些数据被读入到内部的数据缓冲区中. 另外，在可读流数据到达尾部，但在end事件被触发之前，也会触发readable事件. 1234const readable = getReadableStreamSomehow();readable.on('readable', () =&gt; &#123; // there is some data to read now&#125;); 通常情况下，当readable事件被触发时，意味着可读流中有新的信息产生：要么是有新的数据到达，要么是可读流中的数据到达了尾部. 在前一种情况下， stream.read()方法将返回有效的数据块. 而在后面一种情况下， stream.read()方法将返回null. 例如，在下面的例子中，foo.txt是个空的文件： 123456789101112const fs = require('fs');const rr = fs.createReadStream('foo.txt');rr.on('readable', () =&gt; &#123; console.log('readable:', rr.read());&#125;);rr.on('end', () =&gt; &#123; console.log('end');&#125;);$ node test.jsreadable: nullend 备注：通常情况下，优先考虑使用readable.pipe()方法以及设置data事件监听器的方式来读取数据. readable.isPaused()该方法将返回当前readable对象的readable状态. 这个方法通常是在readable.pipe()方法内部来使用的，因此，在实际的使用中，基本上不需要直接调用该方法. 1234567const readable = new stream.Readablereadable.isPaused() // === falsereadable.pause()readable.isPaused() // === truereadable.resume()readable.isPaused() // === false readable.pause() 返回: this readable.pause()方法会将可读流对象切换到paused模式，并停止继续触发data事件. 后续到达的可读数据将被存储在可读流的内部缓冲区中. 12345678910const readable = getReadableStreamSomehow();readable.on('data', (chunk) =&gt; &#123; console.log(`Received $&#123;chunk.length&#125; bytes of data.`); readable.pause(); console.log('There will be no additional data for 1 second.'); setTimeout(() =&gt; &#123; console.log('Now data will start flowing again.'); readable.resume(); &#125;, 1000);&#125;); readable.pipe(destination[,options]) destination: stream.Writable 用于写入数据的目标位置 ，可写流 options: Object pipe选项参数 end Boolean 标识当readable流关闭时是否自动关闭关联的writable流，默认为true 返回: 目标流对象的引用 readable.pipe()方法将writable对象关联到当前的readable对象，这个操作会自动将readable对象切换到flowing模式，并且把后续所有可读的数据自动推到writable对象. 数据在readable对象和writable对象之间的流转是自动管理的，因此这种方式可以避免读写对象的速度不一致的问题. 以下的例子展示了将readable对象中的数据存储到file.txt文件中： 1234const readable = getReadableStreamSomehow();const writable = fs.createWriteStream('file.txt');// All the data from readable goes into 'file.txt'readable.pipe(writable); 可以多次调用这个方法将同一个readable对象关联到多个writable对象. 另外，readable.pipe()方法返回目标可写流对象的引用，以此来方便链式方法的调用. 1234const r = fs.createReadStream('file.txt');const z = zlib.createGzip();const w = fs.createWriteStream('file.txt.gz');r.pipe(z).pipe(w); 默认情况下，当可读流对象触发end事件时，也会同时触发目标可写流对象的end事件，这意味着可写流对象将无法被继续写入. 如果要禁止这种默认的行为，可以在pipe()方法的第二个参数中设置end参数为false, 这样就可以在可读流对象触发end事件后，可以继续往可写流对象中写入数据. 1234reader.pipe(writer, &#123; end: false &#125;);reader.on('end', () =&gt; &#123; writer.end('Goodbye\\n');&#125;); 但这样做有个很重要的缺点就是，当readable流对象中处理发生错误，从而触发了error事件后，writable流对象不会被自动关闭，此时必须手动关闭writable对象，否则会发生内存泄露.备注： process.stderr和process.stdout对象会忽略end参数，它们总是在node.js进程退出的时候被关闭. readable.read([size]) size: 可选参数，指定读取多少数据. 返回: String | Buffer | null readable.read()方法会从可读流对象的缓冲区中读取部分数据并将它们返回. 如果调用该方法的时候，缓冲区中没有数据，那么就返回null. 默认情况下，读取到的数据是以buffer对象的形式返回，但是如果设置了编码格式，那么返回的将是字符串格式. 如果readable对象处于“对象模式”，那么调用read()方法会忽略size参数，而总是会返回单个对象. 可选的size参数指定了要读取的字节数. 如果没有指定size参数，那么缓冲区中所有的数据都将被返回. 另外，readable.read()方法只应该在readable对象处于paused模式时被调用. 在flowing模式下，只要缓冲区还有数据，readable.read()方法就会被自动调用. 1234567const readable = getReadableStreamSomehow();readable.on('readable', () =&gt; &#123; var chunk; while (null !== (chunk = readable.read())) &#123; console.log(`Received $&#123;chunk.length&#125; bytes of data.`); &#125;&#125;); 通常来讲，建议开发者避免直接监听readable事件和调用read()方法，而应该使用pipe()方法和监听data()事件. 值得一提的是，如果调用了readable.read()方法，并且成功返回了数据，那么readable对象的data事件也会被触发. 如果在end事件被触发后调用read()方法，那么该方法将返回null，并且不会产生运行时错误. readable.resume() 返回: this 调用readable.resume()方法将使可读流对象切换到flowing，从而继续触发data事件. 该方法可用于消费可读流中的全部数据，但又不需要处理数据的场景，如下所示： 12345getReadableStreamSomehow() .resume() .on('end', () =&gt; &#123; console.log('Reached the end, but did not read anything.'); &#125;); readable.setEncoding(encoding) encoding: String 设置该可读流对象的编码格式 返回: this readable.setEncoding()方法设置了可读流对象默认的字符串编码格式. 设置完编码格式后，可读流对象后续的数据将会以该格式编码的字符串的形式返回，而不是buffer对象. 例如，当调用完readable.setEncoding(‘utf-8’)后，后续的数据都会以utf-8的格式进行编码； 如果调用了readable.setEncoding(“hex”)方法，那么后续的数据会以16进制的字符串形式返回. 如果需要移除设置的编码格式，可以通过调用readable.setEncoding(null)来设置. 这种方式在需要直接操作二进制数据，或者需要操作的大块数据分散到多个chunk时非常有用. 123456const readable = getReadableStreamSomehow();readable.setEncoding('utf8');readable.on('data', (chunk) =&gt; &#123; assert.equal(typeof chunk, 'string'); console.log('got %d characters of string data', chunk.length);&#125;); readable.unpipe([destionation]) destination: stream.Writable 可选的目标可写流对象 readable.unpipe()方法将指定的目标可写流对象与当前的可读流对象的pipe关系移除. 如果没有指定destionation参数，那么所有与当前可读流对象关联的可写流对象将被移除. 如果设置的destination目标并没有当前可读流对象关联，那么调用该方法不会执行任何操作. 1234567891011const readable = getReadableStreamSomehow();const writable = fs.createWriteStream('file.txt');// All the data from readable goes into 'file.txt',// but only for the first secondreadable.pipe(writable);setTimeout(() =&gt; &#123; console.log('Stop writing to file.txt'); readable.unpipe(writable); console.log('Manually close the file stream'); writable.end();&#125;, 1000); readable.unshift(chunk) chunk: String | Buffer 需要unshift到读队列中的数据块 readable.unshift()方法将数据块重新推回到可读流对象的内部缓冲区中. 这在有些场景下十分有用，例如有时候应用程序可能需要“预先”消费部分数据来决定后续的处理逻辑，但后续又需要把这部分数据重新推回到缓冲区中. 但是，unshift()方法不能在end事件被触发后被调用，否则将抛出运行时异常. 开发者在使用readable.unshift()方法时，通常也要考虑下能否使用Transfrom接口来代替. 关于Transfrom接口的具体内容，可以看“Transfrom接口的API”一节. 123456789101112131415161718192021222324252627282930313233// Pull off a header delimited by \\n\\n// use unshift() if we get too much// Call the callback with (error, header, stream)const StringDecoder = require(&apos;string_decoder&apos;).StringDecoder;function parseHeader(stream, callback) &#123; stream.on(&apos;error&apos;, callback); stream.on(&apos;readable&apos;, onReadable); const decoder = new StringDecoder(&apos;utf8&apos;); var header = &apos;&apos;; function onReadable() &#123; var chunk; while (null !== (chunk = stream.read())) &#123; var str = decoder.write(chunk); if (str.match(/\\n\\n/)) &#123; // found the header boundary var split = str.split(/\\n\\n/); header += split.shift(); const remaining = split.join(&apos;\\n\\n&apos;); const buf = Buffer.from(remaining, &apos;utf8&apos;); stream.removeListener(&apos;error&apos;, callback); // set the readable listener before unshifting stream.removeListener(&apos;readable&apos;, onReadable); if (buf.length) stream.unshift(buf); // now the body of the message can be read from the stream. callback(null, header, stream); &#125; else &#123; // still reading the header. header += str; &#125; &#125; &#125;&#125; 与stream.push(chunk)方法不同的是，unshift()方法并不会改变readable对象内部的读状态，从而打断整个读的过程. 如果在read方法实现的过程中（也就是在自定义实现stream._read()方法的过程中）调用了unshift()方法，将会导致非预期的结果. 如果在调用了unshift()方法后，立即再调用push()方法，可以将readable对象的内部读状态恢复正常，但是总的来讲，应该避免在stream._read()方法中调用unshift()方法. readable.wrap(stream) stream: Stream 旧”风格”的可读流对象 在V0.10版本的node.js之前，stream模块中的一些流对象并没有实现目前版本中定义的API接口（具体可以看“兼容性”一节）当使用老版本的node.js库时，流对象只会触发data事件，并且只定义了stream.pause()方法，在这种情况下，可以使用readable.wrap()方法将老版本的stream对象进行包装. 通常情况下并不需要用到这个方法，但提供这个方法可以更方便地使用老版本的node.js和函数库. 例如： 12345678const OldReader = require('./old-api-module.js').OldReader;const Readable = require('stream').Readable;const oreader = new OldReader;const myReader = new Readable().wrap(oreader);myReader.on('readable', () =&gt; &#123; myReader.read(); // etc.&#125;); Duplex和Transfrom流stream.Duplex流Duplex流同时实现了Writable接口和Readable接口，它是可读写流. Duplex流的实现包括： TCP Socket zlib流 crypto流 stream.Transform流Transform流是Duplex流的特例，它的输出是对输入进行了某种“变换”后得到的（这也是Transform的由来）. 和所有的Duplex流对象一样，Transform实例也是同时实现了Writable接口和Readable接口. Transfrom流的实例包括： zlib流 crypto流 参考文献 官方文档","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"stream.Readable","slug":"stream-Readable","permalink":"http://yoursite.com/tags/stream-Readable/"},{"name":"stream.Duplex","slug":"stream-Duplex","permalink":"http://yoursite.com/tags/stream-Duplex/"},{"name":"stream.Transform","slug":"stream-Transform","permalink":"http://yoursite.com/tags/stream-Transform/"}]},{"title":"Node.js — Stream(1)","slug":"前端/nodejs/Nodejs.stream.1","date":"2017-03-14T00:30:00.000Z","updated":"2017-03-14T03:18:24.000Z","comments":true,"path":"2017/03/14/前端/nodejs/Nodejs.stream.1/","link":"","permalink":"http://yoursite.com/2017/03/14/前端/nodejs/Nodejs.stream.1/","excerpt":"","text":"流在node.js中，流对象是用来操作流数据的抽象接口. Stream模块提供了基础的API接口，以方便应用程序实现相应的接口来构建对象.Node.js提供了多种流对象，比如说，httpServer中request对象，以及process中的标准输出对象stdout，都属于流接口的实现. 流对象可以是只读的，只写的，也可以是支持读写的. 所有的流对象都是EventEmitter的实例.Stream模块可以通过以下代码进行访问: 1var stream = require(\"stream\"); 虽然对于所有的node.js用户而言，理解流是如何工作是非常重要的，但是stream模块本身是为那些意在创建新类型的流对象的开发者设计的. 对于那些只需要使用流对象的开发者而言，通常情况下并不需要直接使用stream模块. 文档的组织这篇文档可以分成两个基础章节，另外还有一个章节用于备注一些细节. 第一小节中详述了在应用中使用stream模块需要用到的api， 第二小节主要阐述实现新的stream对象所需要用到的API接口. 流对象的类型Node.js中提供了四种基础的流类型： 只读: 用于读取流数据（如fs.createReadStream()) 只写: 用于写入流数据（如fs.createWriteStream()) 可读写: 可同时用于读和写(如net.socket) 转换流: 这也是一种可读写流，但是它在读写操作的时候，可以对流数据进行一些变化(如zlib.createDeflate()) 对象模式所有由node.js的api创建的流对象都可以直接操作String类型和Buffer对象. 但是，流对象也可以用于直接操作js中的其它类型（除了null, 在流中null有特殊的含义）这种直接操作其它类型对象的流被称为是处于“对象模式”（Object Mode). 流对象在创建时可以通过设置objectMode参数来进入“对象模式”，但是尝试将已有的流对象转化成“对象模式”不是个安全的操作. 缓冲区可写流(Writable)和可读流(Readable)在存储数据时，会分别在内部通过writable._writeableState.getBuffer()和readable._readbleState.buffer来缓冲数据. 缓冲区的大小取决于构造流对象时传入的highWaterMark参数值. 对于一般的流对象来讲，highWaterMark参数定义是缓冲区的字节大小，但对于处在“对象模式”的流对象来讲，highWaterMark参数定义是缓冲的对象个数. 当可读流对象调用stream.push(chunk)方法时，数据就会被存储于缓冲区中，如果流的消费者没有调用 stream.read()方法，那么数据将一直存在内部的队列中，直到被消费为止. 一旦缓冲的数据大小达到了highWaterMark参数设置的值，可读流对象就会暂时停止从底层的资源中读取数据，直到缓冲区中的数据被消费了为止.（也就是说，可读流对象将停止调用内部的readable._read()方法来填充缓冲区） 可写流对象是通过不断调用writable.write(chunk)方法来缓冲数据的，当内部缓冲的数据没有超过highWaterMark参数设置的上限值时，调用writable.write()方法将返回true,一旦缓冲区里的数据大小超过了highWaterMark设置的值时，调用writable.write()方法将返回false. Stream模块中API设计的主要目标，尤其是stream.pipe()方法，是为了将缓冲区的数据量限制在可接受的水平，这样当源和目标对象的处理速度有差异时，不至于占用所有可用的内存. 由于Duplex和Transform类型的流对象既是可读的，也是可写的，因此它们内部都维护了两个缓冲区，分别用于读和写操作，以此来保证读写操作相互不受影响. 例如，net.socket对象是Duplex类的实例，它的读缓冲区里缓冲了从socket中接收到的数据，同时写缓冲区里缓冲了往socket里写的数据. 因为往socket中写数据的速度与读数据的速度存在差异，所以有必要为两种操作分别使用相应的缓冲区. 使用流的API几乎所有的Node.js程序，不论它多么简单，都会以某种方式使用流对象. 以下是一个在node.js中使用流对象的例子， 123456789101112131415161718192021222324252627282930313233343536373839const http = require('http');const server = http.createServer( (req, res) =&gt; &#123; // req is an http.IncomingMessage, which is a Readable Stream // res is an http.ServerResponse, which is a Writable Stream let body = ''; // Get the data as utf8 strings. // If an encoding is not set, Buffer objects will be received. req.setEncoding('utf8'); // Readable streams emit 'data' events once a listener is added req.on('data', (chunk) =&gt; &#123; body += chunk; &#125;); // the end event indicates that the entire body has been received req.on('end', () =&gt; &#123; try &#123; const data = JSON.parse(body); // write back something interesting to the user: res.write(typeof data); res.end(); &#125; catch (er) &#123; // uh oh! bad json! res.statusCode = 400; return res.end(`error: $&#123;er.message&#125;`); &#125; &#125;);&#125;);server.listen(1337);// $ curl localhost:1337 -d '&#123;&#125;'// object// $ curl localhost:1337 -d '\"foo\"'// string// $ curl localhost:1337 -d 'not json'// error: Unexpected token o 可写流(Writable)对象，如res对象，对外提供了write()方法以及end()方法来将数据写入流对象. 可读流（Readable）对象使用EventEmitter对象的事件机制来从流中读取数据. 可读流中的数据可以以多种方式来读取. 不管是可写流，还是可读流，都使用EventEmitter类的消息机制来判断当前流的状态. Duplex和Tranform流是可读写流的两种实现. 应用程序如果只是要从流中读取数据，或者是往流中写入数据，那么它不需要直接使用流接口，因此也就没必要通过require(“stream”)方法引用stream模块. 开发者若是想要实现新的流类型，可参阅“流接口的实现API”一节. 可写流（Writable Stream)可写流是对可写入数据的“目标”的一种抽象. 可写流的例子包括： 客户端的Http请求对象 服务端的Http响应对象 fs文件流对象 zlib流对象 crypto流对象 TCP socket对象 子进程stdin对象 process.stdout, process.stderr 注意： 以上的例子中，有些对象其实是Duplex流对象 所有可写流对象实现了stream.writable接口定义的方法. 虽然说不同的可写流可能会有些区别，但是它们都遵循基本的使用模式，如下所示： 1234const myStream = getWritableStreamSomehow();myStream.write('some data');myStream.write('some more data');myStream.end('done writing data'); stream.Writable类close事件close事件是当可写流对象被关闭时，或者其底层的资源被关闭时（如文件描述符,fd)被触发. 这个事件意味着后续不再会有其它的事件被触发，也不会再有新的数据产生. 但是，并不是所有的可写流对象都会触发close事件. drain事件当调用stream.write(chunk)返回false时，意味着缓冲区里的数据已经到达了highWaterMark设置的上限值，后续如果缓冲区里的数据被消费了，缓冲区将可以继续接收新的数据，这时候就会触发drain事件. 示例如下： 12345678910111213141516171819202122232425// Write the data to the supplied writable stream one million times.// Be attentive to back-pressure.function writeOneMillionTimes(writer, data, encoding, callback) &#123; let i = 1000000; write(); function write() &#123; var ok = true; do &#123; i--; if (i === 0) &#123; // last time! writer.write(data, encoding, callback); &#125; else &#123; // see if we should continue, or wait // don't pass the callback, because we're not done yet. ok = writer.write(data, encoding); &#125; &#125; while (i &gt; 0 &amp;&amp; ok); if (i &gt; 0) &#123; // had to stop early! // write some more once it drains writer.once('drain', write); &#125; &#125;&#125; error事件 ： error对象 当写入数据时(包括pipe)，如果发生错误则会触发error事件. 监听器的回调方法中会包含一个error参数，表示当前的错误信息. 注意：当error事件被触发时，流对象还没有关闭. finish事件在调用完stream.end()方法，并且所有的数据已经被flush到底层系统后，finish事件才会被触发. 12345678const writer = getWritableStreamSomehow();for (var i = 0; i &lt; 100; i ++) &#123; writer.write('hello, #$&#123;i&#125;!\\n');&#125;writer.end('This is the end\\n');writer.on('finish', () =&gt; &#123; console.error('All writes are now complete.');&#125;); pipe事件​ * src: 连接到当前可写流对象的源可读对象 当某个可读流对象调用了stream.pipe()方法，并且把当前可写流对象当成是目标时，会触发当前对象的pipe事件，事件的回调参数中带有src参数，它表示连接到当前可写流对象的源可读对象. 1234567const writer = getWritableStreamSomehow();const reader = getReadableStreamSomehow();writer.on('pipe', (src) =&gt; &#123; console.error('something is piping into the writer'); assert.equal(src, reader);&#125;);reader.pipe(writer); unpipe事件 src: 与当前可写流对象断开连接的源可读流对象 当某个可读流对象调用stream.unpipe()方法，将当前可写流对象从目标中移除中时，会触发unpipe事件，事件的回调函数带有src参数，它表示发起unpipe操作的源可读对象. 12345678const writer = getWritableStreamSomehow();const reader = getReadableStreamSomehow();writer.on('unpipe', (src) =&gt; &#123; console.error('Something has stopped piping into the writer.'); assert.equal(src, reader);&#125;);reader.pipe(writer);reader.unpipe(writer); writable.cork()writable.cork()方法强制将写入的数据放到缓冲区中，这些数据在调用stream.uncork()方法或者stream.end()方法后才会被flush. 这个方法的主要目的是避免在大量写入小块数据到流中时，不会利用内部缓冲从而导致性能急剧下降的问题. 在这种场景下，writable._writev()方法的实现可以通过缓冲区的机制达到更好的性能. writable.end([chunk][,encoding][,callback]) chunk: String | Buffer | any 需写入的数据(可选)，对于不是工作在“对象模式”的流对象而言，chunk必须是String类型或者是buffer类型. 而对于工作在“对象模式”的流来讲，chunk可以是任意的js类型，除了null encoding: String 如果chunk的类型是string, 那么该参数定义了其编码格式 callback： Function 当stream操作完成时的回调函数，可选 调用writable.end()方法意味着后续不再会有数据被写入流对象. 可选的chunk参数和encoding参数可被用于在关闭stream对象前，最后一次写入数据. 如果提供了callback参数，那么它将会被用于监听finish事件. 在调用了stream.end()方法之后，如果再调用stream.write()方法会产生错误. 12345// write 'hello, ' and then end with 'world!'const file = fs.createWriteStream('example.txt');file.write('hello, ');file.end('world!');// writing more now is not allowed! writable.setDefaultEncoding(encoding) encoding: String 默认的编码方式 返回： this 该方法用于设置可写流对象默认的编码方式 writable.uncork()调用该方法会导致之前调用writable.cork()方法缓冲的数据被flush. 当使用writable.cork()和writable.uncork()方法来管理缓冲区的数据时，建议使用process.nextTick()来延迟writable.uncork()方法的调用，这样可以允许node.js在一个事件loop中，批量进行writable.write()操作. 1234stream.cork();stream.write('some ');stream.write('data ');process.nextTick(() =&gt; stream.uncork()); 如果多次调用了writable.cork()方法，那么也必须多次调用writable.uncork()方法才能flush所有的缓冲数据. 123456789stream.cork();stream.write('some ');stream.cork();stream.write('data ');process.nextTick(() =&gt; &#123; stream.uncork(); // The data will not be flushed until uncork() is called a second time. stream.uncork();&#125;); writable.write(chunk[,encoding][,callback]) chunk: String | Buffer 要写入的数据 encoding: String 当chunk参数是string类型时，该参数指定了它的编码格式 callback: Function 当数据被flush时的回调函数 返回: Boolean 如果当前流的缓冲区大小已经达到highWaterMark设置的上限，可写流希望调用代码在drain事件触发时再次写入数据，则会返回false, 否则返回true. writable.write()方法往流中写入数据，并在数据被处理完时调用callback方法. 如果在写入的时候发生错误， 那么回调方法有可能不会被触发，也有可能会被触发，此时error会作为它的第一个参数. 为了更有效的处理写操作中可能发生的错误，可以监听error事件. 当内部的缓冲区存储的数据不超过highWaterMark值时，该方法会返回true. 如果方法返回了false, 那么后续的写操作应该暂停，直到drain事件被触发. 但是，false返回值只是建议，即使在drain事件被触发之前，可写流对象还是会无条件的接收后续写入的数据. 另外，处于“对象模式”的可流读对象会忽略encoding参数.","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"stream.Writable","slug":"stream-Writable","permalink":"http://yoursite.com/tags/stream-Writable/"}]},{"title":"nodejs — debugger","slug":"前端/nodejs/Nodejs.debugger","date":"2017-03-13T04:11:00.000Z","updated":"2017-03-13T05:00:28.000Z","comments":true,"path":"2017/03/13/前端/nodejs/Nodejs.debugger/","link":"","permalink":"http://yoursite.com/2017/03/13/前端/nodejs/Nodejs.debugger/","excerpt":"","text":"DebuggerNode.js通过TCP协议提供了内置的调试客户端. 可以通过在node命令后增加debug参数来启动调试, 在启动成功后，命令行中会有相应的提示信息，如下： 12345678patrick-macbook:nodejs $ node debug test&lt; Debugger listening on [::]:5858connecting to 127.0.0.1:5858 ... okbreak in test.js:1&gt; 1 module.exports = exports = function(a, b)&#123; 2 return a + b; 3 &#125;debug&gt; Node.js提供的调试客户端只提供了基本的调试功能. 在代码中增加debugger;声明，在代码执行的时候，将会在那行中增加断点: 12345678910111213141516171819202122232425262728293031323334353637$ node debug myscript.js&lt; debugger listening on port 5858connecting... okbreak in /home/indutny/Code/git/indutny/myscript.js:1 1 x = 5; 2 setTimeout(() =&gt; &#123; 3 debugger;debug&gt; cont&lt; hellobreak in /home/indutny/Code/git/indutny/myscript.js:3 1 x = 5; 2 setTimeout(() =&gt; &#123; 3 debugger; 4 console.log('world'); 5 &#125;, 1000);debug&gt; nextbreak in /home/indutny/Code/git/indutny/myscript.js:4 2 setTimeout(() =&gt; &#123; 3 debugger; 4 console.log('world'); 5 &#125;, 1000); 6 console.log('hello');debug&gt; replPress Ctrl + C to leave debug repl&gt; x5&gt; 2+24debug&gt; next&lt; worldbreak in /home/indutny/Code/git/indutny/myscript.js:5 3 debugger; 4 console.log('world'); 5 &#125;, 1000); 6 console.log('hello'); 7debug&gt; quit repl命令允许执行远程代码， 而next命令则会运行下一行代码. help命令会在stdout中打印相应的帮助信息，并展示所有可用的命令参数. 直接回车的话，会重复最后一次命令. 如果要查看某个表达式的值，可以通过watch(“expression”)方法来设置，后续可通过watchers命令打印出所有设置过watch的表达式列表，以及相应的值. 如果要移除对某个表达式的观察，可以通过unwatch()方法来移除. 命令行参数单步执行 cont, c - 继续执行 next, n - 执行下一行代码 step, s - 步入 out, o - 步出 parse - 暂停代码的执行，类似于IDE中的暂停功能 断点 setBreapoint(), sb() - 在当前行设置断点 setBreapoint(line), sb(line) - 在指定行设置断点 setBreakpoint(‘fn()’), sb(…) - 在函数体的第一行设置断点 setBreapoint(‘script.js’, line), sb(…) - 在指定脚本的指定行上设置断点 clearBreakpoint(‘script.js’, line), cb(…) - 删除指定脚本的指定行上的断点 另外，可以在还未加载的文件中设置断点： 1234567891011121314151617181920$ node debug test/fixtures/break-in-module/main.js&lt; debugger listening on port 5858connecting to port 5858... okbreak in test/fixtures/break-in-module/main.js:1 1 var mod = require('./mod.js'); 2 mod.hello(); 3 mod.hello();debug&gt; setBreakpoint('mod.js', 23)Warning: script 'mod.js' was not loaded yet. 1 var mod = require('./mod.js'); 2 mod.hello(); 3 mod.hello();debug&gt; cbreak in test/fixtures/break-in-module/mod.js:23 21 22 exports.hello = () =&gt; &#123; 23 return 'hello from module'; 24 &#125;; 25debug&gt; 打印信息 backtrace, bt - 打印当前执行堆栈 list(5) - 打印当前执行行的上下文，参数指定了前后的代码行数 watch(expr) - 将表达式加到观察队列中 unwatch(expr) - 将表达式从观察队列中移除 watchers - 打印当前观察队列中所有的表达式 repl - 在当前调试上下文下打开repl exec expr - 在当前调试上下文中执行expr表达式 执行控制 run - 运行脚本（在调试器启动时自动执行） kill - 杀死脚本 restart - 重启脚本 其它 scripts - 打印当前加载的脚本 version - 打印当前V8的版本 高级用法另一种使用node.js调试器的方法是在node.js启动时附加—debug参数，或通过SIGUSR1来终止Node.js. 一旦通过这种方式指定node.js以debug模式启动后，就可以通过URI或者指定Pid的方式来进行调试： node debug -p pid - 连接到指定的pid node debug \\ - 连接到指定的URI, 如localhost:5858","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"debugger","slug":"debugger","permalink":"http://yoursite.com/tags/debugger/"}]},{"title":"Node.js - Console","slug":"前端/nodejs/Nodejs.console","date":"2017-03-13T00:40:00.000Z","updated":"2017-03-13T04:10:11.000Z","comments":true,"path":"2017/03/13/前端/nodejs/Nodejs.console/","link":"","permalink":"http://yoursite.com/2017/03/13/前端/nodejs/Nodejs.console/","excerpt":"","text":"Consoleconsole模块提供了类似于web浏览器中命令行的功能. 这个模块主要导出了两个组件： Console类： 这个类提供了console.log, console.error以及console.warn等方法，这些方法可以输出到任何node.js定义的流对象中 全局的console对象: 该对象的输出流被定义到stdout和stderr中，由于该实例是全局对象，因此不需要通过require(‘console’)方法导入 使用全局console实例的方法如下: 12345678910console.log('hello world');// Prints: hello world, to stdoutconsole.log('hello %s', 'world');// Prints: hello world, to stdoutconsole.error(new Error('Whoops, something bad happened'));// Prints: [Error: Whoops, something bad happened], to stderrconst name = 'Will Robinson';console.warn(`Danger $&#123;name&#125;! Danger!`);// Prints: Danger Will Robinson! Danger!, to stderr 使用Console类的方法示例如下: 1234567891011121314const out = getStreamSomehow();const err = getStreamSomehow();const myConsole = new console.Console(out, err);myConsole.log('hello world');// Prints: hello world, to outmyConsole.log('hello %s', 'world');// Prints: hello world, to outmyConsole.error(new Error('Whoops, something bad happened'));// Prints: [Error: Whoops, something bad happened], to errconst name = 'Will Robinson';myConsole.warn(`Danger $&#123;name&#125;! Danger!`);// Prints: Danger Will Robinson! Danger!, to err 虽然Console类中的API提供了与浏览器终端类似的功能，但它并没有覆盖终端的所有方法. 异步终端 VS 同步终端除非输出流对象为文件流，否则console提供的方法均为异步输出. 通常情况下，操作系统会为写操作提供缓冲操作. 虽然写操作被阻塞的机会很少，但它还是有可能会发生. 另外，在OSX系统中，当输出的流对象为TTY终端时，console方法是同步输出，以此来解决OSX中缓冲区只有1KB的问题，另外，这也解决了stdout和stderr相互交叉输出的问题. Console类Console类对象可以通过require(“console”).Console方法或者console.Console来获取，它的构造函数中接受两个参数，分别用于设置stdout和stderr的输入流对象，由此提供简单的日志输出功能. 12345678const output = fs.createWriteStream('./stdout.log');const errorOutput = fs.createWriteStream('./stderr.log');// custom simple loggerconst logger = new Console(output, errorOutput);// use it like consoleconst count = 5;logger.log('count: %d', count);// in stdout.log: count 5 全局的console对象是Console类的特例，它的stdout和stderr分别被设置为stdout和stderr对象，可以认为是由以下语句生成的: 1new Console(process.stdout, process.stderr) console.assert(value[, message][, …args])该方法用于测试value值是否为true, 如果不是true， 则抛出AssertException异常，且异常信息由message与args定义，输出的格式为util.format()定义的格式 1234console.assert(true, 'does nothing');// OKconsole.assert(false, 'Whoops %s', 'didn\\'t work');// AssertionError: Whoops didn't work 注: 这里的assert方法的实现与web浏览器中assert方法的实现不同. 具体来讲，在web浏览器调用console.assert()方法时，如果值为false, message信息会被打印，但它并不会打断后续代码的执行；但是在node.js中，如果assert方法的结果为false, 它会抛出AssertExcetion异常. 当然，也可以通过重新Console类的assert()方法，以提供与web浏览器的console.assert()方法类似的功能，以下是相应的示例代码: 1234567891011121314151617181920'use strict';// Creates a simple extension of console with a// new impl for assert without monkey-patching.const myConsole = Object.create(console, &#123; assert: &#123; value: function assert(assertion, message, ...args) &#123; try &#123; console.assert(assertion, message, ...args); &#125; catch (err) &#123; console.error(err.stack); &#125; &#125;, configurable: true, enumerable: true, writable: true, &#125;,&#125;);module.exports = myConsole; 这样，就可以直接替换默认的console.assert()方法了: 123const console = require('./myConsole');console.assert(false, 'this message will print, but no error thrown');console.log('this will also print'); console.dir(obj[, options])该方法会使用util.inspect(obj)方法，并将结果打印到stdout上. 这个方法会忽略obj上定义的inspect()方法. options参数可用来改变输出的信息格式： showHidden： 如果设置为true, 那么non-numberable和链接属性也会被打印出来，默认设置为false depth: 该参数将被传递到util.inspect(), 以确定输出信息时遍历的对象深度， 常用于输出大型对象. 默认为2，如果设置null， 则表示不限制递归深度. colors: 如果设置为true, 则输出时信息会以ANSI风格的颜色来展示，默认为false. 输出的颜色也可以自定义，具体方法见util.inspect()的颜色配置方法 console.error([data], [, …args])将错误信息输出到stderr中，第一个参数为定义的错误信息格式，后续的参数为具体的信息参数，该方法接受的参数与printf(3)类似，所有的参数被传递给util.format()方法. 12345const code = 5;console.error('error #%d', code);// Prints: error #5, to stderrconsole.error('error', code);// Prints: error 5, to stderr 如果格式化元素（如上述的%d)在第一个参数中没有出现， 那么util.inspect()将作用到后续的每个参数，并将所有的结果进行组合. 对于util.inspect()方法的具体实现方式， 可参见util.inspect()的文档. 12console.log(\"hello, world.\", \"---from essviv\", new Date());//Prints: hello, world ---from essviv 2017-03-13T03:50:53.568Z console.info([data], [, …args])该方法是console.log()的别名 console.log([data], [, …args])该方法与console.error类似，区别在于该方法将信息输出到stdout中，而console.error将信息输出到stderr中. console.time(label)该方法会开启一个定时器，定时器的标识为label, 以用于统计某个操作所需要的时间. 当需要终止定时器时，可以调用console.timeEnd()方法，并传入同一个label值，这样就会在stdout中输出从调用console.time()方法到console.timeEnd()方法之间的时间长度， 时间精确到毫秒级别. console.timeEnd(label)该方法会关闭之前的console.time(lable)方法，并在stdout中输出从调用console.time(label)方法到调用该方法之间的时间长度. 123456console.time('100-elements');for (let i = 0; i &lt; 100; i++) &#123; ;&#125;console.timeEnd('100-elements');// prints 100-elements: 225.438ms 注：在node.js v6.0.0以后，该方法会删除定时器对象以防止内存泄露，但在之前的版本中，该定时器会被保留，这意味着可以多次调用console.timeEnd(label)方法, 以多次输出时间长度. 这个功能在v6.0.0之后已被移除. console.trace([data], [, …args])该方法在stderr中输出当前的堆栈信息，堆栈信息的格式由util.format()方法确定, 如下： 12345678910111213console.trace('Show me');// Prints: (stack trace will vary based on where trace is called)// Trace: Show me// at repl:2:9// at REPLServer.defaultEval (repl.js:248:27)// at bound (domain.js:287:14)// at REPLServer.runBound [as eval] (domain.js:300:12)// at REPLServer.&lt;anonymous&gt; (repl.js:412:12)// at emitOne (events.js:82:20)// at REPLServer.emit (events.js:169:7)// at REPLServer.Interface._onLine (readline.js:210:10)// at REPLServer.Interface._line (readline.js:549:8)// at REPLServer.Interface._ttyWrite (readline.js:826:14) console.warn([data], [, …args])该方法为console.error()的别名方法.","categories":[],"tags":[{"name":"node.js","slug":"node-js","permalink":"http://yoursite.com/tags/node-js/"},{"name":"console","slug":"console","permalink":"http://yoursite.com/tags/console/"}]},{"title":"Node.js — Modules","slug":"前端/nodejs/Nodejs.modules","date":"2017-03-09T08:40:00.000Z","updated":"2017-03-10T06:23:30.000Z","comments":true,"path":"2017/03/09/前端/nodejs/Nodejs.modules/","link":"","permalink":"http://yoursite.com/2017/03/09/前端/nodejs/Nodejs.modules/","excerpt":"","text":"ModulesNode.js的模块加载系统非常简单. 在Node.js中，文件和模块是一一对应的. 例如：在foo.js文件中定义以下代码 123//foo.jsconst circle = require('./circle.js');console.log(`The area of a circle of radius 4 is $&#123;circle.area(4)&#125;`); 在foo.js的第一行中加载了circle.js模块，这个模块与foo.js位于同一个路径下. 以下是circle.js的代码 1234//circle.jsconst PI = Math.PI;exports.area = (r) =&gt; PI * r * r;exports.circumference = (r) =&gt; 2 * PI * r; circle.js模块导出了area()方法和circumference()方法. 如果要将更多的对象和函数导出模块，只需要将它们回到特殊的exports对象即可. 模块中的局部变量是私有变量，对外部不可见，因为模块中的代码会被Node.js进一步包装(参见“模块包装器”一节). 在上面这个例子中，PI变量就是circle.js模块的私有变量，对外部不可见. 如果希望将函数(如构造函数)或者完整的对象导出模块，需使用module.exports来导出，而不是exports.(原因可参见： Eloquent Javascript 第10章). 下面的例子中，bar.js依赖于square模块，在square模块中，导出了它的构造函数： 123456789101112//bar.jsconst square = require('./square.js');var mySquare = square(2);console.log(`The area of my square is $&#123;mySquare.area()&#125;`);//square.js// 这里如果使用exports来导出，将不会影响模块的输出，必须使用module.exportsmodule.exports = (width) =&gt; &#123; return &#123; area: () =&gt; width * width &#125;;&#125; Node.js中的模块系统是通过require(“module”)来实现的 访问主模块当某个模块被Node.js直接加载运行时，require.main变量就被设置成module. 这就意味着，可以通过以下的表达式来测试某个模块是否由Node.js直接运行，还是通过其它模块依赖加载. 1require.main === module 例如，对于foo.js模块而言，当通过node foo.js来运行时，上述的表达式返回true; 但当通过require(‘./foo’)来加载运行时，上述的表达式返回的是false. 因为module变量提供了filename属性(通常情况下等于__filename变量)，因此当前模块的路径信息可以通过require.main.filename来获取. 补充说明：包管理Node.js中定义的require()方法的语义非常通用，因此能支持不同的文件目录结构. 目前常见的包管理工具，如dpkg, rpm，npm， 都可以在不修改模块内容的前提下，直接基于Node.js的模块结构来生成相应的目录结构. 接下来我们将给出建议的目录结构. 例如： 我们希望将指定版本的模块内容放于/usr/lib/node//目录下. 模块之间可以存在依赖关系. 在加载foo模块之前，可能需要先加载某个版本的bar模块. 而bar模块可能又需要依赖于其它的模块，在某种情况下，这种依赖链可能会导致版本冲突和循环依赖. 由于Node.js会通过真实路径来加载模块(软链接会被解析)，并且会按照”node_moduls”一节描述的方法来查找依赖，因此上述的情景可以通过很简单的方式来解决： /usr/lib/node/foo/1.2.3 - foo模块的内容，版本号为1.2.3 /usr/lib/node/bar/4.3.2 - bar模块的内容，foo模块依赖于此模块 /usr/lib/node/foo/1.2.3/node_modules/bar - 软链接到/usr/lib/node/bar/4.3.2 /usr/lib/node/bar/4.3.2/node_modules/* - 软链接到bar模块依赖的模块 因此，即使有循环依赖和版本冲突的情况，每个模块都可以正确依赖指定版本的其它模块，并正常执行. 当foo模块中调用require(‘bar’)方法时，它会得到上述第3步中指定的模块内容，即版本号为4.3.2的bar模块. 当bar模块中调用require(“quux”)方法，它会得到第4步中指定的模块，该模块位于/usr/lib/node/bar/4.3.2/node_modules/quux. 而且, 为了进一步优化模块路径的查找，除了将模块直接放在/usr/lib/node目录下，我们也可以将它们放于/usr/lib/node_modules//中，这样当Node.js要查找依赖关系时，就不用再去/usr/node_modules目录以及/node_modules目录下查找了. 为了在Node.js的REPL中使用模块，最好将/usr/lib/node_modules路径加到$NODE_PATH环境变量中. 由于在node_modules中的模块都是基于相对路径来解析的，同时，这些相对路径都是相对于调用require()方法的模块而言的，因此这些模块可以被放在任何地方. 放在一起为了准确获取require方法加载的文件路径信息，可以使用require.resolve()方法. 把上面所有的内容放在一起，则会得到以下的伪码，它阐述了require.resolve()的工作机制: 简单概括如下： 如果是核心模块，直接加载返回 如果模块名是以”./“, “/“或者”../“开头，那么分别按文件和目录的方式加载 如果不满足2的情况，那么按node_modeuls的方式加载 到这里还没找到，抛出异常 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647require(X) from module at path Y //当前模块所在的路径为Y, 它依赖于X模块，以下是require(X)的解析过程1. If X is a core module, //如果X是核心模块，直接返回并退出 a. return the core module b. STOP2. If X begins with './' or '/' or '../' //如果X是以/或者./或者../开头 a. LOAD_AS_FILE(Y + X) //尝试使用loadAsFile加载 b. LOAD_AS_DIRECTORY(Y + X) //再尝试使用loadAsDir加载 //尝试使用loadNodeModule加载, 注：这里的else是自己加上的3. Else LOAD_NODE_MODULES(X, dirname(Y)) 4. THROW \"not found\" //到这里如果还是没有加载到，则抛出异常LOAD_AS_FILE(X) //loadAsFile的工作机制1. If X is a file, load X as JavaScript text. STOP //尝试将X作为JS脚本加载2. If X.js is a file, load X.js as JavaScript text. STOP //尝试将X.js作为JS脚本加载 3. If X.json is a file, parse X.json to a JavaScript Object. STOP //尝试将X.json作为JS对象加载4. If X.node is a file, load X.node as binary addon. STOP //尝试将x.node作为二进制插件加载LOAD_AS_DIRECTORY(X) //loadAsDir的工作机制//尝试寻找X/package.json文件，并寻找main属性，并尝试加载X+main属性指定的路径1. If X/package.json is a file, a. Parse X/package.json, and look for \"main\" field. b. let M = X + (json main field) c. LOAD_AS_FILE(M)//尝试寻找X目录下的index.js文件，并将它作为JS脚本加载2. If X/index.js is a file, load X/index.js as JavaScript text. STOP //尝试寻找X目录下的index.json文件，并将它作为json对象加载3. If X/index.json is a file, parse X/index.json to a JavaScript object. STOP//尝试寻找X目录下的index.node文件，将将它作为二进制插件加载4. If X/index.node is a file, load X/index.node as binary addon. STOPLOAD_NODE_MODULES(X, START) //loadNodeModule的工作机制//获取START目录所有的祖先目录，并在所有的祖先目录下搜索node_modules目录1. let DIRS=NODE_MODULES_PATHS(START) 2. for each DIR in DIRS: //对于每个目录下的node_modules，尝试加载X模块 a. LOAD_AS_FILE(DIR/X) b. LOAD_AS_DIRECTORY(DIR/X)NODE_MODULES_PATHS(START) //nodeModulePaths的工作机制1. let PARTS = path split(START) //拆分目录， 如/a/b/c/d ===&gt; [a, b, c, d]2. let I = count of PARTS - 1 3. let DIRS = []4. while I &gt;= 0, //依次沿目录而上，除非遇到目录名为node_modules，否则直接在目录上搜索node_modules目录 a. if PARTS[I] = \"node_modules\" CONTINUE b. DIR = path join(PARTS[0 .. I] + \"node_modules\") c. DIRS = DIRS + DIR d. let I = I - 15. return DIRS 缓存模块在第一次被加载后就会被缓存起来. 这意味着每次调用require(‘foo’)都会得到同一个对象，前提上模块被解析到同一个文件名. 另外，这也意味着，即使多次引用同一个模块，被引用的模块代码可能也不会被执行多次，这是个很重要的特性，这个特性将允许”部分完成”的对象被返回，即使在有循环依赖的情况下，也能正常地处理依赖传递. 如果希望能多次运行同一个模块的代码，那么请将该模块作为函数导出，然后多次执行该函数即可. 但是，还有一点值得注意的就是，模块的缓存是基于解析得到的文件名. 由于不同模块在加载同一个模块时，有可能会解析到不同的文件名（上述第3种情况，从node_modules加载的时候），因此并不能保证对同一个模块的引用会得到同一个对象. 另外，在一些大小写不敏感的系统中，不同的文件名也可能指向同一个文件，但是缓存系统会把它们当作不同的对象，也会导致模块被加载多次. 例如，require(‘./foo’)和require(‘./FOO’)会返回两个不同的模块对象，即使这两个路径指向的是同一个文件. 核心模块Node.js中打包了一些核心模块， 这些模块的使用将会在文档的其它地方有更详细的描述. 它们的源码位于lib目录下. 核心模块总是会被优先加载，即使在目录中存在相应的文件名. 例如，require(“http”)总是会加载内置的http模块. 循环依赖当有循环依赖出现时，模块可能会在初始化完成前就被返回. 考虑以下这种场景： 123456789101112131415161718192021//a.jsconsole.log(&apos;a starting&apos;);exports.done = false;const b = require(&apos;./b.js&apos;);console.log(&apos;in a, b.done = %j&apos;, b.done);exports.done = true;console.log(&apos;a done&apos;);//b.jsconsole.log(&apos;b starting&apos;);exports.done = false;const a = require(&apos;./a.js&apos;); //这里返回的是&quot;部分完成&quot;的对象console.log(&apos;in b, a.done = %j&apos;, a.done);exports.done = true;console.log(&apos;b done&apos;);//main.jsconsole.log(&apos;main starting&apos;);const a = require(&apos;./a.js&apos;);const b = require(&apos;./b.js&apos;);console.log(&apos;in main, a.done=%j, b.done=%j&apos;, a.done, b.done); 当main.js中加载a.js时，a.js又进一步依赖于b.js, 而在加载b.js时，它又反过来依赖于a.js. 为了防止发生无限循环，“部分完成”的a模块在b模块中被返回，然后b模块完成代码，然后a模块完成执行. 而当main模块要加载这两个模块时，它们已经都完成了自己的初始化工作，因此上述代码的输出将会是: 123456789$ node main.jsmain startinga startingb startingin b, a.done = falseb donein a, b.done = truea donein main, a.done=true, b.done=true 如果知道在代码中存在”循环依赖”的情况，那么需要谨慎考虑代码的实现顺序. 文件模块如果根据提供的文件名无法解析到相应的模块，那么Node.js将会依次在文件名后加上.js, .json和.node后缀继续查找. .js后缀的文件内容将会被解析成js脚本，.json后缀的文件内容将会被解析成JSON对象，而.node后缀的文件内容将被会编译成插件模块，然后用dlopen来加载. 如果文件名是以/开头，那么它将被当成是绝对路径.例如， require(“/home/marco/foo.js”)将会从/home/marco/foo.js加载该文件. 如果文件名是以./开头，那么文件名被解析成相对于当前模块的路径. 例如， 在foo.js中调用了require(“./circle.js”)，那么circle.js必须和foo.js出现在同一个目录下，否则将无法找到这个模块. 如果文件不是以”/“，”./“和”../“开头，那么模块将会被当成是核心模块，或者是从node_modules中加载的模块 如果无法加载到提供的模块，那么Node.js会抛出异常Error, 它的code属性将会被设置成MODULE_NOT_FOUND 目录模块实际应用中，通常会将一组相关的程序和库文件放在同一个目录下，形成自包含的结构，并对外提供单独的入口 ，以方便后续使用. Node.js中提供以下的方式来通过目录名解析模块. 第一种方式是在目录根路径下创建package.json文件，并在该文件中指定main属性，该属性指定了主模块的路径. 123&#123;\"name\": \"some-library\", \"main\": \"./lib/some-library.js\"&#125; 如果目录的名称为./some-library, 那么require(“./some-library”)操作将会加载./some-library/lib/some-library.js文件. 如果package.json中缺少main属性的定义，那么Node.js将会抛出以下的错误： 1Error: Cannot find module 'some-library' 如果目录下没有提供package.json文件，那么Node.js将尝试加载根路径下的index.js文件和index.node文件. 例如： require(“./some-library”)操作将会依次尝试加载./some-libray/index.js和./some-library/index.node 从node_modules目录中加载如果传递给require()方法的参数不是核心模块标识，而且也不是以”/“, “./“和”../“开头的，那么Node.js将从当前模块的父目录开始，逐级向上查找node_modules目录，并尝试从node_modules中加载模块. 如果目录名称以node_modules结尾，那么将路过该目录. 例如， 如果在/home/ry/projects/foo.js模块中调用require(“bar.js”), 那么Node.js将依次从以下路径中搜索该模块： 1234/home/ry/projects/node_modules/bar.js/home/ry/node_modules/bar.js/home/node_modules/bar.js/node_modules/bar.js 另外，你还可以通过在模块名称后加“路径后缀”的方式来加载模块. 例如： 当调用require(“example-module/path/to/file”)时，path/to/file会相对于example-module模块来加载，加载时使用同样的机制. 从全局目录中加载如果设置了环境变量NODE_PATH，那么Node.js会在其它地方搜索不到模块的时候，从这个环境变量指定的目录中进行查找. NODE_PATH环境变量是在当前的路径搜索机制完全确定之前，用来支持各种各样的路径搜索. 目前仍然可以使用NODE_PATH环境变量设置模块搜索的路径，但已经不是那么必要了，Node.js生态已经对模块搜索的路径有了一致的认识. 有时候，部署那些依赖于NODE_PATH环境变量的模块时，如果忘了设置这个变量的值，可能会导致一些很奇怪的现象产生. 例如，依赖的模块版本号会发生变更（有时候甚至是不同的模块） 另外，Node.js也将会在以下目录搜索模块, 其中$PREFIX是在Node.js中设置的. 1231: $HOME/.node_modules2: $HOME/.node_libraries3: $PREFIX/lib/node 值得一提的是，这些搜索路径大部分是由于历史原因被保留，当前Node.js强烈建议将依赖的模块放在本地的node_modules， 这样不但能加快模块的加载速度，还能提高模块加载的稳定性. 模块包装器在模块中的代码被执行之前，Node.js会使用类似于以下的方法对模块中的代码进行包装: 123(function (exports, require, module, __filename, __dirname) &#123;// Your module code actually lives in here&#125;); 通过这种包装，Node.js可以有以下的优势: 避免了全局变量污染，通过包装器，模块的顶级变量被限制在模块内部，成为模块的私有变量，而不是全局变量 同时，通过包装器，它还提供了这样一些变量，它们看起来像是全局变量，但实际是模块的私有变量，如module, exports以及require等 module和exports变量是CommonJS规范中，用于导出模块变量时使用 __filename和__dirname可以很方便地用来获取当前模块的文件路径信息和目录信息 module对象在每个模块中，module变量都被用来指向当前模块. 为了方便，通常可以使用exports变量来指代module.exports. 事实上，module变量只是模块的局部变量，而不是全局变量（具体原因见“模块包装器”一节） module.children这个变量表示被当前模块依赖的所有模块对象，类型为数组 module.exports这个对象是由Module系统创建的，但是有时候模块需要导出由它们自己创建的对象. 为了达到这点，需要重新将module.exports指向新的对象，从而达到导出对象的目的. 值得注意的是，如果是让exports变量重新指向新的对象，那么模块将不会导出这个对象. 根本原因是模块最后导出的是module.exports, 而不是exports. 例如： 1234567891011121314151617//a.jsconst EventEmitter = require('events');module.exports = new EventEmitter();// Do some work, and after some time emit// the 'ready' event from the module itself.setTimeout(() =&gt; &#123; module.exports.emit('ready');&#125;, 1000);//b.js//那么我们可以在另一个文件中加载该模块const a = require(\"./a\");a.on(\"ready\", ()=&gt;&#123; console.log(\"module a is ready\");&#125;); 注意这里重新将module.exports指向新的对象，而不是exports. 另外，必须是直接导出对象，而不能通过回调函数来导出. 12345678//x.jssetTimeout(() =&gt; &#123; module.exports = &#123; a: 'hello' &#125;;&#125;, 0);//y.jsconst x = require('./x');console.log(x.a); exportsexports变量在模块内可见，并且在模块被加载之前，exports的值被设置为module.exports的值. 在使用exports变量的时候，可以对它赋予新的属性，如exports.f = …. 但是，不能将新的对象和函数赋予exports， 因为这将导致exports和module.exports变量指向的对象不是同一个，而模块在最后导出的是module.exports指向的变量，因此指定的新对象将不会被导出. 12module.exports.hello = true; // Exported from require of moduleexports = &#123; hello: false &#125;; // Not exported, only available in the module 当将一个新的对象赋予module.exports变量时，常见的方式是同时将它也赋予exports， 这样后续还是可以使用exports增加属性. 例如: 12module.exports = exports = function Constructor() &#123; // ... etc. 为了更好地说明module.exports和exports变量的作用，以下是require()方法的模拟实现，它与Node.js中真实的实现非常的类似: 1234567891011121314function require(...) &#123; var module = &#123; exports: &#123;&#125; &#125;; ((module, exports) =&gt; &#123; // Your module code here. In this example, define a function. function some_func() &#123;&#125;; exports = some_func; // At this point, exports is no longer a shortcut to module.exports, and // this module will still export an empty default object. module.exports = some_func; // At this point, the module will now export some_func, instead of the // default object. &#125;)(module, module.exports); return module.exports;&#125; module.filename该变量指向当前模块的路径名 module.id当前模块的id， 通常情况下就是当前模块的路径名 module.loaded指示当前模块是否已经加载完成，还是正在加载中 module.parent第一个依赖于当前模块的模块 module.require(id)module.require()方法提供了一种方法，可以从module中来加载指定id的模块. 这里首先要获取到module对象，通常情况下，模块导出的都是module.exports, 因此必须显式地导出module对象，从而调用这个方法.","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"Modules","slug":"Modules","permalink":"http://yoursite.com/tags/Modules/"}]},{"title":"Node.js - Events","slug":"前端/nodejs/Nodejs.events","date":"2017-03-08T08:25:00.000Z","updated":"2017-03-08T08:25:40.000Z","comments":true,"path":"2017/03/08/前端/nodejs/Nodejs.events/","link":"","permalink":"http://yoursite.com/2017/03/08/前端/nodejs/Nodejs.events/","excerpt":"","text":"Events许多nodejs的核心API都是建立在异步的事件驱动框架下，在这种框架下，某些对象（事件触发器）可以不定期地触发某些事件，并触发相应的监听器处理函数. 比如说，net.Server对象在每次有端点连接时，都会触发事件；fs.ReadStream对象在每次打开文件的时候都会触发事件；stream对象在每次有可读数据到达时也会触发事件. 所有能触发事件的对象都是EventEmitter类的实例。这些对象通过eventEmitter.on()方法允许指定一个或多个监听器，当这些对象触发事件时，这些监听器也会被触发执行. 通常情况下，事件的名称是通过”驼峰命名法”命名的，但是所有有效的js属性名称均可以作为事件的名称. 当EventEmitter对象触发了事件时，所有监听了这个事件的监听器都会被同步地调用. 这些监听器的返回值都会被忽略. 以下是一个简单的例子，EventEmitter对象只有一个事件监听器. eventEmitter.on()方法被用来注册监听器，而eventEmitter.emit()被用来触发事件. 1234567891011const EventEmitter = require('events');class MyEmitter extends EventEmitter &#123;&#125;const myEmitter = new MyEmitter();myEmitter.on('event', () =&gt; &#123; console.log('an event occurred!');&#125;);myEmitter.emit('event'); 传递参数和this变量eventEmitter.emit()方法允许将传递任意的参数传递给监听器方法. 值得注意的是，当任意的监听器方法被EventEmitter触发时，this参数总是被隐式地传递给监听器方法，并且指向该监听器所关联的那个EventEmitter对象. 1234567891011const myEmitter = new MyEmitter();myEmitter.on('event', function(a, b) &#123; console.log(a, b, this); // Prints: // a b MyEmitter &#123; // domain: null, // _events: &#123; event: [Function] &#125;, // _eventsCount: 1, // _maxListeners: undefined &#125;&#125;);myEmitter.emit('event', 'a', 'b'); 也可以使用ES6中的箭头函数，但是，在这种情况下，this变量就不再是指向EventEmitter对象了.123456const myEmitter = new MyEmitter();myEmitter.on('event', (a, b) =&gt; &#123; console.log(a, b, this); // Prints: a b &#123;&#125;&#125;);myEmitter.emit('event', 'a', 'b'); 同步vs. 异步当EventEmitter触发某个事件时，它会按注册的顺序依次同步地调用这些监听器方法,这种调用顺序的保证对于避免”竞争条件”以及逻辑错误有重要的意义. 对于一些场景来讲，监听器方法可以使用setImmediate()或者使用process.nextTick()方法将同步方法转化为异步方法.1234567const myEmitter = new MyEmitter();myEmitter.on('event', (a, b) =&gt; &#123; setImmediate(() =&gt; &#123; console.log('this happens asynchronously'); &#125;);&#125;);myEmitter.emit('event', 'a', 'b'); 一次触发当使用eventEmitter.on()方法注册监听器时，后续每次事件被触发时，都会调用这个监听器方法.123456789const myEmitter = new MyEmitter();var m = 0;myEmitter.on('event', () =&gt; &#123; console.log(++m);&#125;);myEmitter.emit('event');// Prints: 1myEmitter.emit('event');// Prints: 2 而使用eventEmitter.once()方法来注册监听器，则只会在事件第一次被触发时调用，后续事件再被触发时则不会被再次调用.123456789const myEmitter = new MyEmitter();var m = 0;myEmitter.once('event', () =&gt; &#123; console.log(++m);&#125;);myEmitter.emit('event');// Prints: 1myEmitter.emit('event');// Ignored Error事件当EventEmitter对象中发生错误时，典型的处理方法是触发error事件，这些事件在nodejs中会被特殊处理. 当EventEmitter对象中没有监听器监听error事件时，当error事件被触发时，nodejs将会打印出调用堆栈，然后退出nodejs进程. 123const myEmitter = new MyEmitter();myEmitter.emit('error', new Error('whoops!'));// Throws and crashes Node.js 为了避免nodejs进程因为error事件而退出，可以在process对象的uncaughtException事件上注册监听器，也可以在domain模块上注册监听器（注意: domain模块已经被废弃不用了)12345678const myEmitter = new MyEmitter();process.on('uncaughtException', (err) =&gt; &#123; console.log('whoops! there was an error');&#125;);myEmitter.emit('error', new Error('whoops!'));// Prints: whoops! there was an error 作为一种最佳实践，建议注册error事件的监听器.123456const myEmitter = new MyEmitter();myEmitter.on('error', (err) =&gt; &#123; console.log('whoops! there was an error');&#125;);myEmitter.emit('error', new Error('whoops!'));// Prints: whoops! there was an error Class: EventEmitterEventEmitter类是通过events模块来定义的： 1const EventEmitter = require(\"events\"); 所有的EventEmitter在注册新的监听器方法时，都会触发newListener事件，而在移除监听器方法时，则会触发removeListener事件. newListener事件* eventName: String | Symbol 被监听的事件名称 * listener: Function 注册的监听器方法 EventListener对象会在监听器方法注册之前触发newListener事件. 监听newListener事件的监听器方法会得到被监听的事件名称以及该事件的监听器方法的引用. 在监听器方法注册之前触发newListener事件，会导致一个很隐蔽但很重要的结果，任何在newListener监听器方法中注册的新监听器方法，都会出现在正在被注册的监听器方法之前. 1234567891011121314151617181920const myEmitter = new MyEmitter();// Only do this once so we don't loop forevermyEmitter.once('newListener', (event, listener) =&gt; &#123; if (event === 'event') &#123; // Insert a new listener in front myEmitter.on('event', () =&gt; &#123; console.log('B'); &#125;); &#125;&#125;);myEmitter.on('event', () =&gt; &#123; console.log('A');&#125;);myEmitter.emit('event');// Prints:// B// A removeListener事件* eventName: String | Symbol 事件名称 * listener: Function 监听器方法 removeListener事件将在listener事件被移除之后被触发 EventEmitter.defaultMaxListeners默认情况下，任何事件最多可以注册10个监听器方法. 对于EventEmitter实例来讲，这个限制可以通过emitter.setMaxListener(n)方法来改变. 如果要改变所有EventEmitter实例的上限，则可以通过EventEmitter.defaultMaxListeners属性来修改. 值得注意的是，当使用EventEmitter.defaultMaxListeners来修改所有实例的上限值时，它会影响所有的实例，包括那些已经创建的对象. 但是，如果实例通过setMaxListener(n)方法修改了这个值，则会覆盖全局的值. 另外，EventEmitter的这个上限值并不是个绝对的限制，EventEmitter的实例还是可以注册超过这个上限值的监听器方法，但是当超过这个上限值时，nodejs会在stderr中打印相应的警告，并提示”EventEmitter可能存在内存泄露”的提示. 对于EventEmitter实例来而言，可以通过getMaxListeners()方法和setMaxListeners()方法来暂时避免这个警告出现.12345emitter.setMaxListeners(emitter.getMaxListeners() + 1);emitter.once('event', () =&gt; &#123; // do stuff emitter.setMaxListeners(Math.max(emitter.getMaxListeners() - 1, 0));&#125;); –trace-warning命令行标识可以用来显示这些警告. 这些警告也可以通过process.on(“warning”)事件监听，这个事件会有emitter, type以及count参数，分别指示对应的EventEmitter实例，事件名称以及注册的监听器数量. emitter.addListener(eventName, listener)这个方法是emitter.on(eventName, listener)的别名方法 emitter.emit(eventName[, …args])同步地调用注册到eventName事件上的监听器方法，调用的顺序与注册的顺序保持一致，每次调用都将传递args值作为参数.当eventName有监听器器方法时返回true, 否则false. emitter.eventNames()返回该emitter中所有有监听器方法的事件数组，数组中的值为String或者Symbol. 12345678910const EventEmitter = require('events');const myEE = new EventEmitter();myEE.on('foo', () =&gt; &#123;&#125;);myEE.on('bar', () =&gt; &#123;&#125;);const sym = Symbol('symbol');myEE.on(sym, () =&gt; &#123;&#125;);console.log(myEE.eventNames());// Prints: [ 'foo', 'bar', Symbol(symbol) ] emitter.getMaxListeners()返回当前emitter的事件监听器方法的上限值，这个值可以通过emitter.setMaxListeners()来指定，也可以通过EventEmitter.defaultMaxListeners属性来指定. emitter.listenerCount(eventName)* eventName: String | Symbol 被监听的事件名称 返回当前监听eventName事件的监听器数量. emitter.listeners(eventName)返回当前监听eventName事件的监听器数组 emitter.on(eventName, listener)* eventName: String | Symbol 被监听的事件名称 * listener: Function 监听器方法对象 在eventName的监听器队列的末尾增加新的监听器listener. 注意，这里并不会检查listener监听器是否已经在监听队列中了. 如果多次将同一个eventListener注册到eventName中，会导致这个监听器被调用多次. 123server.on('connection', (stream) =&gt; &#123; console.log('someone connected!');&#125;); 该方法返回EventEmitter实例，从而可以进行链式调用. 默认情况下，监听器会按照注册的顺序被调用. emitter.prependListener()方法可以用来在监听队列的头部插入新的监听器方法1234567const myEE = new EventEmitter();myEE.on('foo', () =&gt; console.log('a'));myEE.prependListener('foo', () =&gt; console.log('b'));myEE.emit('foo');// Prints:// b// a emitter.once(eventName, listener)* eventName: String | Symbol 被监听的事件名称 * listener: Function 监听器方法 为eventName事件增加一次性的监听器方法. 当eventName事件被触发时，listener监听器被移除，然后被调用.123server.once('connection', (stream) =&gt; &#123; console.log('Ah, we have our first user!');&#125;); 该方法返回EventEmitter实例，从而可以进行链式调用. 默认情况下，监听器会按照注册的顺序被调用. emitter.prependOnceListener()方法可以用来在监听队列的头部插入新的监听器方法 1234567const myEE = new EventEmitter();myEE.once(&apos;foo&apos;, () =&gt; console.log(&apos;a&apos;));myEE.prependOnceListener(&apos;foo&apos;, () =&gt; console.log(&apos;b&apos;));myEE.emit(&apos;foo&apos;);// Prints:// b// a emitter.prependListener(eventName, listener)* eventName: String | Symbol 被监听的事件名称 * listener: Function 监听器方法 在eventName的监听器队列的头部增加新的监听器listener. 注意，这里并不会检查listener监听器是否已经在监听队列中了. 如果多次将同一个eventListener注册到eventName中，会导致这个监听器被调用多次.​123server.prependListener('connection', (stream) =&gt; &#123; console.log('someone connected!');&#125;); 该方法返回EventEmitter实例，从而可以进行链式调用. emitter.prependOnceListener(eventName, listener)* eventName: String | Symbol 被监听的事件名称 * listener: Function 监听器方法 在eventName事件的监听器队列的头部增加一次性的监听器方法. 当eventName事件被触发时，listener监听器被移除，然后被调用. 该方法返回EventEmitter实例，从而可以进行链式调用. emitter.removeAllListeners([eventName])移除emitter对象的所有监听器，或者移除emitter对象的eventName事件的所有监听器. 注意，移除由其它模块或组件注册的监听器是非常不好的行为，例如，移除由sockets或fileStream对象注册的监听器. 该方法返回EventEmitter实例，从而可以进行链式调用. emitter.removeListener(eventName, listener)移除eventName事件的listener监听器. 123456var callback = (stream) =&gt; &#123; console.log('someone connected!');&#125;;server.on('connection', callback);// ...server.removeListener('connection', callback); 注意，removeListener方法最多移除监听器队列中的一个监听器，如果某个监听器被多次加入到监听器队列中，则需要多次调用removeListener()方法来移除这些监听器. 另外，如果监听的事件已经被触发，那么在事件触发时注册的所有监听器方法都会被调用. 这意味着，事件被触发后，在所有的监听器方法被执行完成之前，即使使用removeListener()方法或者removeAllListeners()方法移除了某个监听器，它也不会从队列中移除，直到它处理完后才会从监听器队列中移除. 123456789101112131415161718192021222324252627const myEmitter = new MyEmitter();var callbackA = () =&gt; &#123; console.log('A'); myEmitter.removeListener('event', callbackB);&#125;;var callbackB = () =&gt; &#123; console.log('B');&#125;;myEmitter.on('event', callbackA);myEmitter.on('event', callbackB);// callbackA removes listener callbackB but it will still be called.// Internal listener array at time of emit [callbackA, callbackB]myEmitter.emit('event');// Prints:// A// B// callbackB is now removed.// Internal listener array [callbackA]myEmitter.emit('event');// Prints:// A 因为监听器是通过内部的数组来维护的, 这意味着在调用removeListener()方法后，所有的监听器在数组中的下标将会发生改变. 这不会影响监听器的执行顺序，但通过emitter.listeners()方法的监听器数组的拷贝都需要重新创建. 该方法返回EventEmitter实例，从而可以进行链式调用. emitter.setMaxListener(n)默认情况下，如果某个事件上注册的监听器超过10个时，EventEmitter将会打印警告信息. 很明显，并不是所有的事件监听器都需要被限制在10个以内，emitter.setMaxListener()方法允许修改EventEmitter实例的监听器上限数量. n值如果被设置成0或者Infinity, 意味着不对监听器数量做限制. 该方法返回EventEmitter实例，从而可以进行链式调用. 参考文献 nodejs官网：Events","categories":[],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://yoursite.com/tags/nodejs/"},{"name":"events","slug":"events","permalink":"http://yoursite.com/tags/events/"}]},{"title":"JS基础 - 对象的生命周期","slug":"前端/eloquent_js/JS基础---对象的生命周期","date":"2017-02-07T00:56:00.000Z","updated":"2017-02-08T02:16:50.000Z","comments":true,"path":"2017/02/07/前端/eloquent_js/JS基础---对象的生命周期/","link":"","permalink":"http://yoursite.com/2017/02/07/前端/eloquent_js/JS基础---对象的生命周期/","excerpt":"","text":"当函数作为对象的方法被调用时，this变量指向调用这个方法的对象 1234567891011例1: obj.method(); //this指向obj例2: var obj = &#123; speak: function()&#123; &#125; &#125;; var func = obj.speak; func(); //this指向全局变量，而不是obj Prototypes关于原型的作用，以下这段话做了很好的阐述，它是作为对象属性的“后备”来源的另一个对象 当请求对象的属性时，首先检查对象本身是否定义了这个属性，如果没有找到，则从对象的原型中查找，如果对象的原型中也没有定义相应的属性，则从原型的原型查找，自下而上形成了”原型链”. 原型链的顶端是Object.prototype对象 A prototype is another object which used as fallback source of properties prototype属性的含义 在每个对象中，都有两个属性，一个叫__proto__, 一个叫prototype, 在刚接触原型的时候，经常会搞混这两个属性含义，这里做个澄清： __proto__: 这个属性是对象在查找属性时访问的原型对象，也就是”原型链”中的对象，在通过new操作符创建对象时，会把对象的prototype复制给__proto__属性, 通过Object.getPrototypeOf方法返回的也是这个属性. prototype: 这个属性是在创建新的对象时，新对象的__proto__属性的来源 __proto__ is the actual object that is used in the lookup chain to resolve methods, etc. prototype is the object that is used to build __proto__ when you create new object with new. 12345var car = new Car();console.log(car.__proto__ == Car.prototype); //trueconsole.log(car.prototype === undefined); //trueconsole.log(Object.getPrototypeOf(car) == car.__proto__); //trueconsole.log(Object.getPrototypeOf(car) == car.prototype); //false Constructor在JS中, 使用new操作符来调用方法的时候，就把这个方法当作是新建对象的构造器. 关于构造器，有以下几个事实需要说明： 构造器自动获得一个名为prototype的属性，默认情况下它是一个空对象，这个空对象是从Object.prototype继承来的 使用该构造器创建的所有对象都将以这个空对象为prototype 即 constructor.prototype == Object.getPrototypeOf(instance) 1234567891011//Car是普通的方法function Car()&#123;&#125;var car = new Car(); //这里把Car方法当作是构造函数来使用//测试constructorconsole.log(car.constructor == Car); //trueconsole.log(Car.prototype == Object.getPrototypeOf(car)); //trueconsole.log(Object.getPrototypeOf(Car) == Function.prototype); //true 原型干涉从原型的定义可以看出，可以在原型中增加相应的属性，从而所有实例都可以自动获取到这个属性，绝大部分情况下，这是我们期望得到的结果，但有些时候，这可能会给我们造成困扰. 如下代码所示：123456789101112131415161718192021222324252627282930Car.prototype.wheelCount = 4;car.wheelCount = 5;console.log(car.wheelCount); //5console.log(new Car().wheelCount); //4//原型干涉function put(event, data)&#123; map[event] = data;&#125;function printAll()&#123; for(var obj in map)&#123; console.log(obj); &#125;&#125;var map = &#123;&#125;;put('pizza', 'dfsadf');put('touched', 'toudsafd');printAll();Object.prototype.nonsense = 'hi';printAll();console.log('nonsense' in map); //trueconsole.log('toString' in map); //truedelete Object.prototype.nonsense; 从以上代码执行的结果可以看出，虽然我们并没有定义nonsense和toString属性，但通过Object的原型继承得到了这两个属性，并且，虽然toString属性没有被列举出来，但通过in操作的结果也可以看出，这个属性也被继承了. 事实上， JS中将属性分了enumerable和nonenumerable两大类.默认情况下，通过赋值操作新增的属性都是enumerable的，而Object.prototype中定义的属性都是nonenumerable的，这也是为什么toString没有被列举出来的原因. 如果需要定义nonenumerable的属性，可以通过Object.defineProperty方法来完成，如下所示： 123456//通过defineProperty方法来声明属性Object.defineProperty(Object.prototype, 'hiddenNonse', &#123;enumerable: false, value: 'hi'&#125;);printAll();console.log('hiddenNonse' in map); //trueconsole.log(map.hasOwnProperty('hiddenNonse')); //falsedelete Object.prototype.hiddenNonse;","categories":[],"tags":[{"name":"JS","slug":"JS","permalink":"http://yoursite.com/tags/JS/"},{"name":"Object","slug":"Object","permalink":"http://yoursite.com/tags/Object/"}]},{"title":"JS基础 - 数据结构与对象","slug":"前端/eloquent_js/JS基础---数据结构与对象","date":"2017-02-04T13:09:00.000Z","updated":"2017-02-05T04:52:52.000Z","comments":true,"path":"2017/02/04/前端/eloquent_js/JS基础---数据结构与对象/","link":"","permalink":"http://yoursite.com/2017/02/04/前端/eloquent_js/JS基础---数据结构与对象/","excerpt":"","text":"数组 数组中的元素可以是不同类型的 数组下标以0开始 12345例子：var arr = [2, 3, 4, '5', function()&#123;&#125;, &#123;&#125;];for(var obj in arr)&#123; console.log(arr[obj]);&#125; 属性 获取属性的方法有两种： obj.pro: 点号后面的属性名必须是有效的属性名 obj[pro]: 括号中可以是任何形式的表达式，表达式先经过计算，再被当作对象的属性获取相应的属性信息 12345678910var obj = &#123; name: 'essviv', age: 27, 3: 103&#125;var propName = 'age';console.log(obj.name); //essvivconsole.log(obj[propName]); //27console.log(obj[3]); //103 数组可以认为是属性全部为数字的对象 值为函数的属性被称为是方法 1234567var obj = &#123; sayHello: function(name)&#123; console.log('hello, ' + name); &#125;&#125;obj.sayHello('essviv'); //hello, essviv 对象的属性通过key: value的形式指定，尝试访问不存在的属性会返回undefined, 可以通过=进行新增或更新属性操作，delete操作符可以将对象的属性删除 12345678910111213141516171819obj = &#123; name: '熊猫', // age: 27&#125;;console.log(obj.name);judge(obj, 'age');obj.age = 27; //赋值操作judge(obj, 'age');delete obj.age; //删除属性操作judge(obj, 'age');function judge(obj, prop)&#123; console.log(prop in obj); //判断当前属性是否存在 console.log(obj[prop]); //直接打印&#125;","categories":[],"tags":[{"name":"-js -Object","slug":"js-Object","permalink":"http://yoursite.com/tags/js-Object/"}]},{"title":"JS基础 - 函数","slug":"前端/eloquent_js/JS基础-函数","date":"2017-02-04T00:56:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/02/04/前端/eloquent_js/JS基础-函数/","link":"","permalink":"http://yoursite.com/2017/02/04/前端/eloquent_js/JS基础-函数/","excerpt":"","text":"函数的定义定义函数和定义普通变量没有太大的区别，只是在指定值的时候，使用的是个function对象而已，如下：12345示例:var square = function(x)&#123; return x * x;&#125; 变量的作用域函数的参数变量以及在函数体中以var关键字声明的变量，均为局部变量，也就是说这些变量仅在函数内部可见，而在函数体外声明的变量，可以认为是全局变量 在JS中，除了局部变量和全局变量，还存在作用域嵌套的情况. 简单来讲，每个局部作用域都可以看到包含它的所有作用域变量, 函数可见的变量集合取决于函数在程序中所处的位置. 另外，在JS中，只有函数才会产生新的作用域，其它的代码块，即使用大括号包起来，也不会产生新的作用域 1234var sth = 1;&#123; var sth = 2; //这里并没有产生新的作用域&#125; 调用堆栈调用堆栈中存储了函数执行的上下文(context),当调用函数时，程序会将当前调用环境的上下文入堆，当函数返回时，则从堆顶弹出相应的上下文以恢复函数调用前的环境. 函数参数在JS中，函数参数的个数并不是严格限制的， 如果传入的参数多于函数定义的参数个数，那么多出来的参数会被自动忽略； 如果传入的参数个数少于函数定义的参数个数，那么没有传入的那些参数会被自动填充为undefined 12345function add(a, b)&#123;&#125;例1: add(1) ===&gt; b为undefined例2: add(1, 2, 3) ===&gt; 参数3被忽略 闭包结合“函数局部变量对外不可见”以及“函数可以存储于变量中”这两个事实，可以得到一个很有意思的问题： 如果在访问局部变量的时候，局部变量所处的函数变量已经出了作用域了，那么局部变量会是什么样的状态？ 1234567891011//这里定义了一个闭包function wrapValue(n)&#123; var localValue = n; return function()&#123;return localValue;&#125;&#125;var wrap1 = wrapValue(1);var wrap2 = wrapValue(2);console.log(wrap1()); //1console.log(wrap2()); //2 以上的例子说明，局部变量仍然是可见的，这种情况就被称为是“闭包”.使用这个特性可以写出一些很有意思的代码.","categories":[],"tags":[{"name":"js","slug":"js","permalink":"http://yoursite.com/tags/js/"},{"name":"function","slug":"function","permalink":"http://yoursite.com/tags/function/"}]},{"title":"JS基础 - 表达式","slug":"前端/eloquent_js/JS基础-表达式","date":"2017-02-03T12:27:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/02/03/前端/eloquent_js/JS基础-表达式/","link":"","permalink":"http://yoursite.com/2017/02/03/前端/eloquent_js/JS基础-表达式/","excerpt":"","text":"变量变量的声明是通过var关键字来完成的，变量可以用来存储值，以供后续使用 变量名区分大小写，可以包含数字、字母及下划线，但不能以数字开头. 1234567var name = 'essviv';var one, two = 2, three = 'three';console.log(name); //'essviv'console.log(one); //undefinedconsole.log(two); //2console.log(three); //three 函数函数是JS中比较特殊的一种变量，它将一段代码声明成某个变量，后续可以通过这个变量来引用这个函数，如果要“执行”这个函数的话，只需要在这个变量后增加“()”即可. 1234var func = function()&#123; return 'hello world';&#125;console.log(func()); //调用func变量所指向的函数，输出hello world 在不同的环境中，预定义了一系列和环境相关的函数： alert: 通常在浏览器环境中会定义这个方法 console.log: 在几乎所有的环境中，都会定义这个方法，事实上，真正的方法名为log confirm: 在浏览器环境中通常会定义这个方法，用于向用户展示确认信息 prompt: 在浏览器环境中通常会定义这个方法，用于获取用户的输入 控制流1. 条件语句12345678910111213141516171819202122语法1: if(condition)&#123; &#125;语法2: if(condition)&#123; &#125;else&#123; &#125;语法3：if(condition)&#123; &#125;else if(condition_2)&#123; &#125;else if(condition_3)&#123; &#125;else&#123; &#125; 循环语句 while…do… do…while… for 1234567891011121314语法1：while(condition)&#123; &#125;语法2:do&#123; &#125;while(condition) 语法3:for(var i = initValue; condition; updateLoop)&#123; &#125; 跳出循环 break continue 多条件分支 123456789语法：switch(checkValue)&#123; case 1: break; case 2: break; default: break;&#125; 注释 单行注释: 以双斜杠(//)开始 多行注释: 以/*开始， 以*/结尾","categories":[],"tags":[{"name":"expression","slug":"expression","permalink":"http://yoursite.com/tags/expression/"},{"name":"eloquent js","slug":"eloquent-js","permalink":"http://yoursite.com/tags/eloquent-js/"}]},{"title":"","slug":"前端/eloquent_js/JS基础-数据类型","date":"2017-02-03T11:54:18.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/02/03/前端/eloquent_js/JS基础-数据类型/","link":"","permalink":"http://yoursite.com/2017/02/03/前端/eloquent_js/JS基础-数据类型/","excerpt":"","text":"JS的数据类型 数值 字符串 布尔 对象 函数 Undefined 数值类型 JS中所有的数值类型均占用64位内存空间. 操作符： 优先级 特殊数值： Infinity, -Infinity, NaN(Not A Number, JS中唯一一个不与自身相等的数值) 字符串 使用单引号(‘)或者双引号(“) 转义符(): \\n, \\t, \\ 级联： ‘hell’ + ‘o_’ + ‘world’ 一元操作符: typeof 布尔类型 true或者false 比较符: 3&gt;2, 3&lt;=4, ‘hello world’ &gt; ‘binary’ 逻辑操作符: &amp;&amp;(且), ||(或), !(非) 三元操作符: ?: Undefined null undefined 类型转换需要进一步明确转换规则","categories":[],"tags":[]},{"title":"JVM的类加载机制","slug":"JVM/JVM的类加载机制","date":"2017-02-03T07:18:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/02/03/JVM/JVM的类加载机制/","link":"","permalink":"http://yoursite.com/2017/02/03/JVM/JVM的类加载机制/","excerpt":"","text":"JVM的类从被加载到虚拟机，到从内存中卸载，可以分为七个步骤，分别是加载， 验证， 准备， 解析， 初始化， 使用以及卸载. 如下图所示， 其中验证、准备和解析又被称统称为是“连接”阶段. 加载虚拟机在加载阶段将类定义加载到内存中，并在方法区中建立相应的运行时数据结构，同时在堆上建立相应的Class对象实例，作为访问方法区运行时数据的入口. 通过类的全限定名获取类定义的二进制字节流 根据获取到的二进制字节流在方法区中建立运行时的数据结构 在堆中建立相应的Class对象实例，作为访问方法区运行时数据结构的入口 这里值得一提的是， 在第1步中”根据类的全限定名获取类定义的二进制字节流”， 并不要求一定要从Class文件中加载，任何可以获取到合法的类定义字节流的途径都可以. 相对于类加载的其它阶段， 类加载的过程（准确地说，是类加载阶段的第1步）是开发者可控性最强的阶段， 开发人员可以根据需要选择从任何需要的地方获取相应的类定义数据. 类加载器在虚拟中， 类加载的第1步是通过“类加载器”来完成的，类加载器又可以分为不同的层次, 每个类加载器均有父加载器（启动加载器除外）， 在加载类对象的时候 ，类加载器采用了的是“双亲委派模型”. (这里). 具体来讲，当类加载器尝试加载某个类之前，它总是先将加载操作“委托”给父加载器执行，只有当父加载器无法加载该类对象时，它才自己尝试加载. 启动加载器(Bootstrap ClassLoader) 扩展加载器(Extend ClassLoader) 应用加载器(Application ClassLoader) 自定义加载器(Custom ClassLoader) 另外，在虚拟机中，任意类的唯一性由类加载器以及这个类本身共同决定. 换句话说，即使是完全相同的一个类，如果加载的时候使用不同的类加载器，那么对于虚拟机而言就是两个不同的类. 在JAVA虚拟机中，类加载器可以细分为四类: 验证在完成类加载后，紧接着虚拟机要完成的操作就是“验证”, 在这步中虚拟机要完成的对类定义对象的验证，以保证加载的类对象是符合格式规范，且不会做破坏虚拟机的事情. 在验证阶段，虚拟机要完成的动作又可进一步细分为： 文件格式的验证: 这步验证的主要目的是保证字节流能够在方法区中正确无误地建立运行时数据结构，主要验证的内容包括确认字节流是否符合文件规范的要求，以及文件版本号能否被当前的虚拟机所处理. 在完成这步验证后，类定义的字节流就会在方法区中建立相应的运行时数据结构，后续的验证都会基于方法区中的数据结构来进行. 元数据的验证： 这步验证主要是对类定义的各种元数据进行确认，保证不会出现不符合JAVA规范的元数据信息 字节码的验证： 这步验证主要是确保类定义的数据流和控制流没有问题，以确保在类的执行过程中，不会有危及虚拟机安全的动作 符号引用的验证： 这步验证主要是确保类定义的符号引用都能被正确地解析成直接引用 准备准备阶段是正式为类变量分配内存并设置初始值的阶段， 这些内存都将在方法区中分配.这句话中，有三个地方值得引起我们的注意： 为类变量分配内存： 在准备阶段，只会为类变量分配内存，不包括实例变量 为类变量设置初始值： 这里的“初始值”通常情况下是指数据类型的初始值，而不是开发人员设置的值. 内存将在方法区中分配： 类变量的内存都在被分配在方法区中，而不是堆上 这里有个例外，就是当类变量被final修饰时，在准备阶段会完成类变量的赋值操作. 123456789例1： public static int value = 3;在完成准备阶段后，value的值会被初始化成0，而不是3（这步是在初始化阶段完成的）例2： public static final int value = 3;则完成“准备”阶段后，value的值会被初始化成3 解析类文件的解析过程是将符号引用转化成直接引用的过程. TODO 初始化在进入初始化阶段后，虚拟机才真正开始执行程序中定义的代码. 在“准备”阶段，类变量完成了内存分配及初始值的设置，而在这里，类变量将进一步按照程序的设置，完成赋值操作. 或者可以这么说，在初始化阶段，虚拟机调用了类构造器()方法. 这里简单介绍下类构造器()方法的执行规则： ()方法是由编译器收集类中定义的所有类变量的赋值操作和静态语句块合并产生的，收集的顺序由语句在源文件中出现的顺序决定. 其中， 静态语句块中只能访问到定义在静态语句块之前的变量，对于定义在静态语句块之后的变量，在语句块中可以赋值，但不能访问 ()方法与实例构造器不同，它不需要显式地调用父类构造器，虚拟机会保证在调用子类()方法之前，已经完成父类()方法的调用. ()方法对于类或接口来讲不是必需的，如果类中没有静态语句块，也没有对类变量进行赋值操作，则可以不用生成()方法 虚拟机会保证类的()方法在多线程环境中被正确地加锁和同步. 参考文献 类加载机制 JVM类加载机制 示例代码 类加载的“初始化”1 类加载的“初始化”2","categories":[],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"类加载","slug":"类加载","permalink":"http://yoursite.com/tags/类加载/"}]},{"title":"2017学习计划","slug":"2017年学习计划","date":"2017-02-03T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/02/03/2017年学习计划/","link":"","permalink":"http://yoursite.com/2017/02/03/2017年学习计划/","excerpt":"","text":"2017年学习计划1. 内容篇 JVM机制: OSGi, 反射，注解， 动态加载， 内存模型， 性能调优 JMS: Kafka, RabbitMQ, ActiveMQ, RocketMQ 缓存: 页面缓存， 浏览器缓存， 代理服务器缓存， 数据库热点数据缓存 SOA：分布式程序实践 数据库： 执行原理， 优化原理， 优化（开发者） 前端技术： JS基础强化， 常见框架学习 NoSQL: MongoDB 2. 教材篇(持续更新)JVM 深入理解Java虚拟机 JVM高级特性与最佳实践， 周志明 Java性能优化权威指南， Charlie Hunt / Binu John ， 柳飞/陆明刚(译)","categories":[],"tags":[]},{"title":"reactor模型","slug":"IO/netty/reactor模型","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/IO/netty/reactor模型/","link":"","permalink":"http://yoursite.com/2017/01/25/IO/netty/reactor模型/","excerpt":"","text":"Reactor模型Reactor模型的中心思想是将所有要处理的IO事件及其处理器注册到一个中心的IO多路复用器上，并将主线程阻塞在多路复用器上；当有相应的IO事件到达时，多路复用器将IO事件分发给相应的处理器进行处理.Reactor模型的模型图如下所示， 其中包括几个核心组件： Initiation Dispatcher（分发器）： 这是Reactor模型的中心组件，所有的IO事件及其处理器都要在这里进行注册，同时它还拥有一个多路复用器（Synchronous event demultiplexer），在进程启动时，它会阻塞多路复用器以监听注册的IO事件，当有IO事件到达时，多路复用器会通知分发器，而分发器会调用之前注册的事件处理器对相应的IO事件进行处理. Synchronous event demultiplexer(多路复用器）: 多路复用器负责监听相应的IO事件，当IO事件到达时，由多路复用器负责通知到分发器. 事件处理器： 通常情况下，事件处理器会将它要处理的事件及其自己注册到分发器中，当分发器得到多路复用器的事件通知后，就会回调这些事件处理器进行处理， IO事件中往往有发生当前IO事件的句柄信息（handle） 事件： Reactor中的事件基本上可以认为是IO事件， 这些IO事件都会发生在一定的句柄中(handle)， 它是用来标识网络设备的标识. 拿JAVA中的NIO编程举例（以下的例子）， ServerSocketChannel就是图中的handle, 它可能发生的事件包括Accept, Read, Write等等， 而Selector就是NIO中的多路复用器，整个程序的实现就是分发器. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public static void main(String[] args) throws IOException &#123; //handle ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); serverSocketChannel.socket().bind(new InetSocketAddress(HOST, PORT)); serverSocketChannel.configureBlocking(false); //demultiplexer Selector selector = Selector.open(); int insteresSet = SelectionKey.OP_ACCEPT; //register serverSocketChannel.register(selector, insteresSet); //event loop while (true) &#123; //阻塞等待事件 selector.select(); Set&lt;SelectionKey&gt; selectionKeyset = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectionKeyset.iterator(); while (iter.hasNext()) &#123; SelectionKey selectionKey = iter.next(); if (selectionKey.isAcceptable()) &#123; //accept event handler acceptProcessor(selectionKey); &#125; else if (selectionKey.isReadable()) &#123; //read event handler readProcessor(selectionKey); &#125; else &#123; System.out.println(\"Unknown op.\"); &#125; iter.remove(); &#125; &#125; &#125; //read事件处理器 private static void readProcessor(SelectionKey selectionKey) throws IOException &#123; SocketChannel sc = (SocketChannel) selectionKey.channel(); ByteBuffer byteBuffer = ByteBuffer.allocate(64); sc.read(byteBuffer); byteBuffer.flip(); String content = Charset.forName(\"utf-8\").newDecoder().decode(byteBuffer).toString(); String response = \"你好, 客户端. 我已经收到你的消息, 内容为\\\"\" + content + \"\\\"\"; sc.write(ByteBuffer.wrap(response.getBytes())); sc.close(); &#125; //accept事件处理器 private static void acceptProcessor(SelectionKey selectionKey) throws IOException &#123; ServerSocketChannel ssc = (ServerSocketChannel) selectionKey.channel(); SocketChannel sc = ssc.accept(); sc.configureBlocking(false); sc.register(selectionKey.selector(), SelectionKey.OP_READ); &#125; Reactor的实现在java nio中，提供了Reactor实现需要的组件， Selector实现了多路复用器的角色， SelectionKey代表了通道到多路复用器间的注册关系（在Reactor模型中， 事件及其处理器是注册到分发器中， 这里有点小小的区别，但不影响实现 ），同时也代表了相应的IO事件； 对比下来，如果想用JAVA NIO实现Reactor模型，还需要提供分发器（Dispatcher）， 事件处理器， 并在通道注册到多路复用器时绑定相应的事件处理器（这个可以在调用注册方法时提供相应的attachment来实现）, 因此， 实现的关键点就是提供分发器和事件处理器. 根据实现方式的不同，又可以进一步细分为： 1. 单线程版本参见代码Reactor单线程（版本号： ea3714f）， 查看这个版本的代码可以看出，分发器自始至终只启动了一个线程，这个线程负责监听IO事件，当IO事件到达时，它依次调用相应的事件处理器进行处理， 待所有IO事件被处理完后，分发器将开启下一轮监听. 这样的实现代码逻辑简单，思路清晰，不需要处理多线程时存在的竞争条件和锁的问题，但它有个致命的缺点，当客户端数量增加时，可能会有大量的IO事件产生，这些事件的处理过程都是串行进行的，必然会导致分发器的处理效率下降，无法满足业务量扩展的需要. 2. 多线程版本参见代码Reactor线程池版本（版本号：4019213), 这个版本的代码要注意的是事件处理器的处理逻辑被放到了线程池中进行，这也就意味着，程序必须要考虑并发的情况（比如，同一个读事件被多次分发， 读事件处理的过程中注册的事件类型被改变等等问题）. 在代码中每个处理器都引入了当前的处理模式（读或者写），以此来避免处理过程中事件类型被改变的问题； 同时引入了“正在处理中”的状态，防止同一次读事件被多次分发多次处理. 但值得一提的是，这个版本只是将事件的处理放到线程池中，而对于事件的分发(比如连接事件)还是保持单线程，在客户端连接数增加时，会出现性能瓶颈 3. 扩展的多线程版本略 参考文献 Reactor的论文 http://blog.csdn.net/u013074465/article/details/46276967 http://jeewanthad.blogspot.jp/2013/02/reactor-pattern-explained-part-1.html RI","categories":[],"tags":[]},{"title":"备忘","slug":"IO/netty/备忘","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/IO/netty/备忘/","link":"","permalink":"http://yoursite.com/2017/01/25/IO/netty/备忘/","excerpt":"","text":"备忘 netty是如何实现对于某个channel的IO事件，交由同一个线程去处理?channel持有一个eventloop，后续所有的操作都会交由这个eventllop操作，具体是在每个操作前，判断一下当前的执行线程是不是eventloop，如果是，直接执行，如果不是，则将要执行的事情放到eventloop的执行任务队列中 什么时候创建的多个NioServerSocketChannel?（与问题5类似）netty中, nioServerSocketChannel只是在bind的时候被创建一次，后续也不会创建，但可以通过register方法创建相应的channel. NIO的标准操作哪里去了? NioEventLoop中实现了轮询的逻辑 NioServerSocketChannel构造函数中实现了channel的配置信息 Register方法实现了注册 selector是在哪里开始监听事件的，它的interseteOps是在哪里被设置的（如何监听连接，与问题6类似）?bind操作中触发了fireChannelActive事件，这个事件会调用channel的read方法，然后会调用unsafe的doBeginRead方法，在这个方法中，channel的兴趣事件被真正设置, 而channel的interestOps在它的构造函数中就已经被设置好了 SocketChannel是怎么被创建的（它是有多个吗？还是只有一个？）socketChannel是在客户端发起连接操作时，由NioEventLoop的轮询操作捕获到相应的IO事件，它调用了unsafe的read方法，在该方法中，NioSocketChannel被创建，且通过NioServerSocketChannel的read事件传播, 在NioSSC的pipeline初始化的时候，设置了ServerBootstrapAcceptor的处理器，它会处理channelRead事件，并在这个事件的处理中，初始化NioSC的相应属性, 然后将NioSC注册到childGroup中. 对于每一个新的连接，服务端都会创建一个新的请求. server端是如何处理连接的见问题5 参考文献 NioEventLoop的运行情况 服务启动的代码 系列的开始文章","categories":[],"tags":[]},{"title":"反射-Member接口","slug":"JVM/反射/反射-Member接口","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/JVM/反射/反射-Member接口/","link":"","permalink":"http://yoursite.com/2017/01/25/JVM/反射/反射-Member接口/","excerpt":"","text":"反射-Member接口Member接口定义了单个成员（可以是属性、构造函数或者方法）的信息，它提供了以下几个方法： getDeclaringClass: 返回声明这个成员的类信息 getModifier: 返回修饰符信息 getName: 返回成员的名称信息 isSynthetic：标识当前成员是否为人造的，即是否是由编译器引入的 在反射框架中，Field, Method以及Constructor都实现了这个接口","categories":[],"tags":[]},{"title":"ClassLoader的双亲委派模型","slug":"JVM/ClassLoader的双亲委派模型","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/JVM/ClassLoader的双亲委派模型/","link":"","permalink":"http://yoursite.com/2017/01/25/JVM/ClassLoader的双亲委派模型/","excerpt":"","text":"ClassLoader的双亲委派模型ClassLoader的层次在java中，ClassLoader的层次可细分为： BootStrap ClassLoader： 负责加载JDK中的核心类库 Extension ClassLoader： 负责加载/lib/ext/下的类库，负责JAVA扩展库 App ClassLoader： 负责加载应用程序classpath下的所有Jar包和class文件 自定义classLoader： 用户自定义的classLoader， 在自定义classLoader的时候，强烈建议重载findClass，而不是loadClass， 因为默认的loadClass方法实现了委派模型，通过重写findClass方法，可以让自定义的classLoader保持这种模型； 但由于loadClass方法并不是final的，因此，还是可以直接重写这个方法来破坏委派模型，但这不是一种好的实践方式. 双亲委派模型在加载类时，会先试图让父类加载器执行这个任务， 只有当父类加载器找不到该类时，才会由自己开始尝试加载这个类. ClassLoader的loadClass的过程 ， 其源码与时序图如下： 首先调用findLoadedClass来检查当前类是否已经加载，如果加载就直接返回 如果没有加载，则通过父classLoader来尝试加载，这里体现了委派模型（delegation model） 如果前面两步都没有加载到类信息，则此时classLoader调用findClass尝试加载findClass一般是先加载类数据(loadClassData),然后再调用defineClass方法来定义类，defineClass被final修饰 如果前三步都没有加载成功，则抛出ClassNotFoundException的异常信息. 类的加载在JAVA语言中，类由它的全限定名来唯一确定，当类加载到JVM中时，事实上，类是由加载它的类加载器以及它的全限定名共同确定的， 也就是说: 即使是同一个类文件，如果是由不同的类加载器加载，那么对于JVM来讲，就是不同的类对象 不同类加载器加载得到的类对象永远都是不同的，即使它们是由同一份类文件加载获得 加载某个类时，与它关联的所有类也会由当前类的类加载器加载得到. 比如，在JAVA中所有的类都继承自Object对象，那么当加载类时，会自动发起加载Object对象的操作（但是由于委派模型，Object类通常是由启动加载器加载得到的） 类对象的状态 未加载： 未加载的类对象仅仅是class文件 加载：加载的类对象是指已经被加载到JVM中，但还没被实例化，或者调用子类信息，或者运行相应的方法 激活：激活状态的类对象是指被加载到JVM中，且被实例化，或调用子类信息，或运行了相应的方法 数组是由JVM加载的，因此如果对数组获取getClassLoader，获取到的和它的元素类型的classLoader是一样的，如果数组元素为原生类型，则返回null 加载资源class的getResourceAsStream方法将加载资源的操作委托给它的classLoader来执行，但是在委托之前，它会将传入的资源名称进行解析，解析规则如下： 如果资源的名称是以“/”开始， 那么对应的资源名称就是去掉“/“后的那部分名称 如果资源的名称不是以“/“开始，那么对应的资源名称就是将当前的包名加资源名(将包名中的”.”替换为”/“） 123例如： getResourceAsStream(&quot;/A/B/C/D&quot;) ------&gt; A/B/C/D通过类com.cmcc.syw.LoaderTest.java调用getResourceAsStream(&quot;A/B/C/D&quot;) -------&gt; com.cmcc.syw.A.B.C.D 参考文献 http://blog.csdn.net/xyang81/article/details/7292380 http://docs.oracle.com/javase/7/docs/technotes/tools/findingclasses.html http://www.programgo.com/article/64522195716/","categories":[],"tags":[]},{"title":"zookeeper学习摘要","slug":"zookeeper学习摘要","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/zookeeper学习摘要/","link":"","permalink":"http://yoursite.com/2017/01/25/zookeeper学习摘要/","excerpt":"","text":"Stat结构 zxid: zookeeper transaction id, 这个ID表示了全局的顺序, zxid的值越小，那么该操作发生的时间越早. czxid: 导致这个节点创建的操作zxid编号 ctime: 创建这个节点的时间 mzxid： 最近一次编辑这个节点数据的zxid编号（注意，这里只会在编辑节点数据时才会变化，如果编辑的是子节点，并不会引起这个数据发生变化） mtime: 最近一次编辑这个节点的时间 dataVersion: 节点的数据版本，每个修改节点数据版本号都会增加1 cversion: 子节点的数据版本， 增加或删除子节点都会导致这个版本增加1 aversion: 节点的权限版本，每个修改节点的权限信息都会导致这个版本增加1 ephemeralOwner: 当这个节点是个ephemeral节点时，这个值代表创建这个节点的session id；如果这个节点不是ephemeral节点，那么这个值为0 dataLength: 数据长度. numChildren: 子节点个数 ACL控制 ZK中的ACL格式为scheme:id:permissions， 内置的scheme包括以下几种，针对每一种scheme都有对应的id形式： world: 这种scheme只有一个id，就是anyone, 即world:anyone, 它代表所有人. auth: 这种scheme不需要id信息，只要是认证过的用户都有相应的权限 digest: 这种scheme对应的id为username:BASE64(SHA1(password)) ip: 这种scheme对应的id为ip信息，ip信息中可以指定ip段，例如：192.168.32.15/16表示只需要校验前16位的ip段信息 权限信息 读权限(r)：读取节点数据和子节点信息 写权限(w)： 设置节点数据的权限 删除权限(d)： 删除子节点的权限 增加权限(c)：增加子节点的权限 管理权限(a)： 更改权限的权限 参考文档 http://www.itnose.net/detail/6445740.html","categories":[],"tags":[]},{"title":"Netty源码学习系列---服务端的启动","slug":"IO/netty/Netty源码学习系列---服务端的启动","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/IO/netty/Netty源码学习系列---服务端的启动/","link":"","permalink":"http://yoursite.com/2017/01/25/IO/netty/Netty源码学习系列---服务端的启动/","excerpt":"","text":"Netty源码学习系列—服务端的启动netty作为一套NIO框架，它封装了JAVA中的原生的NIO操作逻辑. 在学习netty源码的时候，我们就遵循这样的思路，先用JAVA原生的NIO API实现服务端程序，然后对照这些步骤，一步步地去netty中寻找相应的实现，看看作为框架，它是怎么封装基本的操作，并实现良好的扩展性. 原生JAVA NIO API实现服务端代码如下所示，从代码中可以将整个服务端的启动过程分为以下几个步骤： 创建ServerSocketChannel, 初始化成非阻塞模式，并将它绑定到指定的地址 初始化selector（多路复用器） 将serverSocketChannel注册到selector， 并监听accept事件 开启轮询操作，将对IO事件进行处理，在IO事件的处理中，还会变化ssc的监听事件 Netty是怎么做的在阅读netty源码之前，比照之前的原生java代码实现，我们要试图回答这样的一个问题，netty是分别在哪里实现这些“标准”操作的？ netty实现的服务端代码如下，表面上看来，和上面那段代码看起来完全不一样，但作为java nio框架，底层的操作一定离不开那些原生的API，现在看到的不过是被高度封装后的代码而已， OK，那就顺着这段代码开始阅读netty源码. ServerSocketChannel的初始化源码的第一行初始了ServerBootStrap对象，它继承自AbstractBootStrap对象，从名字上可以看出，这个对象的作用就是引导服务端的启动程序，通过这个对象可以配置服务端启动过程中需要用到的参数. 从上面的“标准”代码中可以看到，对于服务端来讲，需要配置的参数包括绑定的地址和服务端channel，但在netty的配置中除了这两项还多了其它的一些配置： bossEventGroup和workerEventLoopGroup: netty框架在实现的过程中，使用了Reactor模型，这两个参数就是在reactor模型中定义的，关于Reactor模型的详细内容，可以参考这里. 简单来讲，reactor模型的实现中，可以定义两组线程池，一组用来处理来自客户端的连接请求，而另一组用来处理已连接的客户端产生的IO事件， 从而提高处理连接和IO事件的效率. childHandler: 这个参数配置是客户端连接完成后，后续的IO事件的处理器， 事件处理器的概念也是来自reactor模型， 在netty中，事件处理器被组织成channelPipeline的形式，当通道中有IO事件发生时，这些事件会顺着pipeline依次通过每个事件处理器，事件处理器会根据需要选择对IO事件进行处理或发往下一个处理器，关于pipeline和处理器的更多内容，会在后续的文章中进行阐述。 初始化完serverBootStrap后，就会调用bind方法执行绑定操作. bind方法中会调用doBind方法， 在doBind方法中，netty会根据之前配置的引导参数，完成SSC的初始化和绑定. doBind方法的实现可以简单的分成两个步骤： initAndRegister和doBind0, 前者完成初始化和注册（后续会讲到），后者完成真正的绑定操作. 1. 初始化SSC的初始化是在initAndRegister方法中完成的， 可以看到，首先调用了channelFactory这个工厂类获取一个新的channel对象， 还记得在serverBootStrap对象中配置的channel参数吗？这里工厂类就是根据配置的这个channel参数通过反射机制生成新的对象. 反射机制中调用ServerSocketChannel的无参数构造函数，并通过一系列的父类构造函数完成了包括pipeline、unsafe、channel的非阻塞模式以及监听事件的初始化操作. 在完成channel对象的构造之后， 会调用init方法继续完成对channel对象的初始化操作. 这里初始化的内容也是根据serverBootStrap中的配置参数进行的， 这里值得注意的是第181行，在这里往pipeline中增加了一个ChannelInitializer处理器，这个处理器在通道注册(channelRegister事件)的时候，会往pipeline中添加ServerBootstrapAcceptor处理器，而这个sba处理器会对新进来的连接进行处理, 后续会再次对这个地方进行说明. 到这里为止，netty就完成了SSC的初始化操作. 2. 注册操作在完成SSC的初始化操作后，netty会马上将这个SSC通道注册到eventLoop中， 也就是在这个注册操作中，完成了IO事件的轮询及SSC到Selector的注册. register方法会最终调用AbstractNioChannel对象的doRegister方法，如下所示，在doRegister方法中调用了java nio原生的register方法，将创建的SSC注册到相应的selector（多路复用器）中. 这里值得一提的是，在register的时候，设置的interestOps为0， 这意味着当selector进行轮询的时候，并不会监听SSC的任何IO事件，那么netty又是怎么实现监听客户端的连接的呢（这种情况下，要注册的InterestOps为OP_ACCEPT），这个问题会在后面提到. 3. 绑定操作回到ServerBootStrap的doBind操作， 在完成了InitAndRegister操作后，serverBootStrap会继续调用doBind0操作， 这个操作会沿着pipeline传播到unsafe对象，并由unsafe调用java原生的bind方法执行绑定操作. SSC监听客户端的连接在前面的描述中我们提到，在SSC注册到多路复用器的时候，设置的interesOps为0，那么它是怎么实现监听连接事件的呢？ 回到之前的注册方法， 将SSC注册到多路复用器之后，netty会通过pipeline触发channelRegister的事件，这个事件会沿着pipeline往下传播，还记得SSC在初始化的时候，往pipeline里设置的那个ChannelInitializer处理器吗？这个处理器会处理channelRegister事件，并往pipeline中添加一个ServerBootstrapAcceptor的处理器，这个处理器会监听channelRead事件，而客户端的连接请求就是在这个处理器中被处理的. 可以看到，在serverBootstrapAcceptor的处理中，在初始化新建的客户端连接之后，将它注册到了workerEventLoopGroup之中. 那么，现在的问题就变成了，这个channelRead事件是在哪里被触发，又是怎么被传递给这个sba的呢？回头看“标准”实现代码，服务端启动后，是通过selector不断轮询的方式，来及时地处理客户端连接以及其它的IO事件. 那么在netty实现的代码中，肯定也有哪些地方完成了这些操作，执行那些操作的地方也必然会触发这个channelRead事件，并传递给sba. 再回头来看看绑定部分的代码. 在SSC发起bind操作之后，它会将绑定操作委托给它的pipeline, 后者又进一步委托给channelHandlerContext(关于channel, channelPipeline, channelHandler, ChannelHandlerContext之间的关系，可以参考这里.）查看channelHandlerContext的绑定操作，我们会发现以下这样的代码，这段代码在netty的源码中经常可以看到，简单来说，它的意思是判断一下当前操作的线程是不是当前channel绑定的那个eventLoop线程，如果是就直接执行相应操作，否则，将要执行的操作包装成一个task扔给eventLoop线程中的任务队列，在它方便的时候执行. 这样的机制保证了netty中任何一个channel的所有事件，都是由同一个eventLoop线程执行的, 即使是在同时有多个channel的情况下，也可以保证同一个channel的所有事件是按顺序执行的，而不用考虑多线程情况下的竞争条件和锁等问题（这个实现后续会在其它文章中进一步说明，这里点到为止） 在了解了以上的执行方式之后，我们就会发现channelHandlerContext的bind操作是在ssc的eventLoop中被调用的，因此它要执行的任务也是被包装成了task对象放到eventLoop的消息队列中等待执行. 继续查看eventLoop的代码，终于看到了熟悉的for循环轮询操作，这段代码的实现逻辑把任务分成两种类型，一种是IO事件（select操作），还有一种是消息队列中的任务. 两者执行的时候由ioRatio分配. 继续追踪代码，可以发现在processSelectedKeysOptimized方法中调用了处理selectionKey的方法，后者在处理accept事件时，会调用unsafe的read方法，这个方法最终会调用doReadMessage方法，doReadMessage中我们再次看到了熟悉的“标准”代码， SSC先是执行了accept操作，然后为每个新建立的客户端连接创建NioSocketChannel对象，并把这些对象作为pipeline的channelRead事件发布，这些事件会被ServerBootstrapAcceptor处理，完成客户端的连接. 现在还剩下最后一个问题，SSC初始化时，注册到selector中的interesOps为0， 那它是什么时候修改了这个interesOps的呢？（要不然即使进入了processSelectedKey方法，也是处理不了accept事件的啊）. 再回头来看看SSC的注册过程吧，在SSC完成第一次注册后，SSC会触发channelActive事件，这个事件会最终触发channel的doBeginRead操作，在这个方法中，会根据SSC在生成时设置好的interesOps来修改注册参数. 到此为止，我们已经从netty源码中找到了所有服务端启动的关键步骤. 总结一下： ServerBootStrap中包含了整个服务端启动过程中需要的所有配置参数 SSC构造函数中完成了SSC初始化， 并设置了相应的pipeline， pipeline中预置了客户端连接事件的处理器(ServerBootstrapAcceptor)，在后续的initAndRegister方法中完成了包括注册到selector, 关联eventLoop, 启动监听线程，设置interesOps等操作 在NioEventLoop的轮询中，会将新建立的客户端连接作为channelRead事件传播，并最终由ServerBootstrapAcceptor处理 参考文献 服务启动的代码","categories":[],"tags":[]},{"title":"java集合学习之Queue的实现5","slug":"集合/java集合学习之Queue的实现5","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/集合/java集合学习之Queue的实现5/","link":"","permalink":"http://yoursite.com/2017/01/25/集合/java集合学习之Queue的实现5/","excerpt":"","text":"java集合学习之Queue的实现5Queue接口这个接口的类继承图如下所示，可以看到，这个接口主要有三种实现， AbstractQueue、Deque以及BlockingQueue（在图中未展示）. 这个接口只定义了6个方法，可以分成两组，一组在操作无法完成时会抛出异常，一组在操作无法完成时会返回特定的值， 如下，分别为add/remove/element和offer/poll/peek AbstractQueue抽象实现AbstractQueue代表了Queue接口的抽象实现，它定义了add/remove/element方法，底层调用了相应的抽象方法offer/poll/peek AbstractQueue有一个具体的实现为PriorityQueue, 它的底层是使用“最大堆”来实现的， 具体的实现过程可参考（演示动画） Deque接口Deque接口可以认为是”double end queue”， 从它提供的接口中也可以看出，这是双向都可以进行操作的队列实现， 观察它的接口方法也可以知道，基本是把queue接口的方法分别在头和尾两端定义相应的方法即可，如queue接口中的add方法，到了deque接口就有addFirst和addLast方法，queue接口中的peek方法对应了deque接口中的peekFirst和peekLast方法等等. Deque接口实现包括LinkedList和ArrayDeque. 其中LinkedList的实现已经在LinkedList一章讲过，这里就不再赘述. 重点来讲讲ArrayDeque的实现. ArrayDeque顾名思义可以知道，这个类的底层是通过数组来实现的，但既然是个双向队列，必然会涉及到从两端进行操作. 在ArrayDeque中，最重要是addFirst/addLast以及pollFirst/pollLast方法，其它的方法都是通过这些方法定义的. 首先值得说明的是head和tail变量，任何时刻，ArrayDeque中元素的顺序都是从head开始，沿数组下标增长的方向前进，直到遇到tail所处的位置，如果在遇到tail之前，已经达到了数组的最后一个元素，则从头开始继续，直到遇到tail. 从后面的方法实现中也可以看到，所有与位置相关的操作都需要与数组长度进行掩码操作，这样可以循环地利用数组. 先从addFirst和addLast方法讲起，可以看到这个两个方法的实现并不复杂，通过当前head和tail的位置插入相应的元素即可，有个细微的差别，head是在当前位置的前一个位置上进行插入，而tail是在当前的位置插入后再往后移一个位置，换句话说，head的位置为下一次poll获取元素的位置，而tail的位置始终指向下一次要插入的位置. 另外，可以看到在获取位置的时候，均进行了掩码操作，这么做的原因是可以循环地利用数组. pollFirst/pollLast方法的实现逻辑也大致相同， 当底层数组被元素填满后，就会调用doubleCapacity将数组的容量扩展为原来的2倍，容量扩展后还要涉及到数组元素的拷贝. 由于调用doubleCapacity的时候，数组已经被元素填满了，即tailf==head，从doubleCapacity的实现中看到，在将数组大小翻倍后，它会将head右边所有的元素拷贝到新数组的前面，然后将head左边的所有元素拷贝到后面. 前面我们说过，arrayDeque中维护了head和tail变量，任意时刻，元素的顺序为从head开始，沿数组下标增加的方向前进，直到遇到tail, 这也是为什么先拷贝右边的元素，然后再拷贝左边元素的原因. 明白了addFirst, addLast, pollFirst以及pollLast方法的实现后，再来理解offerFirst, offerLast, removeFirst和removeLast就比较容易了，实现如下，不做过多阐述. 不管是增加还是删除，前面讲的都是在两端执行操作，在arrayDeque中还定义了针对某个元素的操作，如删除某个元素，实现如下，这段代码的逻辑是遍历整个deque, 在找到元素后，执行delete操作 delete方法的实现如下，可以看到，删除元素后，要对剩余的元素进行挪动。这个方法中为了让挪动的元素最少， 以被删除的元素为界，分为前半段和后半段， 哪段元素数量少就挪哪一段. 以前半段元素少为例（即满足front","categories":[],"tags":[]},{"title":"java集合学习之Map和Set的实现4","slug":"集合/java集合学习之Map和Set的实现4","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/集合/java集合学习之Map和Set的实现4/","link":"","permalink":"http://yoursite.com/2017/01/25/集合/java集合学习之Map和Set的实现4/","excerpt":"","text":"java集合学习之Map和Set的实现4Map接口并不继承自Collection接口，它是一系列key-value值的集合，它的类继承图如下， AbstractMap作为map接口的抽象实现，它实现了map接口中的通用方法，如size, get, putAll等方法. 这个类只定义了一个抽象接口，entrySet， 其它所有的方法都是通过这个方法返回的值来完成的，实现逻辑比较清晰. 从containValue和containsKey的实现中也可以证实， abstractMap的key和value值都是可以包含null的，并且这个实现不是线程安全的，继承自这个抽象类的包括HashMap、TreeMap， LinkedHashMap， 所以这些类都继承了相应的性质. HashMapHashMap的底层使用“链表数组”来存储节点数据, hashMap中定义了两个变量，loadFactor与size， 前者定义了哈希表中容量达到“多满”时，对哈希表进行“重新哈希”操作，后者记录了哈希表中的entry数量. 当往hashmap中插入新的entry(key, value)时，会执行以下的操作： 根据key值计算其哈希值，hash(key), 这里返回的就是key对象的hashCode值 在插入之前，首先判断用于存储节点的数组是否为空，如果为空的话则进行resize操作（后续会重点阐述这个方法） 然后根据hash(key)与n-1进行“与”操作，得到该entry所属的bin位置（即数组中的位置） 如果bin中的值为null, 那么新增的节点为该bin的第一个节点，直接创建新的节点，并将该节点放在该bin的首节点位置（即数组中的元素） 如果bin中的值不为null, 则依次遍历链表上的结点，如果该结点的hash(key)与key值都与要插入的(key, value)相等，那么就执行更新操作，否则为插入操作. 在上面的阐述中我们提到，HashMap的实现中有两个很重要的变量，一个是init capacity，初始的容器大小， 一个load factor， 负载系数. 两者乘积的结果决定了这个hashMap最多能容纳的entry数量，一旦超过这个数量，hashMap就会执行一次rehash，导致整个哈希表的容量扩展为原来的两倍. 默认的load factor为0.75. resize方法可以分为两个部分，第一部分是根据当前容器的状态（oldCap, oldThr, oldTable)来确定新的容器大小(newCap, newThr), 第二部分则是根据第一部分计算得来的值申请新的存储空间，并对现有的entry进行“重哈希”操作. 首先获取bin的数量大小(oldCap)，只要这个值不为0，且不超过最大数量(MAXIMUM_CAPACITY), 就直接将bin的数量翻倍. 如果这个值为0，同时oldThr的值不为0， 说明这是第一次初始化bin大小，直接将oldThr的值赋予newCap即可. 如果两者均为0， 则直接使用默认值(16和12). 在获取到新的容器大小后，就可以直接分配新的数组， 并对现有的节点进行”重哈希“操作. 这里是个比较精妙的做法，详解如下： 首先每个节点的bin的位置是通过hash(key)与容器大小n-1进行”与“操作后得到的，由于n的值为2的幂次方，任何一个数与n-1进行”与“操作，等同于用这个值做掩码. 新的容器大小是原来容器大小的两倍，从位运算的角度来看，就是将1的位置左移一位即可 结合前面两点，重新做哈希的时候，只需要判断所有节点hash(key)在新的容量大小所在的那个1的那个位置上的值，如果是0，则意味着这个节点保持原来的位置（即代码中loHead的逻辑）, 否则（即该位为1）该结点与新的容器大小n-1进行与操作得到的值一定会发生变化，而delta值刚好等于oldCap， 即代码中hiHead的逻辑 关于这个，再具体举个例子，假设原来的bin数组长度为8，即n_old = 8 = 1000(2), 那么此时的n_old - 1= 7 = 0111(2) 此时如果进行resize操作，那么n_new = 8 * 2 = 16 = 10000(2)， 此时的n_new - 1 = 15 = 01111(2) 假设原来有个结点的hash(key) = 2 = 10(2)， 同时还有个结点的hash(key) = 18 = 10010(2)的结点，它们在resize前在数组中第3个bin的位置 ，而在resize操作之后，hash(key)=2的结点，由于它的第5个byte的值为0，它的位置保持不变，还在第3个bin，而hash(key)=18的结点，它的第5个byte=1，因此它的位置从原来的第3个bin移到了第19个， 两者的位置差刚好等于原来的数组大小n_old = 16. 我们经常会说，HashMap是线程不安全的，另外，它的遍历顺序是不确定的，甚至它的遍历顺序随着遍历时间的不同返回的顺序都是不一样的，从源码上看，hashMap的实现中没有任何并发方面的考虑，而它的遍历顺序是顺序着bin数组从前往后遍历，而bin数组中的链表会随着新结点的加入有可能发生变化(resize操作引起的)，因此它的顺序是不可预知的. LinkedHashMapLinkedHashMap派生自HashMap，也就是说，LHM的底层还是和hashMap一样，是通过数组加链表的方式存储着元素，但是， 它与HashMap最大的不同在于，它的遍历是有序的，或者说是可预测的. LHM为了保证遍历时的顺序性，除了链表数组，它还同时维护了一个双向链表，这个双向链表的顺序是随着访问或者插入的顺序而变化的(根据accessOrder参数指定）, 而在遍历的时候，只要依次遍历这个双向链表即可 . LinkedHashMap利用了hashMap的大部分方法，但是在一些方法上进行了重写，以维护双向链表. 以插入新的结点为例， 在hashMap的putVal中，调用了newNode来获取，LHM重写了这个方法，在返回新建的结点之前，通过linkNodeLast将新建的结点插入到双向链表的最后面, 这意味着，如果遍历的时候沿着这个双链表进行，就可以满足按插入顺序进行遍历的需求，事实上也是这么实现的. 删除结点的时候有一点不同，在hashmap的删除结点方法实现的最后 ，调用了afterNodeRemoval方法，这是hashMap定义的hook函数，也是在模板模式中常用的方式，这些hook方法允许子类在一些点上可以自定义类的行为，而LHM就是通过重写这个方法来实现结点从容器中删除后，同时也从双向链表中删除. HashTable前面提到了，基于AbstractMap实现的子类都是线程不安全的，在并发条件下使用存在问题，而hashTable实现解决了这一问题，它的实现机制很简单，甚至可以说是粗暴，就是在HashMap实现的基础上，对每个方法加关键字synchronized，以实现线程安全的目的, 可想而知，这样线程安全的实现性能是不会太好的. 另外，它与hashMap还有一点不同的是，它的key和value值都是不允许为Null的. TreeMap从类的继承图上可以看出， TreeMap实现了SortedMap和NavigableMap接口，前者规定了集合中的元素是按照key的”自然”顺序或者指定的comparator接口进行排序的，而NavigableMap接口定义了一系列遍历集合中元素的方法， 它的底层实现是通过红黑树结构完成的. （红黑树算法原理还没搞明白，所以 TreeMap暂时不分析源码了…）,另外，值得一提的是，这个类的实现也是线程不安全的。 Set在了解了Map及其子类的实现之后，再来理解Set及其子类的实现就变得非常容易. 从图中可以看出，set的继承图与map的继承图完全一样，每个map的实现都有一个对应的set实现（hashMap对应hashSet， TreeMap对应treeSet等等）. 事实上，set的底层实现就是通过其对应的map类型来实现的，从前面的描述可以知道，map的key值是不允许重复的， 但value值是可以重复的（重复的key值会覆盖），而set中的元素也是不能重复的，因此，只要把map的keySet当作对应的set实现就可以了. 因此，HashSet的底层实现是通过HashMap来实现的，LinkedHashSet的底层实现是通过LinkedHashMap来实现的， TreeSet的底层实现是通过TreeMap来实现的 关于set的实现就基本讲完了. 参考文献 http://liujiacai.net/blog/2015/09/03/java-hashmap/ 可视化 resize方法的讲解","categories":[],"tags":[]},{"title":"JVM的方法调用","slug":"JVM/JVM的方法调用","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/JVM/JVM的方法调用/","link":"","permalink":"http://yoursite.com/2017/01/25/JVM/JVM的方法调用/","excerpt":"","text":"JVM的方法调用关键字： 编译时静态多分派，运行时动态单分派 ”静态多分派“是指在编译阶段，编译器根据方法接收者以及参数的静态类型来决定选用的方法版本 ”动态单分派“是指在运行阶段，只根据方法接收者的实际类型来决定调用的实际方法 方法调用并不等于于方法执行，方法调用阶段唯一的任务就是确定被调用方法的版本（即调用哪一个方法） 在class文件编译的过程中，没有传统编译中的连接过程，也就是说，所有的方法调用在class中都是指向常量池符号引用. 1. 解析(Resolution）在类加载的解析阶段，会将其中一部分符号引用改成直接引用，但这么做的前提是，在编译阶段，具体要调用的方法的版本就已经确定下来了，这个过程被称为解析(Resolution). 符合这种“编译时确定，运行时不可变”的方法包括以下几种，这些方法都被称为是”非虚方法“, 它们的解析过程是静态的，在编译阶段就被完全确定，在类加载时的解析阶段就会把相应的符号引用变成直接引用. 私有方法(private) 类静态方法(static) 实例构造器() 父类方法 final方法 2. 分派(Dispatch)分派可以是静态的，也可以是动态的（这里的静态和动态是指分派时依据的参数类型是静态类型还是动态类型），根据分派时依据的宗量数不同，又可以分为单分派和多分派；两者结合就得到了静态单分派、静态多分派、动态单分派以及动态多分派等分派类型. JAVA是一种“编译时静态多分派， 运行时动态单分派”的语言， 这句话的理解包括两个部分：静态/动态, 单分派/多分派; 静态/动态: 静态动态是指在分派时根据方法接收者和参数的哪种类型（静态类型还是动态类型）进行的，如果是根据方法接收者和参数的静态类型决定，那么就被称为是静态分派， 否则就是动态分派 单分派/多分派: 单分派和多分派是根据分派时依据的宗量数来定义的，如果分派时根据多个宗量来确定，那么就被称为是多分派， 否则就是单分派 编译时静态多分派： 在JAVA编译器进行编译时，编译器会同时根据方法接收者以及参数的静态类型选择方法， 静态多分派最直接的例子就是overload（重载） 运行时动态单分派： 在运行时期， JAVA虚拟机会仅根据方法接收者的动态类型选择实际调用的方法， 动态多分派的例子是override(重写) 参考文献 http://wiki.jikexueyuan.com/project/java-vm/polymorphism.html 示例代码","categories":[],"tags":[]},{"title":"Bytebuf学习","slug":"IO/netty/Bytebuf学习","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/IO/netty/Bytebuf学习/","link":"","permalink":"http://yoursite.com/2017/01/25/IO/netty/Bytebuf学习/","excerpt":"","text":"Bytebuf学习ByteBuf同时支持随机和序列化的方式读取byte数组， 它是nio buffer以及byte数组的抽象视图类. ByteBuf的指针ByteBuf中定义了两个指针， readerIndex和writerIndex， 分别指示当前的读位置与写位置; 整个ByteBuf被这两个指标分隔成三个区域， 从起始位置到readerIndex为已读区域(discardable)， 从readerIndex到writerIndex为可读区域(readable)， 从writerIndex到结束为可写区域(writable)， 如下图所示 ByteBuf的读写 随机读写: 通过指定下标， 以随机的方式读写ByteBuf中的数据， ByteBuf提供了一系列set/get开头的方法，这些方法不会改变ByteBuf中的指针位置 顺序读写: 根据当前readerIndex和writerIndex的位置信息，通过readByte和writeByte进行读写操作 ByteBuf的指针操作 clear： 同时将rederIndex与writerIndex重置为0， 也就是意味着恢复到原始的状态，此时可读的字符数为0， 可写的字符数也为0 mark/reset: ByteBuf有两个指针， readerIndex和writerIndex， 因此对这两个指针分别提供了相应的mark/reset操作， mark操作会将当前的指针位置记录下来，后续执行reset操作的时候，会将指针重新指到这个位置. ByteBuf的视图操作ByteBuf提供了许多创建视图对象的操作，这些视图对象拥有独立的readerIndex和writerIndex指针，但和原始的ByteBuf共享同一个byte数组，这意味着，如果视图对象修改了其中的某些值(或者原始的bytebuf对象修改了某些值）， 这些值将反应到所有的视图对象中. 视图操作包括duplicate, slice, retainedDuplicate等等，具体可以参考javadoc. 其它操作 nioByteBuffer: ByteBuf可以通过nioByteBuffer转化成nio ByteBuffer对象 array: Bytebuf通过array方法转化成byte数组对象 这两个方法都会获取到底层的字符数组，同样地，如果修改了这些数组的值，相应的修改也会反应到byteBuf中 ByteBuf的层级结构 AbstractByteBuf： 这个是ByteBuf的抽象实现 ，实现了ByteBuf 接口绝大部分的功能. AbstractDerivedByteBuf： 这个实现也可以认为是实现了包装其它bytebuf对象的功能， 它的所有子类实现都和被包装类共享同一份数据; 它的子类包括ReadOnlyByteBuf, DuplicatedByteBuf以及SliceByteBuf AbstractReferenceCountedByteBuf: 这个抽象类是所有需要进行引用计数的bytebuf的基类，也可以认为是绝大部分ByteBuf实现的基类. 它的实现又可以大体分为Unpooled实现和pooled实现两大类，具体可参见javadoc SwappedByteBuf： 这个实现可以包装其它的ByteBuf实现类，它会将byteBuf对象的byteOrder进行对调. WrappedByteBuf： 这个实现是对其它的bytebuf对象进行包装，实现类似于装饰器模式的功能 示例代码 https://github.com/Essviv/nio/blob/master/src/main/java/com/cmcc/syw/practise/BytebufPractise.java","categories":[],"tags":[]},{"title":"WEB前端学习备忘","slug":"前端/备忘","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/前端/备忘/","link":"","permalink":"http://yoursite.com/2017/01/25/前端/备忘/","excerpt":"","text":"备忘学习路线： https://www.zhihu.com/question/31059577 学习JS的路线: http://blog.xiayf.cn/2013/03/25/learning-js/ JS推荐教材： Eloquent Javascript HTML和CSS推荐教材： Head First HTML &amp; CSS jquery推荐教材：锋利的jQuery 学习路线： 学习HTML与CSS（Head first HTML &amp; CSS） 学习JS（Eloquent Javascript） 学习bootstrap 学习锋利的jquery 实践（每天至少一个小时，持续三个月， 完成个人网站） 深入学习JS（javascript高级程序设计）","categories":[],"tags":[]},{"title":"设计模式总述","slug":"设计模式/设计模式总述","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/设计模式/设计模式总述/","link":"","permalink":"http://yoursite.com/2017/01/25/设计模式/设计模式总述/","excerpt":"","text":"总述参考文献 https://sourcemaking.com/design_patterns/facade http://design-patterns.readthedocs.io/zh_CN/latest/structural_patterns/adapter.html","categories":[],"tags":[]},{"title":"注解","slug":"JVM/注解","date":"2017-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/25/JVM/注解/","link":"","permalink":"http://yoursite.com/2017/01/25/JVM/注解/","excerpt":"","text":"注解元注解元注解是注解的注解，JAVA中提供了四种元注解 @Target: 它是用来标识注解的使用范围，可选的范围包括包、类型、方法、构造方法、成员变量、局部变量以及方法变量等 @Retention: 它是用来标识注解的生命周期，可选的生命周期由RetentionPolicy定义，可选的值包括源代码、CLASS文件以及运行时 @Docuemnted: 它是一个标记注解，用来标识该注解会出现在javadoc中； 如果没有使用这个注解，那么相应的注解就不会出现在javadoc @Inherited: 它也是个标记注解， 用于标识注解是否会被子类继承. 如果某个类被带有@Inherited注解的注解标记时，则该注解会被子类继承 值得注意的是，@Inherited只会被子类继承，但不会从接口、重载方法等地方继承注解 自定义注解使用@interface定义注解时，自动继承了annotation接口 定义注解的语法： public @interface 注解名称{} 关于注解的定义有以下几点需要关注： 注解的每个方法声明一个配置参数， 方法的名称就是配置参数的名称， 方法的返回值就是配置参数的类型 可以通过default来声明参数的默认值 注解支持的参数类型为： 基本数据类型、Class、Enum、Annotation 注解元素必须有值，要么在定义注解时设置默认值，要么在使用注解时设置相应值，不能为空或者NULL. 因此在定义注解时，一般会给每个配置参数设置相应的默认值, 通常情况下，负数或者空字符串代表这个参数不存在 注解处理器一般是在工具框架中、或者反射的方式进行处理 注解的作用 编译检查： 如@Override 反射 生成帮助文档 相关的类图 参考文献 http://www.cnblogs.com/peida/archive/2013/04/24/3036689.html http://www.cnblogs.com/skywang12345/p/3344137.html","categories":[],"tags":[]},{"title":"Synchronized关键字","slug":"多线程/Synchronized关键字","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/多线程/Synchronized关键字/","link":"","permalink":"http://yoursite.com/2017/01/24/多线程/Synchronized关键字/","excerpt":"","text":"Synchronized关键字 synchronized提供的是排它的、不公平的、可重入的锁，这是JAVA对象内含的锁对象. Keep in mind that using synchronized on methods is really just shorthand (assume class is SomeClass) 1synchronized static void foo() &#123; ...&#125; is the same as 1static void foo() &#123; synchronized(SomeClass.class) &#123; ... &#125;&#125; and 1synchronized void foo() &#123; ...&#125; is the same as 1void foo() &#123; synchronized(this) &#123; ... &#125;&#125; happens-beforehappends-before关系是一种保证关系，它保证一个语句的执行结果对另一个语句可见，其本质是一种可见性保证。以下几种情况均存在这种关系: 在同一个线程中执行的语句，自动拥有happends-before的关系 对锁的unlock操作和随后对同一个锁的lock操作有happens-before关系，由于这种关系有传递性，因此在unlock之前的所有操作对lock之后所有的操作可见 对volatile变量的写操作和随后对它的读操作有happens-before关系 线程的start操作和线程中执行的所有操作有happens-before关系 被调用了join操作的线程, 它执行的所有操作对调用该(join)操作的线程有happens-before关系 参考文献 https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html#MemoryVisibility http://blog.sina.com.cn/s/blog_4ae8f77f0101iifx.html","categories":[],"tags":[]},{"title":"snowflake","slug":"工具/snowflake","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/工具/snowflake/","link":"","permalink":"http://yoursite.com/2017/01/24/工具/snowflake/","excerpt":"","text":"Snoaflake snowflake算法背后的思想是： 时间 + 空间 + 序列号","categories":[],"tags":[]},{"title":"Spring的IoC框架","slug":"Web/Spring的IoC框架","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/Web/Spring的IoC框架/","link":"","permalink":"http://yoursite.com/2017/01/24/Web/Spring的IoC框架/","excerpt":"","text":"容器的含义Spring中的ApplicationContext继承自BeanFactory， 除了提供了BeanFactory的功能外，还额外提供了依赖管理，消息、生命周期监听等等功能，它就是所谓的“容器” Bean定义Bean在容器的定义由BeanDefinition定义. 具体的内容包括： 完整的类名 类的依赖关系 类的生命周期回调 类的其它属性 bean的命名 每个Bean可以由一个id, 多个name来命名；必要的时候还可以通过alias来定义别名 对于没有声明名称的bean，spring容器会自动为它生成一个名称，生成的名称遵循Java规范，将类名的首字母小写，而后遵守camelCase原则 Bean的初始化从BeanDefinition的内容中可以看出，bean的类名是不可缺少的元素，在spring的配置中，这个属性是通过class属性来定义的. 可以通过三种方式来指定class属性的值： 直接指定由容器创建的类名，这种情况有点类似于用new操作符直接生成对象 通过其它类的静态工厂方法生成相应的类，这种情况下，class方法为静态类的类名，而通过factory-method指定的静态方法返回的对象类型由为实际生成的对象类型. As a rule, use the prototype scope for all stateful beans and the singleton scope for stateless beans. 通过其它类的方法生成相应的类，这种情况下，class属性的值为空，factory-bean为带有生成方法的对象名，factory-method为生成相应对象的方法名，而真正定义的对象类型则为factory-method返回的类型. 这种方法与类的静态方法生成bean对象基本类似，只不过前一种是通过静态类方法实现，而这种是通过对象方法实现. 依赖注入在Spring的语境中，依赖注入是指在容器中生成相应的对象后，由容器根据配置信息自动完成bean之间依赖关系的注入，而不是由bean自己去维护和管理相应的依赖关系的做法。在spring中，可以通过两种方式进行依赖注入，通常情况下，通过构造函数进行那些必须的依赖关系的注入，而使用setter方法完成那些可选的依赖关系的注入. 1. 构造函数使用构造函数完成依赖注入与使用带有参数的静态类工厂方法来实现依赖注入是一样的道理，因此在后面的描述中，不对这两种情况进行更多的区分. 使用构造函数完成依赖注入的时候，需要完成参数类型的解析. 在类型不会千万歧义的情况下（即所有参数的类型都不相同），提供的参数会自动按照相应的类型提供给相应的构造函数，而不需要显式地指定参数的位置 但是如果遇到无法自动完成类型的解析时，则可以通过type属性显式地指定参数的类型 也可以通过index参数直接指定参数的位置，这样提供的bean将会作为第index个构造参数提供给构造方法 2. setter方法顾名思义，setter方法就是在spring构造完对象后（通常是调用类的无参数构造函数或静态类的不带参数的方法生成的），通过set方法完成相应属性的注入. 依赖注入配置的细节 在配置文件中，属性的值总是通过string类型进行表达的，但实际上的参数类型并不一定是string,两者之间的转换可以通过ConversionService来完成 可以通过p命名空间很方便地实现属性地赋值 通过idref标签可以引用到相应的bean，这种引用方式让容器可以在部署的时候就检查引用的对象是否真实存在，而不是等到运行时再检查 作用域 singleton: 每个容器单实例 prototype: 原型，每次请求时都会创建一个新的实例; 在这种作用域情况下，容器只负责初始化bean，一旦初始化结束，容器就不再管理这个bean，后续的维护包括资源的回收等问题，全部交由客户端自行维护，因此对于这种作用域的对象来讲，它的析构函数并不会被容器调用，如果客户端需要执行这些操作，可以考虑使用PostProcessor接口来完成.另外，如果在singleton作用域的对象中注入了prototype对象，这种引用关系是在初始化的时候就引入了. 这意味着，当singleton对象被初始化完之后，内部的prototype类型的对象就不会再发生变化; 因为初始化操作只会发生一次，如果需要在运行时每次获取新的prototype对象，则需要spring提供的’方法注入’方式来解决 application: 每个servletContext一个实例，它与singleton的区别在于它的范围是servletContext， 而singleton是在容器范围 request: 每个请求一个实例 session: 每个会话一个实例 globalSession, webSocket： 前者是用于PortletServlet，后者用于webSocket，略","categories":[],"tags":[]},{"title":"equals和hashCode方法的使用约定","slug":"JAVA基础/equals和hashCode方法的使用约定","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/JAVA基础/equals和hashCode方法的使用约定/","link":"","permalink":"http://yoursite.com/2017/01/24/JAVA基础/equals和hashCode方法的使用约定/","excerpt":"","text":"Java的Object类中提供了两个重要的方法，equals和hashCode， 在map和set中这两个方法起到了至关重要的作用，以下是使用和重写这两个方法时的约定 如果两个对象通过equals比较的结果为true, 那么它们的hashCode也必须一样 如果两个对象的hashCode相等，那么这两个对象不一定相等（即equals不一定返回true） 默认情况下，equals方法调用了==操作符，也就是说，它只会在指向同一个对象时才会返回true, 而hashCode默认是将对象的地址转化成唯一的整型数字. 在“Effective Java”有这样一条建议： 如果你重写了equals方法，那么一定要重写hashCode 错误的例子： 这里只重写了equals方法，但没有重写hashCode，这导致在这个类被用作map的key时，出现了一些不正常的情况（事实上这个是和hashMap的实现有关系的） 正确的例子，这个例子中的类继承自上面那个类，但是它重写了hashCode类，保证了当equals返回true时，hashCode也能返回相同的值，解决了上述的问题 事实上，上述问题与hashMap的内部实现有着很大的关系，关于hashMap的源码解析，请见HashMap源码解析","categories":[],"tags":[]},{"title":"RabbitMQ的HA方案","slug":"消息队列/rabbitMQ/RabbitMQ的HA方案","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/消息队列/rabbitMQ/RabbitMQ的HA方案/","link":"","permalink":"http://yoursite.com/2017/01/24/消息队列/rabbitMQ/RabbitMQ的HA方案/","excerpt":"","text":"RabbitMQ的HA方案默认情况下，queue可以认为是只存在于它被声明的那个节点中，但是broker和binding可以认为存在于集群中的所有节点中. 可以通过镜像的方式，将queue复制到其它的节点中，以此来提高可用性 镜像队列之间彼此形成了一主多从的关系，当主镜像队列因为某些原因消失时，一个从镜像自动被推选为主镜像 不论客户端连接到哪个结点，它都将连接到主镜像队列中，所有队列的操作也都是通过主镜像队列来完成，这样就保证了队列的FIFO特性 发布到主镜像队列中的消息将会被自动镜像到所有的从镜像中 如果主镜像中的消息确认已经被消费了，那么从镜像会自动将该消息删除 这种镜像的方式并不能将流量分散到各个节点，因为每个节点做的事情是一样的，但是它提高了可用性，如果主镜像队列因为某些原因消失了，那么从镜像可以自动升级为主镜像，保证了队列的可用性 配置镜像 队列镜像的配置是通过policy来完成的，通过ha-mode和ha-params参数来配置 ha-mode = all, ha-params = null: 所有的队列将被镜像到集群中的所有节点中 ha-mode = exactly, ha-params = count: 所有的队列将被准确地镜像count份，如果集群中的节点少于count个，那么将会被镜像到所有的节点中 ha-mode = nodes, ha-params = [node_names]: 所有的队列将被镜像到列表中提供的节点中，节点的名称为rabbitmctl cluster_status中列出的节点名 主队列镜像的位置也是可以配置的，可以在声明队列时设置，也可以在配置文件中设置，也可以通过配置policy中的queue-master-locator参数来完成，可选的参数有： min-masters: 将集群中拥有最少主队列镜像的那个节点作为主镜像所在的节点 client-local: 将声明这个队列的节点作为主镜像所在的节点 random: 随机挑选集群中的节点 如果新配置的policy导致原先的主镜像所在的节点不在队列的镜像集群中，那么RMQ会保持原来的主镜像所在的节点，直到新的主镜像完成同步后，才会将原来的主镜像队列删除。 1如果原来的队列Q被镜像到了[A, B]节点中，其中A节点为主镜像所在的节点，现在配置了新的policy，将队列中镜像到了[C, D]中，那么RMQ的操作将会是，保留[A, C, D], 直到C或者D完成了队列的同步后，再把A从镜像队列中删除，最后变成[C, D] 排它队列: 由于具有排它性的队列当声明它的连接(connection)关闭时，会被自动删除，因此这种类型的队列永远也不会被镜像，而且它也不会被持久化，一旦连接关闭，它就会被自动删除，因此镜像和持久化都是没有意义的。 具体的配置可以通过rabbitmctl的set_policy来完成 ，也可以通过RMQ的管理界面来完成 节点间的同步机制和故障转移1. 同步机制节点可以选择在任意时刻加入到集群中，当一个节点新增到集群中时，它的镜像队列是空的，随后所有新增的消息都会自动发布到这个新的节点中，但之前队列中已经有的消息并不会自动出现在这个新的队列镜像中，但是随着时间的推移，这部分队列头部的消息被逐步消费掉，等到它们被全部消费掉的时候，新增节点中的消息将和其它节点的消息完全一致， 这种同步的机制被称为“自然同步”(natural synchronization) 当然也可以选择显式地同步消息队列，但是由于同步会导致队列暂停响应，因此，建议只针对那些不活跃的队列进行显式同步，而对于那些活跃的队列只需要使用上述的方式进行同步即可 显式的同步有两种方式，一种是手动同步，一种是自动同步，可以通过设置ha-sync-mode参数来完成，分别为manual和automatic. 在RMQ3.6.0之后，可以通过设置ha-sync-batch-size参数来批量同步，默认每次只同步一条消息. 2. 故障转移当集群中的主镜像掉线时，集群中的某个从节点将会被推选为主镜像，如果此时集群中没有其它节点，那么该节点中那些被设置为持久化的消息将被持久化到硬盘，如果重启了该节点，这些消息将会被恢复； 如果某个从镜像原先属于某个集群中，那么当这个从镜像重启时，它将会重新加入到\b集群中，但是由于此时这个从镜像并不知道自己队列中的内容和主\b镜像中的内容是否一致，因此重新加入集群中意味着它将抛弃之前持久化的所有消息，以保持与主镜像内容的一致性 当关闭主镜像时，如果此时集群中所有的从节点都处于未同步的状态，那么要分两种情况： 如果关闭请求是来自可控制的事件(如RMQ服务器关闭或者系统关闭）那么RMQ会拒绝进行故障转移，此时它会直接将整个队列关闭，从而避免数据的丢失，但是这时候会造成队列的不可用 如果关闭请求是来自不可控制的事件(如服务器崩溃或节点掉线) 那么即使所有的从节点都处于未同步的状态，那么集群还是会自动进行故障转移，此时保障了集群的可用性，但是有可能会千万数据的丢失 如果想保证集群的可用性，在任何情况下都进行故障转移，那么可以设置ha-promote-on-shutdown参数为always, 这时在任何情况下，只要主镜像关闭了，即使集群中所有的从节点都处于未同步的状态，集群也将进行故障转移操作. 当整个集群全部下线时，最后一个下线的节点必须是最先重新启动的节点，因为它是整个集群最后的主镜像，如果它没有被最先启动，那么所有启动的从镜像将会等待30秒，然后失败； 如果需要将这个主镜像从集群中移除，那么可以通过rabbitmqctl设置forget_cluster_node\b参数, 关于这个参数含义的解释可以参阅参考文档中的“Loss a master while all slaves are stopped.”一节。 HA方案下消息发送确认及事务的语义HA方案下，消息发送确认及事务的语义是指所有的镜像队列都确认消息发送或者事务提交完成，也就是说，只要集群中的某个镜像没有对发送的消息进行确认，那么发布者将会被暂停，直到这个镜像发送了确认，或者集群中的其他镜像节点认为这个镜像已经掉线（掉线的检测是RMQ通过间隔一定时间的心跳检测来完成的) HA方案下，发布者的流量控制是也是通过这种方式来实现的，只有当集群中所有的镜像都允许发布者发布消息的时候，发布者才可以继续发布消息，否则它只能继续等待或者直到集群中的其它节点认为该节点已经掉线 有些时间，消费者可能会需要知道集群中的某个节点掉线的通知，这可以通过设置x-cancel-on-ha-failover参数来实现，具体可以参见文档 HA方案下节点掉线的处理和语义 如果是从镜像掉线，那么主镜像依然是主镜像，其它的从镜像也不会得到任何通知或者采取任何操作 如果是主镜像掉线，那么以下的操作将会被执行： 最老的那个从镜像将会被推选为新的主镜像，因为这样才能使丢消息的概率降到最低 如果所有的从镜像都没有处于“已同步”的状态，那么那些只存在于主镜像中的消息将会丢失 新的主镜像会认为之前所有到主镜像的连接都已经被突然中断，因此它会把所有没有收到消费确认的消息重新推到队列中，不管这种没有确认是由于网络中断造成的，还是由于原来的主镜像没有把相应的确认消息同步给它们造成的，这种做法可能会导致消费者重新消费到一条它已经处理过的消息 所有到原来的主镜像的连接都将被取消 不管是主镜像掉线还是从镜像掉线，都不会影响到发布者的发布确认，因此从发布者的角度来讲，发布消息到镜像集群中和发布消息到其它任何类型的队列中是没有区别的. 另外，在掉线期间发布的消息也不会丢失，因为所有的消息都是直接发布到镜像集群中的所有节点中 参考文档 HA方案","categories":[],"tags":[]},{"title":"阻塞锁与自旋锁","slug":"多线程/juc/AQS框架/阻塞锁与自旋锁","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/多线程/juc/AQS框架/阻塞锁与自旋锁/","link":"","permalink":"http://yoursite.com/2017/01/24/多线程/juc/AQS框架/阻塞锁与自旋锁/","excerpt":"","text":"阻塞锁与自旋锁在学习JAVA并发包的时候发现其底层的实现是通过AQS框架来完成的，而AQS框架中维护了一个CLH队列，CLH队列使用了CLH锁，因此上网搜了下这方面的内容，发现原来在并行编程中有这么多的锁类型，索性做个总结，此为本篇内容的缘由. 1. 阻塞锁阻塞锁是指当线程尝试获取锁失败时，线程进入阻塞状态，直到接收信号后被唤醒.(线程的状态包括新建、就绪、运行、阻塞及死亡）在JAVA中，能够唤醒阻塞线程的操作包括Object.notify, Object.notifyAll, Condition.signal, LockSupport.unpark(JUC中引入） 阻塞锁的优点是在线程获取锁失败后，不会一直处于运行状态（占用CPU）， 因此在竞争激烈的情况下， 阻塞锁的性能将明显优于自旋锁 2. 自旋锁自旋锁的特性是，当线程尝试获取锁失败时（锁已经被其它线程占用了），它不会将线程切换为沉睡状态，而是开启无限循环，不断地轮询锁的状态（这也是“自旋”的来历），当锁状态被更改时，获取到锁并进入临界区. 自旋锁本身并没有关注公平性、可重入性等特性，另外，由于自旋锁的实现需要不断轮询锁的状态，因此需要占用CPU，适用于临界区时间很短的场景. 自旋锁又可进一步细分为排队自旋锁(TicketLock)、MCS锁以及CLH锁， 其中排队自旋锁解决的是公平性的问题 排队自旋锁： 排队自旋锁关注的是公平性问题，每个线程在尝试获取锁的时候都会被分发唯一的一个号码，而锁本身提供了服务号服务，类似于现实生活中的排队叫号服务，每个线程都有个号码，锁对象每次叫一个号，拿到相应事情的线程获得锁. 排队自旋锁虽然解决了公平性问题，但由于所有的线程都在读写同一个变量(服务号），因此每一次读写操作都必须同步处理器缓存，会导致大量的总线流量 MCS锁：MCS锁采用了链表的形式对尝试获取锁失败的线程排队，MCS锁自旋的是自身的本地变量，它是显式的队列，有真实的后继节点. CLH锁： CLH锁和MCS非常类似，但CLH锁自旋的是前驱节点的变量, 它是隐式的队列，没有真实的后继节点 关于自旋锁的实现代码可以参见这里 3. JUC中的相关实现在java的并发包中，提供了LockSupport类，它提供了阻塞与唤醒的原语操作. 事实上，JUC中的AQS框架就是基于CLH队列实现，但是它还使用LockSupport类进行了改良，使原来的CLH自旋锁变成了阻塞锁. 使用LockSupport类实现CLH阻塞锁的代码可以参见这里 关于JUC中的AQS框架源码的分析，可以查看这里 参考文献 CLH锁学习： http://googi.iteye.com/blog/1736570 自旋锁的比较与实现： http://coderbee.net/index.php/concurrent/20131115/577 JAVA锁的系列文章： http://ifeve.com/java_lock_see1/ MCS锁的文章： https://www.ibm.com/developerworks/cn/linux/l-cn-mcsspinlock/ JUC系列文章：http://www.cnblogs.com/skywang12345/p/java_threads_category.html 示例代码 各种锁的实现示例: https://github.com/Essviv/spring/tree/master/src/main/java/com/cmcc/syw/concurrency/lock","categories":[],"tags":[]},{"title":"Memcached","slug":"缓存/Memcached","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/缓存/Memcached/","link":"","permalink":"http://yoursite.com/2017/01/24/缓存/Memcached/","excerpt":"","text":"Memcached命令格式123456 &lt;command&gt; &lt;key&gt; &lt;flags&gt; &lt;expireTime&gt; &lt;bytes&gt; &lt;datablock&gt;e.g. set name 0 0 8 sunyiwei command: 要执行的命令，如set, get, replace等 key: 操作的键名称 flags: 客户端存储的键值对之外的信息 expireTime: 过期时间，设置为0则为永久有效 bytes: 数据长度 datablock: 实际的数据内容 执行命令（CRUD） C: add, set R: get, gets U: set, cas, replace D: delete 统计命令 stats: 当前服务器运行的统计信息 stats items: 内存模型memcached中将内存模型主要有三个概念，page, slab和chunk，如下图所示 Page: 内存分配的单位，即每次memcached申请内存的大小，申请得到的内存将分配给相应的slab进行使用 Slab: memcached中将一组相同大小的chunk归为一组，称为slab， 每个slab中的chunk大小都是相同的，并且只负责一定大小范围内的数据存储 Chunk: 固定大小，它的大小即为所在slab的最大存储尺寸；memcached中数据实际存储的地方，同一个slab中的chunk大小均相同，但不同slab中的chunk的大小可以不相同 memcached的执行参数12345678910111213141516171819-p &lt;num&gt; 设置TCP端口号(默认不设置为: 11211)-U &lt;num&gt; UDP监听端口(默认: 11211, 0 时关闭) -l &lt;ip_addr&gt; 绑定地址(默认:所有都允许,无论内外网或者本机更换IP，有安全隐患，若设置为127.0.0.1就只能本机访问)-d 以daemon方式运行-u &lt;username&gt; 绑定使用指定用户运行进程&lt;username&gt;-m &lt;num&gt; 允许最大内存用量，单位M (默认: 64 MB)-P &lt;file&gt; 将PID写入文件&lt;file&gt;，这样可以使得后边进行快速进程终止, 需要与-d 一起使用-vv 用very verbose模式启动，将打印相应的调试信息和错误信息-h 打印本帮助文档例如： ./usr/local/bin/memcached -d -u root -l 192.168.1.197 -m 2048 -p 12121 几点说明 memcached内存中的数据并不会被删除，当记录到期后，memcached会采用lazy expiration的策略将这条记录置为不可见，同时，这条记录所占用的空间可以被重复使用; 另外，lazy expiration的策略也会导致memcached并不会花时间在检查记录的过期上 memcached默认使用的是LRU(Least Recently Used)的策略来删除数据，可以通过-M参数禁用该机制 参考 memcached的内存模型： http://xenojoshua.com/2011/04/deep-in-memcached-how-it-works/ memcached和redis在内存管理方面的对比： http://www.biaodianfu.com/redis-vs-memcached.html 缓存的设计: http://data.qq.com/article?id=2879 协议说明： https://github.com/memcached/memcached/blob/master/doc/protocol.txt CheatSheet: http://lzone.de/cheat-sheet/memcached","categories":[],"tags":[]},{"title":"迭代器模式","slug":"设计模式/迭代器模式","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/设计模式/迭代器模式/","link":"","permalink":"http://yoursite.com/2017/01/24/设计模式/迭代器模式/","excerpt":"","text":"迭代器模式迭代器模式中涉及到两个关键的组件， 集合类与迭代器. 迭代器简化了遍历集合类元素的操作，使得外界可以不用了解集合类中的数据组织情况就可以访问其中的所有元素， 它的UML图比较简单，如下所示， UML图中定义了两个抽象接口，Aggregate和Iterator， 分别对应于上面所说的集合类和迭代器，集合类中提供了相应的方法来获取迭代器，而迭代器中提供了相应的方法来遍历集合类中的元素. 以JAVA中提供的集合框架为例，Collection接口就是这里的Aggregate接口，而Iterator接口就对应这里的Iterator接口，collection接口中提供了iterator方法来获取相应的迭代器，而iterator接口中提供了hasNext， next, remove等操作来遍历和访问集合中的元素. 示例代码 迭代器实现","categories":[],"tags":[]},{"title":"多线程进阶","slug":"多线程/多线程进阶","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/多线程/多线程进阶/","link":"","permalink":"http://yoursite.com/2017/01/24/多线程/多线程进阶/","excerpt":"","text":"多线程进阶 锁对象synchronized关键字内部使用的是一种较为简单的重入锁机制，可以通过Lock接口来实现相应的功能。Lock接口的好处在于在尝试获取对象锁时，可以直接返回或者等待一段时间后再返回，更为灵活 另外，读写锁提供了更为精巧的方式来控制同时读写的问题，它特别适合于解决读写互斥，写写互斥，但读读不互斥的情景。 在使用Lock接口时，获取锁对象后，注意要把解锁的操作（unlock）放在finally块中，否则锁对象将被一直占用，导致后续的线程无法继续 Executor这部分内容可分为三部分，分别为： Executor接口： executor, executorService, ScheduledExecutorSerivce 线程池 Fork/Join框架 1. Executor接口 Runnable, Future, Callable: Runnable是用来运行任务，它没有返回值，相对地，callable有返回值，并且有可能会抛出异常；Callable调用时会返回future,可以通过future对象来控制线程的状态，比如取消，等待结果等等 ExecutorService: 在executor的基础上增加了对线程管理和线程池管理的函数，并且可以执行callable线程（Executor只能执行runnable线程，且没有管理生命周期的功能） 2. 线程池线程池可以重复地利用已经存在的线程，一方面可以节省重复创建线程的时间，另一方面也可以防止大量的创建线程导致系统资源耗尽。一般情况下，线程池需要配合阻塞队列来实现，这样，当外部提交执行任务时，只需要往队列里塞入相应的任务即可，在线程池内部的线程处于空间状态时，会自动地去消费队列中的任务，如果此时队列为空，则阻塞等待，其实质是“生产者-消费者” 3. Fork/Join框架待理解 异步集合 BlockingQueue: 阻塞队列，从队列中获取元素时，它会阻塞或超时(javadoc) ConcurrentMap: map的异步实现，它将增加、删除或者修改键值的操作原子化，从而避免了同步操作(javadoc) ConcurrentNavigableMap: 支持模糊匹配的map实现，可以认为是treeMap的异步实现(javadoc) 原子变量对原子变量的操作都是原子性的，也就是说，set操作和之后的get操作有happens-before的关系 异步随机数略 参考资料 http://tutorials.jenkov.com/java-concurrency/locks.html http://docs.oracle.com/javase/tutorial/essential/concurrency/newlocks.html http://tutorials.jenkov.com/java-concurrency/thread-pools.html http://docs.oracle.com/javase/tutorial/essential/concurrency/pools.html hashtable和concurrentMap的比较","categories":[],"tags":[]},{"title":"memo","slug":"集合/memo","date":"2017-01-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/24/集合/memo/","link":"","permalink":"http://yoursite.com/2017/01/24/集合/memo/","excerpt":"","text":"memo需要进一步深入学习的点： BlockingQueue CocurrentHashMap BlockingDeque 以上内容可以从这里找到答案 hashCode和equals： http://tutorials.jenkov.com/java-collections/hashcode-equals.html","categories":[],"tags":[]},{"title":"ThreadPoolExecutor源码解析","slug":"多线程/juc/线程池/ThreadPoolExecutor源码解析","date":"2017-01-04T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2017/01/04/多线程/juc/线程池/ThreadPoolExecutor源码解析/","link":"","permalink":"http://yoursite.com/2017/01/04/多线程/juc/线程池/ThreadPoolExecutor源码解析/","excerpt":"","text":"ThreadPoolExecutor(TPE)源码解析1. Ctl变量TPE的状态由ctl变量定义和追踪，ctl的低位（29位）定义了当前正在运行的工作线程数，而高位（3位）定义了当前线程池的状态 Running: 正在运行，并且接受新的任务 Shutdown: 不接受新的任务，但会执行已提交的任务 Stop: 不接受新的任务，也不会执行已提交的任务 Tidy: 清理工作线程的阶段 Terminated: 线程池已完全停止 2. 配置参数 CoolPoolSize: 核心线程数，当目前线程池中线程数小于这个值时，新的任务提交进来，线程池会创建新的线程来执行这个任务 MaximumPoolSize: 线程池中最大的线程数，线程数达到这个数时，再有新的任务提交进来，会调用RejectHandler接口执行拒绝操作；可以将这个值设置为Integer.MaxValue，表示不对线程池的线程数做限制 BlockingQueue: 当目前线程池中线程数超过CoolPoolSize, 但没有达到MaximumPoolSize时，线程池会优先将新提交的任务缓存到队列中，当入队失败时（或达到队列的上限），才会考虑创建新的线程；可以使用LinkedBlockingQueue这样的实现 ，可以“无限制”的将任务入队，这种情况下，MaximumPoolSize参数无效 RejectHandler: 当目前线程池中的线程数超过MaximumPoolSize的值时，会调用RejectHandler接口，JCU中提供了多种现成的实现 Hook方法： 当线程池执行每个任务时，会调用beforeExecute方法，当执行完每个任务后，会调用afterExecute方法；这两个hook方法允许子类在任务前后执行特定的操作. 参考文章 ThreadPoolExecutor源码分析","categories":[],"tags":[]},{"title":"JUC","slug":"多线程/juc/JUC","date":"2016-12-29T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/29/多线程/juc/JUC/","link":"","permalink":"http://yoursite.com/2016/12/29/多线程/juc/JUC/","excerpt":"","text":"JUC并发包又可以进一步细分为： 原子操作 线程池 并发集合 锁及工具类 其中， 原子操作和LockSupport共同提供了CAS操作机制，它们共同为AQS的实现提供了基石. 而AQS框架进一步可以被封装成各种Lock实现，为线程池、并发集合提供支持. AtomicOperation, LockSupport ===&gt; AQS ===&gt; Lock ====&gt; Executor、Concurrent Collection","categories":[],"tags":[]},{"title":"Redis和Memcached的区别","slug":"缓存/Redis和Memcached的区别","date":"2016-12-22T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/22/缓存/Redis和Memcached的区别/","link":"","permalink":"http://yoursite.com/2016/12/22/缓存/Redis和Memcached的区别/","excerpt":"","text":"Redis和Memcached的区别 支持的数据类型不一样Redis除了支持key-value之外，还支持Set, sortedSet, List以及Map类型，这些不同的类型使得redis可以应对更丰富的场景;Memcached只支持key-value的形式，因此它比较适合做缓存的场景 网络IO模型redis采用单线程IO复用模型，所有的命令是流水线式地进行处理；memcached采用非阻塞IO复用模型，分为监听主线程和worker子线程，主线程负责监听来自客户端的连接，并将接收到的请求交由worker线程进行处理。多线程模型可以充分发挥多核的作用，但也由此引入了竞争条件和锁的问题. （e.g. stats） 内存模型redis使用现场申请内存的方式来分配内存，也就是说，当需要存储数据时，redis会通过malloc等方式申请内存；而memcached是采用预分配内存池的方式，在系统启动时，会按照一定的间隔分配slab和chunk， 每个slab中的chunk块大小是固定的，每次存储数据时，选用能放下数据的最小chunk块， 这种方式可以快速地分配内存，但很容易造成内存碎片， 当slab中的chunk被使用完时，会申请page（1M）大小的内存，然后分配给slab，再按指定的chunk大小进行细分. 持久化redis提供了aof以及rdb的方式进行持久化，而memcached没有相应的持久化机制 集群管理redis更倾向于通过服务端的分布式存储构造集群，同时也提供了主从备份的苏通; 而memcached本身不支持分布式，只能通过客户端通过像一致性哈希等算法进行分布式存储。 参考文献 http://gnucto.blog.51cto.com/3391516/998509 http://blog.jobbole.com/101496/","categories":[],"tags":[]},{"title":"java集合学习之通用接口2","slug":"集合/java集合学习之通用接口2","date":"2016-12-20T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/20/集合/java集合学习之通用接口2/","link":"","permalink":"http://yoursite.com/2016/12/20/集合/java集合学习之通用接口2/","excerpt":"","text":"java集合学习之通用接口2Collection接口collection接口定义了通用的集合操作，简单说来，可以分为： 增加操作: add, addAll, 往集合中增加相应类型的元素或者把另一个集合中的元素全部加到当前集合中 删除操作: remove, removeAll, clear, 删除集合中指定的元素或者删除出现在指定集合中的所有元素, 元素的相等通过equals方法来判断 查询: contains, containsAll, 判断集合中是否含有指定的元素或者是指定集合的超集, 元素的相等通过equals方法判断 size, isEmpty: 查询集合的元素个数 修改: retainAll, 保留指定集合中的所有元素, 可以认为是数学意义上集合的“交集”操作 遍历: iterator, 迭代器模式的应用，整个集合框架中都使用了相同的方式来进行遍历操作 转换: toArray，这个方法是集合类与数组类之间的桥梁，将集合类转化成相应的数组. 同时，这个方法也必须保证返回的数组能够被“安全”地使用，换句话说，如果返回的数组被修改了，也不能影响到原来的集合类. collection接口的操作相对简单，意义也非常明确，不存在理解上的难点. Comparable接口comparable接口只提供了一个方法，就是compareTo(T)， 所有实现了这个接口的类都是可以用来进行比较操作的. Comparator接口Comparator接口提供了compare方法，该方法接收两个参数，分别是用来比较的两个参数，对于那些没有实现comparable接口的对象来讲，comparator实现了“策略模式”, 让这些对象相互之间可以进行比较， 比如在调用sort方法的时候，如果用于排序的对象本身实现了comparable接口，就可以直接使用对象本身提供的compareTo方法进行比较，对于那些没有实现comparable接口的对象而言，可以在sort方法中提供一个comparator接口作为参数，从而通过“策略模式”完成排序. 示例代码 collection comparable comparator","categories":[],"tags":[]},{"title":"java集合学习之接口1","slug":"集合/java集合学习之接口1","date":"2016-12-20T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/20/集合/java集合学习之接口1/","link":"","permalink":"http://yoursite.com/2016/12/20/集合/java集合学习之接口1/","excerpt":"","text":"java集合学习之接口1java集合框架的内容包括四个部分， 分别为接口，实现， 聚合操作和算法. 本章只详述接口部分的内容。 接口： 接口可从宏观上分为两类， 分别为map和collection， collection又可细分为set, list, queue, deque.在理解接口的时候，可以从以下几个方面来理解： 接口提供了哪些功能 接口有哪些具体的实现，以及它们之间的区别 接口可以有哪些操作 Collection接口collection接口中包括了集合最基本的操作，比如size, add, remove, iterator, isEmpty, contains等等操作，不同的collection实现类可以通过构造函数很方便地进行类型转换.如下: List&lt;String&gt; strings = new LinkedList&lt;String&gt;(); Set&lt;String&gt; stringSets = new HashSet&lt;String&gt;(strings); 遍历collection的方式 在JDK1.8中，可以通过聚合操作来完成 collection.stream().filter(e –&gt; e.getColor == Colors.RED).forEach(e –&gt; doSth(e)) 使用forEach操作 for(String s : collection){ doSth(s); } 使用iterator操作: 这里的iterator使用了迭代器模式，可以借此机会复习下迭代器模式，在遍历的过程中，使用迭代器的remove方法也是唯一能够安全地操作collection中的元素的方法，如果在遍历的过程中使用其它的方式改变了collection中的元素，其行为是不可预知的. Iterator iter = collection.iterator(); while(iter.hasNext()){ doSth(iter.next()); } Set接口Set接口定义了元素不能重复的集合类，它可以认为是数学意义上的集合，在JAVA的集合框架中有三种不同的实现： HashSet: 底层使用HashMap进行存储，事实上HashMap的keySet就是这个hashSet, 它不保证集合遍历的顺序，但这是效率最好的实现 TreeSet: 底层使用红黑树存储，根据它们的值进行排序，效率比hashSet略慢 LinkedHashSet: 底层使用hashMap以及双向链接进行存储，它能够保证元素遍历的顺序与插入的顺序一致 既然把set接口认为是数学意义上的集合，很显然就可以对它进行交集、并集、求差等操作，事实上set接口中的retailAll和removeAll方法就是用来实现这些功能的 List接口List接口定义了一组有序集合，集合中的元素可以重复，它提供了顺序访问以及搜寻功能，同时也提供了遍历和局部视图功能，可以取出集合中的某部分元素集合进行操作，在JAVA集合框架中，提供了两种实现： ArrayList: 底层使用数组来实现元素的存储，在绝大部分情况下，这种实现的性能是比较好的。 LinkedList: List的链表实现, 底层使用双向链表进行存储，这种实现在增删元素时性能更佳，但顺序访问时性能不好，因为需要遍历整个链表 Queue接口Queue是一系列准备用于处理的元素的集合，除了collection提供的方法之外，它还提供了额外的增改查操作，对于所有的增改查操作，queue接口都提供了两种实现方式，在操作失败的时候，一种是抛出异常，另一种是返回特定的值（如null或者false， 依不同的操作而定), 具体的操作如下： 其中，add， remove和element在操作失败的时候会抛出异常, 而与它们一一对应的offer, poll和peek在操作失败时则会返回false，另外remove/poll和element/peek的区别在于，前者从queue中取到元素后，会把元素从queue中删除，而后者则不会 Deque接口deque接口是可以从头尾进行增删改的队列接口，应该说它是queue接口的扩展，因为它同时实现了queue(FIFO）以及堆(LIFO)的功能，从它的提供的操作来看，也可以很清楚地看到这点，所有在queue中的六个操作(add, offer, remove, poll, element, peek)在deque都有头元素以及尾元素的实现，具体操作如下： 可以看到，所有的六个操作都有了两种针对头元素和尾元素的实现，但语义不变 Map接口map接口提供了将Key映射成value的对象，它可以认为是数学意义上的函数。它提供了基本的增删改查的操作以及相应的视图操作(put, remove, get, contains, size, empty, entrySet, keySet, values)等等 注意在map接口提供的三个视图中，keySet和entrySet都是set类型的，也就是说它们是不能重复的，但是values只是collection类型，这意味着它的值是可以重复的（这也是数学意义上函数的定义). 另外，在视图上的一些操作（如removeAll, retailAll等)都会影响到原来的map的内容,具体可以查阅HashMap的keySet方法的实现源码 在JAVA集合框架中也提供了三种实现： HashMap, TreeMap和LinkedHashMap.它们的语义及特点正如这些名字所指示的那样，和对应的三个Set(HashSet, TreeSet, LinkedHashSet)相同，事实上，对应的set在实现的时候，内部就是借助了Map的key不能重复的特点，直接将map的keySet进行使用 multimap的语义是它的每个Key值可以指向多个value, 在java的集合框架中并没有这种类型，事实上，这种主义的Map完全可以通过将值的类型设置为某种集合来实现，如Map&lt;String, List&lt;String&gt;&gt;. 因此，这种类型的map将不再被特殊讨论和对待. Comparable接口在进一步学习容器接口之前，有必要先了解下Comparable接口，顾名思义，这个接口定义了对象的比较属性，实现了这个接口的类就具备了可比较性，比较的语义由实现决定，在JAVA的实现中有很多类都实现了这个接口，比如String，按照字母顺序进行比较；Date类会按照时间顺序进行比较 Comparator接口这是另一个用来实现对象比较的接口，在一些集合类算法中，如果某些类对象没有实现comparable接口，在使用排序算法时，可以额外提供一个comparator接口来实现排序功能，事实上，comparator接口是一种策略模式的实现，策略模式的结构图如下: SortedSet接口SortedSet是一种set接口，但是它把元素按照升序进行排列，排序的规则由元素本身提供(自然排序)，前提是元素实现了Comparable接口，否则将返回类型转换错误; 如果元素没有实现comparable接口，也可以提供comparator接口，通过使用策略模式来进行排序SortedSet接口提供了几类操作：视图操作，端点操作以及获取内部使用的comparator的接口的操作 视图操作情况下，如果原来的集合中的元素被修改了，视图中的元素也会发生相应的变化，反过来也一样，也就是说，可以把视图当作集合的一个窗口，视图操作获取的元素只不过通过这个窗口能看到的原来的集合中的部分元素 端点操作默认是左闭右开区间，即包含头节点，但不包含尾节点，但可以通过在头节点或者尾节点后增加“\\0”来改变这种行为，如果头节点增加了这个标识，意味着头节点将不被包括在返回的集合中(左开区间），如果尾节点增加了这个标识，意味着它将会被包括在返回的集合中（右闭区间) SortedMap接口这个接口所有的操作和属性都和SortedSet一致，因此这里就略过不讲 在JAVA的集合框架中，提供了TreeSet来实现SortedSet接口 后续的学习集合框架的学习以源码分析为主，因为集合类的使用主要还是以接口类提供的方法为主，而每个集合类提供的接口完全可以由其特性猜出一二，不需要. 备注 具体的Sample代码可以参见这里 参考文档： 集合说明文档 系列文章： 集合系列文章","categories":[],"tags":[]},{"title":"java集合学习之List的实现3","slug":"集合/java集合学习之List的实现3","date":"2016-12-20T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/20/集合/java集合学习之List的实现3/","link":"","permalink":"http://yoursite.com/2016/12/20/集合/java集合学习之List的实现3/","excerpt":"","text":"java集合学习之List的实现3在Java集合框架中，对于List接口的实现主要有两种， ArrayList和LinkedList. 前者底层是基于数组的实现，后者底层是基于双向链表的实现. ArrayList底层是通过Object[]数组来存储数据， 从arrayList的add方法可以看到，当往ArrayList中添加新的元素时，它会先确保当前的数据容量大小，如果长度不够，则会将数组长度*1.5后复制数组. 既然arrayList以数组作为底层存储，那么它也同样拥有数组访问的特性，即随机读取的效率较高，但增加和删除数据的效率较低，因为要复制数组中的元素 arrayList中还有个重要的变量modCount， 这个变量存储了ArrayList结构被修改的次数，每次增加或者删除数组元素都会引起这个变量值发生变化，在迭代器进行遍历的过程中，这个变量的值保证了不会有其它的线程并发地修改数组的内容，如果有的话，则会引发fast-fail机制. 可以看到，在迭代器进行迭代的过程中，会调用checkForComodification方法， 这个方法就是判断当前的modCount与创建迭代器时的值是否一致 ，如果不一致，则会抛出ConcurrentModificationException. 从ArrayList迭代器的实现中可以看出，在迭代器遍历的过程，如果有其它线程修改了arrayList的结构（增加或者删除元素，但不包括修改元素的值）则会引起ConcurrentModificationException， 但是，如果调用的是迭代器的remove方法则不会有这个问题，从remove方法的实现中可以看到，在数组中移除相应的元素后，迭代器会重新设置expectedModCount的值，这样就能避免抛出异常的问题. LinkedList从名字上可以看出，这个List实现是通过链表的方式来完成的，在LinkedList中定义了链表中的节点Node, 可以看到这个节点定义了前向、后向节点以及当前节点的元素对象. 而在LinkedList的定义中，只定义了first, last节点对象，分别来代表链表中的头尾结点. LinkedList定义了一系列的link和unlink私有方法，这些方法完成了链表相关的所有操作. 从源码中可以看出，first(last结点类似)最开始的时候为null, 当新增结点时，会将first结点指向这个新的结点，同时修改size和modCount的值. 另外，这里可以看到，在新建结点时，first结点的prev设置为null, 也就是说，first结点的前置结点为空，因此这里是双向链表，但不是循环链表.unlink方法将指定元素从链表中移除，这个方法将指定结点与前后结点的链接关系去除，并且重新构建了前后结点的关联关系，逻辑相对简单，不做过多阐述. 有了link和unlink方法后，就可以很方便地实现list接口中定义的方法，只需要在链表中特定的位置上加入或删除相应的结点即可. 另外，LinkedList中还提供了node方法，它是用来支持随机访问的，可以看到，在遍历链表中的节点时，如果查询的index小于链表长度的一半，LinkedList会尝试从前往后找，如果大于长度的一半，则会尝试从后往前找. 对于像add(index, elem)或者像remove(index, elem)这些方法来讲，都需要先调用node方法找到index位置的那个结点，然后才能开始新增或者删除的操作. 由些可见，LinkedList适合于在链表头或者链表尾增加或者删除结点，但不适合于随机访问较多的场合. 值得一提的是，LinkedList不仅仅实现了List接口，它还实现了Deque接口，这个接口定义了双向队列的操作，从前面的描述中也能看到， LinkedList很适合从头或者尾进行操作的场景. 因此可以很方便地实现Deque接口中定义的方法(element, peek, add, remove, offer, poll） ArrayList和LinkedList的比较 ArrayList的底层是用数组实现的，读取任意元素的时间复杂度均为O(1), 因此它非常适合于随机访问比较多的场景，但是由于新增和删除元素都涉及到数组元素拷贝，它的新增和删除操作的性能不如LinkedList. LinkedList底层使用链表实现，新增和删除链表中的结点只需要更改前后结点的prev和next指针指向即可，因此可以很方便地在链表中的任意位置进行新增和插入操作，但是，对于随机访问的场景，由于需要从头结点或者尾结点开始遍历查找，因此性能不如ArrayList. Vectorvector也实现了List接口，同时它的底层也是通过数组实现的，但和ArrayList不同的是，Vector是线程安全的，在进行操作的时候，vector会使用synchronized关键字进行同步，因此它的性能要比ArrayList低很多, 对于不需要考虑多线程的场景，建议使用ArrayList 参考文献 ArrayList和LinkedList源码解析 ArrayList与Vector的比较","categories":[],"tags":[]},{"title":"ReentrantReadWriteLock源码分析","slug":"多线程/juc/AQS框架/ReentrantReadWriteLock源码分析","date":"2016-12-16T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/16/多线程/juc/AQS框架/ReentrantReadWriteLock源码分析/","link":"","permalink":"http://yoursite.com/2016/12/16/多线程/juc/AQS框架/ReentrantReadWriteLock源码分析/","excerpt":"","text":"ReentrantReadWriteLock源码分析在之前的源码分析 中，ReentrantLock展示了排它锁的实现，而CountDownLatch和Semaphore则展示了共享锁的实现，接下来，我们要一起看看ReentrantReadWriteLock(RRWL)的源码，它同时实现了独占锁和共享锁，对于写操作而言，它是独占锁；对于读操作来讲，它是可共享的. 首先看下RRWL的结构，可以发现，它分别提供了readLock和writeLock, 分别用于实现读锁和写锁的相应功能，后续所有的操作也是委托给这两个锁对象进行操作；再进一步查看可发现，这两个锁对象都是由sync对象提供支持的，在RRWL内部提供了sync抽象类的两种实现，公平锁和不公平锁. 也就是说，和之前一样，sync类还是实现整个RRWL机制的关键，接下来就来看看它的具体实现。 1. Sync对象的内部结构 Sync对象将state变量分成两个部分，高16位作为共享锁的数量，低16位作为独占锁的数量， 这也是RRWL的读写锁最大支持65535（2^16-1)的原因. sync类还定义了HoldCounter和ThreadLocalHoldCounter内部类，前者定义了某个线程重入共享锁的次数，后者从名字上可以看出是线程私有的变量， sync定义了cachedHoldCounter变量（类型为HoldCounter）和readHolds变量（类型为ThreadLocalHoldCounter）, 分别用于记录最后一次成功获取读锁的holdCounter对象和线程私有的holdCounter对象. sync还定义了firstReader和firstReaderHoldCount, 用于记录第一个获取到共享锁的线程与它的重入次数. 2. 获取独占锁获取独占锁的逻辑为判断当前锁的状态，只有在可重入（当前线程拥有写锁）或读写锁均没有被占用的时候，才尝试获取锁 如果当前有读锁或者（当前有写锁且占用写锁的线程不是当前线程）， 直接返回. 如果当前线程拥有写锁，此时为重入，直接返回成功 当前没有读锁和写锁(state=0)， 判断是否应该阻塞（公平性策略，稍后提到），如果不需要阻塞，则尝试更新状态，状态更新成功则设置独占线程，写锁获取成功，否则获取独占锁失败 3. 释放独占锁由于同一时间只会有一个线程拥有独占锁，因此释放独占锁的操作不需要考虑并发的情况，释放逻辑也很简单，只要在确认调用该操作的线程为拥有写锁的线程的前提下，更改锁的state状态变量的值即可. 这里的逻辑与ReentrantLock重入锁的逻辑是一模一样的. 4. 获取共享锁获取共享锁的实现逻辑如下： 如果当前已经有线程占用写锁，且占用的线程不是当前线程，直接返回获取共享锁失败 尝试获取共享锁，如果获取共享锁成功，则进入计数器相关的操作 尝试获取共享锁失败，则进行fullTryAcquireShared的操作 4.1 尝试获取共享锁成功当尝试获取共享锁成功时，首先判断当前的线程是否为第一次成功获取读锁的线程（r==0）,如果是，则设置firstReader与firstReaderHoldCount的值; 如果不是, 则获取cachedHoldCounter变量，并判断是不是当前线程的holdCounter，如果不是，则从readHolds中重新获取. (这里使用cachedHoldCounter变量的作用是尽量减少map的查找，因为绝大部分情况下，下一次释放共享锁的线程就是上一次获取共享锁的那个线程. ), 然后把计数器加1并返回成功. 4.2 尝试获取共享锁失败如果尝试获取共享锁失败，则进行fullTryAcquireShared方法的操作. 这里的逻辑也可以分为三个部分： 如果当前已经有线程占用了写锁，且该线程不是当前线程，直接返回获取共享锁失败 如果公平策略中要求读线程要阻塞，则只有一种情况会继续获取共享锁，那就是重入操作，同样也可以分两种情况 当前线程是第一个获取共享锁的线程，直接进入获取锁的操作. 如果不是第一个获取共享锁的线程，则判断它的holdCounter的次数，只要不为0，说明这个是一次重入操作，则尝试获取锁; rh.count==0意味着这个是新的线程在尝试获取共享锁，由于需要保证公平性，则尝试获取共享锁失败. 如果公平策略不要求线程阻塞（即不需要保证公平性），则直接进入获取共享锁的操作 获取共享锁的操作，包括获取成功后更新计数器的操作都和之前的逻辑一样，这里不作赘述. 5. 释放共享锁释放共享锁的操作分为两个部分，首先是减少计数器，然后是通过for循环，不断地尝试更新state变量的值，直到成功为止. 逻辑相对比较简单，略. 6. 公平性RRWL的实现中，提供了公平性策略，在获取读写锁的时候，均可以设置公平性策略. FairSync和NonFairSync分别对应了公平锁和非公平锁的实现. 6.1 非公平的RRWL实现先来看看NonFairSync的实现. 可以看到，公平性是通过两个方法来提供的，writerShouldBlock和readerShouldBlock方法，对应写锁和读锁的获取时的公平性策略. 可以看到，在非公平的锁机制中，写锁是不需要阻塞的，也就是说，在尝试获取写锁时，可以马上尝试获取而不用阻塞等待，这就有可能造成后来的写锁获取请求比等待队列中的写锁获取请求更快拿到写锁，可能会造成写线程”饥饿”的情况. 而在获取读锁的过程中，虽然也是不公平的，但有一点需要保证，就是排队的首节点不是写请求，这样实现是为了防止写请求“饥饿”. 这里可细分为两种情况，一种头结点是写请求，那么后续的读请求都必须进行等待队列（除非是重入操作）另一种头结点是读请求，那么后续再进来的读请求会一直尝试获取读请求. 6.2 公平锁的实现公平锁的实现逻辑比较简单，在尝试获取锁（不论是共享锁还是独占锁）之前，先判断下当前的队列中是否已经有其它线程在排队等候了，如果有，直接进入自旋等待操作. 参考文献 http://www.cnblogs.com/leesf456/p/5419132.html http://brokendreams.iteye.com/blog/2250866 http://blog.csdn.net/yuhongye111/article/details/39055531 http://www.cnblogs.com/chenssy/p/4922430.html","categories":[],"tags":[]},{"title":"LockSupport与Condition的使用","slug":"多线程/juc/AQS框架/LockSupport与Condition的使用","date":"2016-12-16T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/16/多线程/juc/AQS框架/LockSupport与Condition的使用/","link":"","permalink":"http://yoursite.com/2016/12/16/多线程/juc/AQS框架/LockSupport与Condition的使用/","excerpt":"","text":"LockSupport与Condition的使用LockSupportLockSupport提供了线程阻塞与唤醒的原语操作(primitive)，它是JUC的锁机制的基础.每一个使用LockSupport的线程都有一个许可，在该许可可用的情况下，调用park操作会占用该许可，并直接返回, 否则将阻塞等待; 而其它的线程则可以通过unpark方法唤醒处于阻塞中的线程. ConditionCondition对象提供了和await, notify, nofityAll类似的功能，但是它要和Lock一起使用. 参考文献 http://cmsblogs.com/?p=1735","categories":[],"tags":[]},{"title":"CountDownLatch, Semaphore, CyclicBarrier源码解析","slug":"多线程/juc/AQS框架/CountDownLatch, Semaphore, CyclicBarrier源码解析","date":"2016-12-15T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/15/多线程/juc/AQS框架/CountDownLatch, Semaphore, CyclicBarrier源码解析/","link":"","permalink":"http://yoursite.com/2016/12/15/多线程/juc/AQS框架/CountDownLatch, Semaphore, CyclicBarrier源码解析/","excerpt":"","text":"CountDownLatch, Semaphore, CyclicBarrier源码解析java并发包下提供了AQS框架，使得锁的实现变得非常容易. 之前我们分析了ReentrantLock的源码（这里），我们知道这个是可重入的、公平性可选的独占锁. 简单回忆一下，在线程尝试获取锁对象时，RL底层会委托给sync对象进行处理，sync对象派生自AQS抽象类，并实现了AQS类中独占锁的两个方法, tryAcquire和tryRelease. 在获取锁的时候，如果锁被占用，则构建新的CLH队列节点并等待，直到其它的节点将它唤醒. 这里我们接着来分析java并发包中提供的另一种类型的锁，共享锁. 顾名思义，这种锁允许多个线程共同持有. 在并发包中，信号量semaphore和计数器(?). CountDownLatch的底层都是基于共享锁来实现的. 有了之前阅读独占锁源码的经验，我们还是直接从共享锁的具体实现入手. 这里以信号量为主进行分析. 1. 信号量信号量定义了同一时间最多能有多少个线程获取到锁，超过这个数量时，尝试获取线程的锁会进行等待. semaphore对象在初始化时，需要传入一个数量，这个数量意味着能同时获取共享锁的线程数量（也被称为是许可数量）. 这个数量最终被用来构造AQS类，并成为AQS类中state的初始值.事实上，每当有线程尝试获取共享锁时，semaphore就会把state变量的值减掉相应的许可数量，state数量为0意味着当前许可全部发放出去了，当前持有共享锁的线程数达到饱和，后续再有线程尝试获取共享锁时，就需要等待，直到其它的线程释放了许可. 2. 获取共享锁semaphore的实现与ReentrantLock重入锁保持了一致的风格，底层都是委托给sync对象. acquire方法调用了AQS抽象类的acquireSharedInterruptibly方法. 这个方法中首先判断线程是否被打断，然后尝试获取共享锁，如果获取共享锁失败（返回负值），那么将当前线程构建CLH节点后进入等待状态， 直到获取共享锁成功为止. semaphore的sync对象也提供了两种公平性支持， 公平锁和非公平锁. 这里以公平锁的实现为例. 在公平锁的实现中，尝试获取锁之前，首先调用hasQueuedPredecessors方法，判断是否有其它线程等待的时间超过当前线程，如果有，直接返回获取失败. 这样的机制保证了锁的公平性，等待时间越长的线程，在尝试获取锁时拥有越高的优先级. 这个方法中，h!=t意味着队列不为空，也就是目前已经有等待中的线程了. 而下一个条件判断目前队列的第一个节点是否为当前线程. 两个条件同时成立就意味着，目前等待的队列不为空，且第一个节点的线程不是当前线程，也就意味着当前已经有其它线程在等待了. 在公平性判断完成后，tryAcquireShared方法会判断当前可用的许可数量及申请的许可数量，如果数量足够就尝试获取，数量不够就直接返回. 非公平锁只要把公平性判断的逻辑移除就可以了，但在一些极端条件下，有可能会有导致线程饥饿的情况出现. 如果尝试获取共享锁失败，说明要么已经有优先级更高的线程，要么就是可用的许可数量不够了，那么尝试获取共享锁的线程进入排队等待逻辑， 这部分是通过doAcquireSharedInterruptibly方法实现的. 这个方法首先调用addWaiter方法构建新的CLH节点并进行入队操作（具体可参见独占锁关于这部分的描述这里，这里略），接着开始“自旋”的逻辑，也就是for循环的内容. 循环中的逻辑可以分为两个部分. 第一部分判断当前节点是否为头节点的下一节点, 这意味着当前节点是队列的首节点，可以继续尝试获取共享锁. 如果获取锁成功，则将释放信息往下传播；否则判断当前节点是否需要进入等待状态，并根据返回结果进行相应的处理. 如果当前节点是CLH队列的头节点，并成功获取到共享锁时，semaphore会调用setHeadAndPropagate方法将释放信息继续往下传播. 这个方法会将当前节点置为头结点，并通知下一个结点. 这也是共享锁和独占锁最大的区别，独占锁在同一时间只会有一个线程占用锁对象，因此在释放锁的时候，只需要唤醒后继节点即可，而共享锁需要将释放的信息由队列传播，这样队列中的节点才有可能同时获取到共享锁. 传播释放信息的代码在doReleaseShared方法中, 可以看到，这里判断头结点的状态是否为signal，是的话就将它重置成0后唤醒后继节点，如果是处于重置状态的头结点，则将它置为传播状态，以供后续传播使用.注意这里只会在头结点状态为signal的时候尝试将release信息传播给队列中的下一个结点，但它本身并不会去改变头结点的位置. 当release信息传播之后，会唤醒队列中的下一个结点，如果这个结点获取共享锁成功，才会调用上面的setHeadAndPropagate方法，这时候头结点的位置才会发生变化. 如果尝试获取共享锁失败，则会进行判断是否等待的逻辑. 这部分逻辑和RL重入锁部分的逻辑是一样的，都是根据前置节点的状态（CLH锁的特点）决定是否需要执行park操作. 3. 释放共享锁semaphore中关于释放共享锁的代码就是在上面说的doReleaseShared方法中实现的，上面已经讲过了，这里不再赘述. 4. CountDownLatch前面以信号量为例，讲解了java并发包中的共享锁的实现. CountDownLatch底层也是共享锁，只不过做了点小小地改动. CountDownLatch在构造函数中接受一个数值N作为参数，这个数值N也被当作是state的值，此时可以认为countDownLatch被N个对象共享. 当有线程调用countDown时，底层的实现其实是调用releaseShared释放一把共享锁，而调用await意味着尝试获取锁. 与一般共享锁不一样的地方是，在CountDownLatch的实现中，只有当N个对象都被释放后（即调用了N次countDown操作, state=0），获取共享锁的操作才会成功. 这也就意味着，调用了N次countDown之后，await方法才会返回. 这也是CountDownLatch作为计数器最常见的使用场景. 5. CyclicBarriercyclicBarrier也被称为是栅栏，它为多个线程协同操作提供了类似于“到达点”一样的功能，多个线程必须都到达代码中的某个时点后，才可以继续进行，否则必须等待其它线程的到达. 它的实现比较简单，底层是通过ReentrantLock重入锁和Condition对象实现同步，同时，它也可以作为学习Condition类的示例. 参考文献 http://www.infoq.com/cn/articles/java8-abstractqueuedsynchronizer http://www.cnblogs.com/zhanjindong/p/java-concurrent-package-aqs-AbstractQueuedSynchronizer.html 示例代码 https://github.com/Essviv/spring/blob/master/src/main/java/com/cmcc/syw/concurrency/lock/SharedLockTester.java","categories":[],"tags":[]},{"title":"AQS框架源码解析","slug":"多线程/juc/AQS框架/AQS框架源码解析","date":"2016-12-14T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/14/多线程/juc/AQS框架/AQS框架源码解析/","link":"","permalink":"http://yoursite.com/2016/12/14/多线程/juc/AQS框架/AQS框架源码解析/","excerpt":"","text":"AQS框架源码解析AQS类的全称是AbstractQueuedSynchronizer， 它的核心是通过维护一个名为state的整型变量和CLH队列（这里）， 子类通过修改状态变量的值来实现获取锁和释放锁的操作. AQS可以说是整个JUC并发包中的核心类，它是典型的模板模式应用（这里）， 同时，它也提供了实现Lock接口的基础. 事实上， JUC中的可重入锁(ReentrantLock)、可重入读写锁(ReentrantReadWriteLock)、信号量（Semaphore)以及栅栏（Barrier)等类的底层实现就是基于AQS类来完成的. 0. AQS实现概述查看整个AQS框架的源码可以发现，虽然整个类中定义了很多方法，但是绝大部分方法都是private或者final，这意味着这些方法都是无法被子类继承和重载的，整个模板类只定义五个方法供子类重写： tryAcquire: 定义了以独占方式获取锁的方法 tryRelease: 定义了释放独占锁的方法 tryAcquireShared： 定义了以共享的方式获取锁 tryReleaseShared: 定义了释放共享锁的方法 isHeldExclusivly: 判断当前锁是否被当前线程以独占的方式获取. 这五个方法默认的实现是抛出UnsupportedOperationException异常，子类可以根据需要选择相应的方法进行重写. 下面将以可重入锁（ReentrantLock）的实现为例对AQS类的源码进行阐述. 1. 可重入锁（ReentranLock, RL）可重入锁是一种排它锁（exclusive）， 当一个线程获取到锁对象后，其它的线程就无法再获取相应的锁对象. 这种锁是可重入的，意味着当一个线程可以多次获取锁对象，前提是它已经抢到这个锁了. 2. RL源码 —- Lock操作上面说到，RL的底层实现就是依赖于AQS模板类来实现的，查看RL的源码我们发现，它本身并不继承自AQS，但它定义了一个内部类Sync是AQS的子类. 我们还发现，RL中的方法都是调用了这个sync对象的相应方法完成的，因此这个sync对象应该就是阅读源码的重点. 以Lock接口的lock方法为例， RL中的实现调用了sync.lock()方法，而sync类的lock是个抽象方法，被它的两个子类实现（FairSync和NonFairSync）. NonFairSync的lock源码如下，在尝试获取锁对象时，它会尝试先调用AQS的comapreAndSetState方法来改变state状态变量的值(注意这里通过CAS机制保证了操作的原子性). 如果修改成功，意味着当前线程获取到了锁对象，将当前线程设置为独占线程后直接返回；如果修改失败，则调用AQS类的acquire方法，因为RL是独占锁，所以这里传入的参数值为1. AQS类的acquire方法实现如下，可以看到，这里首先尝试调用子类的tryAcquire方法，如果这个方法返回了true， 说明当前线程获取到了锁，直接返回，否则进入acquireQueued操作. 首先来看NonFairSync的tryAcquire实现： 它首先获取锁对象的状态，如果状态变量为0，说明此时没有线程在占用锁，则再次调用compareAndSetState方法设置状态变量的值，如果设置成功，说明当前线程获取到了锁对象，直接返回即可. 如果当前状态变量的值不为0，说明锁已经被占用了，则判断这个独占线程是否为当前线程（“重入”的体现），如果是，直接将state状态变量加上相应的状态值即可. 否则当前线程尝试获取锁失败. 如果tryAcquire方法返回false,说明尝试获取锁失败了，则进入acquireQueued的执行，这个方法主要的作用是实现CLH队列的“阻塞”操作. （CLH锁的内容可查阅这里 ) . 但值得注意的是，CLH锁是“自旋”锁，而acquireQueued方法里使用的是LockSupport类提供的“阻塞”锁机制. 从里往外看，addWaiter首先构建了一个新的等待节点， 接着判断队列的tail尾结点是否为空，如果它为空，意味着队列为空，则直接尝试入队操作, 入队成功直接返回，否则调用enq方法进行入队. 在enq方法中，通过for循环不断地轮询，如果tail结点为空，则初始化这个结点. 可以想像，当线程第一次尝试获取锁对象时会进入这个分支执行，然后初始化head与tail结点，而第二次进入for循环时，就会进入到else分支. 在这里会不断地尝试将新生成的node结点通过compareAndSetTail方法加到tail结点的末尾, 从而完成入队操作. 在enq方法返回后，addWaiter方法就返回了. 然后就会进入acquireQueued方法的执行. 这也是个死循环，每次进入循环后，都会首先判断当前节点的先驱节点是否为head节点，如果是，意味着已经轮到当前节点获取锁了（这里是“公平”锁的体现），于是开始尝试获取锁对象（tryAcquire的实现已经在前面说过了），获取成功则表示当前线程已经获取到锁对象，直接退出. 如果当前节点的先驱结点不是head结点或者尝试获取锁对象时失败, 意味着队列前面还有正在排队的结点，则进入shouldParkAfterFailedAcquire方法的执行. shouldParkAfterFailedAcquire方法的作用是根据先驱结点与当前结点的状态判断是否需要让当前线程进入阻塞状态. 如果先驱节点的状态是signal，则表示需要当前结点关联的线程需要被阻塞； 如果先驱节点的状态是cancel， 则跳过这些结点 否则设置先驱节点的状态为signal 可以看到，第一次进入这个方法的时候，先驱结点的状态为0，然后会被修改成signal，等到第二次进入的时候，就会直接返回true,意味着当前线程需要进入阻塞状态. 如果shouldParkAfterFailedAcquire方法返回true,那么就会进入parkAndCheckInterrupt方法的执行. 这个方法很简单，就是调用了LockSupport.park方法，让当前线程进入阻塞状态，直到有其它线程唤醒它为止. 到这里，整个lock方法已经完成了. 总结来讲，它的执行可以分为以下几步： 首先尝试调用compareAndSetState操作设置状态变量的值，设置成功能意味着获取锁对象成功，直接返回 进入acquire操作， 首先调用子类的tryAcquire方法尝试获取锁对象，获取到了直接返回成功 如果tryAcquire方法返回失败，则通过addWaiter创建新的CLH结点并将它入队. 入队完后调用acquireQueued方法进入阻塞操作，阻塞操作是通过LockSupport类的park来完成的. 在获取到锁对象之前，线程会一直处于阻塞、唤醒、尝试获取锁对象的过程中，直到获取锁对象成功. 3. RL源码 —- Unlock操作在第二小节中，我们提到在成功获取锁对象前，线程会一直处于阻塞、唤醒、尝试获取锁对象的过程中， 同时我们也知道线程是通过LockSuppot.park方法进入阻塞状态的，那么什么时候由谁去唤醒它呢？ 继续来看RL的unlock操作. 同样地，RL的unlock也是调用了sync中的release方法，由于RL是排它锁，同一时间只会有一个线程拥有锁对象，因此在release方法中不需要考虑并发的问题. 但作为重入锁，可能存在多次调用lock的情况，这时候只调用一次unlock并不一定意味着释放锁操作. 因此在release方法中调用了tryRelease方法. 在这个方法中，首先判断调用这个方法的线程是否是独占线程，如果不是直接抛出异常. 接着判断当前的状态变量在执行当前的释放之后是否为0，如果是0意味着锁对象被真正释放了，否则意味着当前线程继续持有锁对象. 当tryRelease返回true时说明锁对象被释放了，则调用unparkSuccssor，在这个方法，找到当前结点后的第一点状态不为cancel的结点，然后调用unpark方法来唤醒它. 到此，unlock方法完成. 4. 其它这里只是以RL为例，简单阐述了AQS框架的实现源码，事实上，在JUC中，除了RL， NonReentrantLock、Semaphore等等都是类似的实现思路, 可以通过类比的方式查看相应的源码，这里不做重复. 5. 参考文献 AQS框架概述1 AQS框架概述2 AQS源码浅析","categories":[],"tags":[]},{"title":"CopyOnWriteList和CopyOnWriteSet","slug":"多线程/juc/并发集合/CopyOnWriteList和CopyOnWriteSet","date":"2016-12-14T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/14/多线程/juc/并发集合/CopyOnWriteList和CopyOnWriteSet/","link":"","permalink":"http://yoursite.com/2016/12/14/多线程/juc/并发集合/CopyOnWriteList和CopyOnWriteSet/","excerpt":"","text":"CopyOnWriteList和CopyOnWriteSetCOWSet的底层是通过COWList实现的， 在写操作的时候，有选择性的选择addIfAbsent版本的操作. COWList的底层实现是通过ReentrantLock来实现的，所有的写操作执行前都必须先获取到相应的RL锁，然后再进行操作.","categories":[],"tags":[]},{"title":"CountDownLatch, CyclicBarrier和Semaphore的使用","slug":"多线程/juc/AQS框架/CountDownLatch, CyclicBarrier和Semaphore的使用","date":"2016-12-12T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/12/多线程/juc/AQS框架/CountDownLatch, CyclicBarrier和Semaphore的使用/","link":"","permalink":"http://yoursite.com/2016/12/12/多线程/juc/AQS框架/CountDownLatch, CyclicBarrier和Semaphore的使用/","excerpt":"","text":"CountDownLatch, CyclicBarrier和Semaphore的使用1. CountDownLatch实现类似于“计数器”一样的功能, 一般用于某个线程等待其它一些线程完成特定工作后开启后续工作时使用. 它最重要的方法是设置计数，计数减1， 等待计数结束， 分别对应于CountDownLatch, countDown以及await. 2. CyclicBarrier回环栅栏. 它的作用类似于在各个线程执行的某个点中增加一个栅栏，各个线程执行到这个点时，会等待其它线程， 当到达这个点的线程数满足预设的条件时，才会继续执行后续的操作. 它有以下几个特点： 它是可以重复利用的，也就是说通过前一个栅栏点时，这个对象可以被重复使用，这也是它被称为“回环”的原因 它是用来协调多个线程间的执行过程的，和countDownLatch不同的是，cdl是用于某个线程等待其它线程完成后执行特定操作，而cyclicBarrier是用于协调多个线程间的操作. 3. Semaphore信号量，可以理解成可同时被获取的信号总数. 使用semaphore的时候，一般是先设置相应的信号量，表示能同时能被获取的信号量，线程在进入临界区时，必须先获取一定的信号量，执行完后也必须释放相应的信号量，以便后续的线程能进入临界区. 比较常用的方法包括： acquire/release/tryAcquire, 具体方法的使用可以参考javaDoc 示例代码 gitHub代码","categories":[],"tags":[]},{"title":"模板模式","slug":"设计模式/模板模式","date":"2016-12-08T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/08/设计模式/模板模式/","link":"","permalink":"http://yoursite.com/2016/12/08/设计模式/模板模式/","excerpt":"","text":"模板模式","categories":[],"tags":[]},{"title":"等待通知机制","slug":"多线程/等待通知机制","date":"2016-12-06T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/06/多线程/等待通知机制/","link":"","permalink":"http://yoursite.com/2016/12/06/多线程/等待通知机制/","excerpt":"","text":"等待通知机制notify, notifyAll, wait这三个方法构成了java的等待通知机制的核心，它们主要是用于线程间通信时使用. 使用这三个方法的前提是当前线程必须拥有对象的锁 wait: 当前线程进入等待状态，直到其它线程调用了对象的notify或者notifyAll方法 notify：随机选取一个正在等待的线程唤醒 notifyAll：将全部等待的线程唤醒 使用notify, notifyAll和wait方法在使用时，必须获取到相应对象的锁才能使用，否则会抛出IllegalMonitorStateException异常. 在执行完wait操作后，当前执行线程会释放对象的锁，以等待其它线程调用该对象的notify或者notifyAll方法 wait与sleep的区别 在使用wait的时候， 线程必须拥有这个对象的锁，也就是说，wait方法调用必须出现在synchronize块中，类似地，notify/notifyAll也有这样的限制. sleep方法不需要出现在synchronize块中 wait/nofity/nofityAll方法是在对象上进行调用，而sleep方法是在线程对象上进行调用 在调用wait方法后，线程会释放持有的该对象锁（但不会释放其它锁），而sleep方法会一直持有对象锁，并不会进行释放 与Condition对象的区别参考文献 http://stackoverflow.com/questions/1036754/difference-between-wait-and-sleep","categories":[],"tags":[]},{"title":"Executors","slug":"多线程/juc/线程池/Executors","date":"2016-12-03T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/03/多线程/juc/线程池/Executors/","link":"","permalink":"http://yoursite.com/2016/12/03/多线程/juc/线程池/Executors/","excerpt":"","text":"ExecutorsExecutors是并发包中提供的工具类，它可以用来创建以下这些对象: ExecutorService ScheduledExecutorService ThreadFactory Callable 1. Executor接口首先先来看看并发包中的Executor接口的结构图. Executor接口只提供了一个方法，就是execute方法，它定义了执行Runnable对象， 至于以什么样的方式执行任务，执行任务的生命周期等其它方面的操作均没有涉及. ExecutorService在executor接口的基础上，增加了对执行任务生命周期的管理，也就是说，提交到executorService接口中执行的任务，可以通过提交任务时返回的future对象，判断任务是否已经执行结束，或进行取消操作. ExecutorService又可以进一步细分为AbstractExecutorService与ScheduledExeuctorService两大类，前者代表了ExecutorService的默认实现. 后者定义了定时操作的接口. 2. ThreadFactory这个接口提供了创建线程的方法， 避免显式地调用Thread对象的创建方法， Executors中提供了默认的线程工厂实现DefaultThreadFactory， 这个实现给每个创建的线程指定了特定的名称. 3. ExecutorService 和 ScheduledExecutorServiceExecutors提供了创建ExecutorService和ScheduledExecutorService的方法， 两种类型都可以分为两类，一种是创建线程池，一种是创建单个线程. 创建的过程很简单，就是调用相应的线程池实现的构造函数，根据不同类型传入不同的构造函数参数即可, 代码如下所示. 这里值得注意的是创建单个线程的代码. 以创建单个线程的executorService为例， 可以看到在构造完threadPoolExecutor对象之后， 又用FinalizableDelegatedExecutorService做了层封装, 这个类继承自DelegatedExecutorService, 而DelegatedExecutorService的作用就是封装底层的实现，只对外暴露ExecutorService接口的方法（这算是外观模式吗？）可以看到，通过这层封装，隐藏了底层的和线程池相关的一些操作，只对外暴露了executorService的API方法, 相当于是缩小了threadPoolExecutor对象的操作能力， 这种方式在一些用具体实现完成特定功能时会很有用（比如，单个线程的ExecutorService可以认为是通用线程池的特殊情况， 这时候就可以用线程池来实现单个线程的功能，但为了防止对外暴露过多的操作，就可以使用上述的方式进行包装，以达到隐藏线程池其它方法的功能） 4. CallableExecutors中提供的创建callable的方法都使用了RunnableAdapter类，从这个类的名字中就可以看出，这个类是适配器，它接受一个runnable和一个可选的返回值，然后将它们包装成一个Callable对象. 可以看到，Executors作为工具类，并没有实现太多新的功能，它只是对已有的一些功能进行包装，如果想要深入地了解线程池的实现，还是需要看看ThreadPoolExecutor的源码才行.","categories":[],"tags":[]},{"title":"netty源码学习系列----channelHandler","slug":"IO/netty/netty源码学习系列----channelHandler","date":"2016-12-02T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/02/IO/netty/netty源码学习系列----channelHandler/","link":"","permalink":"http://yoursite.com/2016/12/02/IO/netty/netty源码学习系列----channelHandler/","excerpt":"","text":"netty源码学习系列—-channelHandlerCodec FieldLengthBasedFrameDecoder","categories":[],"tags":[]},{"title":"netty源码学习系列-----eventLoop","slug":"IO/netty/netty源码学习系列-----eventLoop","date":"2016-12-02T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/02/IO/netty/netty源码学习系列-----eventLoop/","link":"","permalink":"http://yoursite.com/2016/12/02/IO/netty/netty源码学习系列-----eventLoop/","excerpt":"","text":"netty源码学习系列—–eventLoopnetty是如何实现对于某个channel的IO事件，交由同一个线程去处理?channel持有一个eventloop，后续所有的操作都会交由这个eventllop操作，具体是在每个操作前，判断一下当前的执行线程是不是eventloop，如果是，直接执行，如果不是，则将要执行的事情放到eventloop的执行任务队列中 参考文献 NioEventLoop的运行情况","categories":[],"tags":[]},{"title":"责任链模式","slug":"设计模式/责任链模式","date":"2016-12-01T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/12/01/设计模式/责任链模式/","link":"","permalink":"http://yoursite.com/2016/12/01/设计模式/责任链模式/","excerpt":"","text":"责任链模式责任链模式由两个重要组件组成，请求(指令的产生者)以及处理器(指令的处理者)。 这个模式在很多常用的框架中都可以找到踪迹，比如netty的channelHandler， Spring Security中的Filter等等。 简单来讲，请求产生后会在处理器链中按一定的顺序传播，每个处理器的逻辑分为两部分 判断是否能处理相应的请求 如果判断能处理到达的请求，则处理，否则继续“向下”传播(即责任器链上的每个节点都有它自己的责任) 它的UML图如下：Request接口代表了请求（或者说是指令的产生者），而handler接口代表了处理器（或者说是指令的处理器），每个处理器都有对链上相邻处理器的引用，当它自己不能处理到达的请求时 ，会将请求进一步传播给相邻的处理器. 以Netty中的channelHandler为例，它是用来channel中IO事件的处理器，当IO事件产生时，channel会将这个事件交给与之关联的channelPipeline，而channelPipeline事实上就是一组前后相连的channelHandler， IO事件沿着pipeline一直传播，直到被channelHandler处理或到达pipeline的末端. 另外，值得一提的是，虽然被称为是责任链，但请求在责任器链中的传播并不一定要是线形的，有些责任器可能会有类似于“分发”器一样在的功能，在这种情况下，责任器就会是“树形”的形状. 参考文献 http://www.oodesign.com/chain-of-responsibility-pattern.html https://en.wikipedia.org/wiki/Chain-of-responsibility_pattern 示例代码 https://github.com/Essviv/designPattern/tree/master/src/main/java/com/cmcc/syw/responsibility","categories":[],"tags":[]},{"title":"状态模式","slug":"设计模式/状态模式","date":"2016-11-29T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/29/设计模式/状态模式/","link":"","permalink":"http://yoursite.com/2016/11/29/设计模式/状态模式/","excerpt":"","text":"状态模式在学习nio编程的时候，遇到需要在不同的状态间进行转换，程序也需要根据不同的状态作不同的处理，搜了下相应的设计模式，果然发现有种叫”状态模式“的非常符合自己的需要，简单地做个记录 按照惯例，还是先上UML图，从图中可以看出， 状态模式有两个重要的组成部分，一个是上下文，也就是维护状态的地方；另一个则是状态，不同的状态决定了在同一个上下文中会执行不同的操作. 状态模式允许一个对象在其内部状态发生变化时改变其行为，就好像改变了一个类的实现一样. 举个例子，信用卡在银行进行开户后，默认是处于已激活的状态，当用户进行消费后，处于透支状态，如果用户消费的额度超过了银行给的额度，则信用卡处于被冻结状态，针对这三种状态下，银行对于用户刷卡这个行为要执行的操作是不一样的. 这里银行可以被当作是状态的上下文，而银行卡的状态决定了上下文执行的具体操作. 另外，在状态模式中，有个很重要的问题，就是状态的迁移，目前的实现中有三种方式， 由外部调用类来管理状态的变换: 要求外部调用类了解所有的状态，且在增加状态的时候，需要修改客户端代码，不推荐 完全由上下文类来管理状态的变迁： 状态对外部完全不可见，仅在上下文内部维护，在增加状态的时候，不需要修改客户端的代码，推荐 由状态类自己完全变迁： 状态类之间产生依赖关系 参考文献 http://blog.csdn.net/lovesomnus/article/details/45750039 https://sourcemaking.com/design_patterns/state","categories":[],"tags":[]},{"title":"spring-test","slug":"测试/spring-test","date":"2016-11-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/25/测试/spring-test/","link":"","permalink":"http://yoursite.com/2016/11/25/测试/spring-test/","excerpt":"","text":"spring-test使用spring进行单元测试使用spring对代码进行 单元测试 时， 同其它的pojo进行单元测试是一样的，不过spring对单元测试提供了一些工具类来帮助用户更方便地进行单元测试. ReflectionTestUtils: 反射相关的系列工具方法类, 可以用于改变常量的值，调用私有方法，访问私有变量以及调用生命周期函数等等 AopTestUtils: spring aop相关的一系列工具方法类 使用spring-test进行集成测试spring的单元测试只能对单个的对象进行测试，不能对整个web, 包括像路径映射，过滤器链等机制进行验证，因此spring-test又提供了关于集成测试的支持. 它对集成测试的支持主要通过MockMvc实现，构建这个对象时可以有两种模式，分别是 使用standaloneSetup模式： 这种模式可以指定某些对象，spring只会加载与这些对象有关的信息，同时用户也可以自定义整个容器的初始化过程. 同时，这种模式也可以mock一些对象，让客户专注于测试关心的那个对象 使用webAppContextSetup模式： 这种模式通过指定配置文件，加载整个web应用的信息，因此可以对整个应用的各个部分进行验证. 同时, spring-test提供了包括上下文缓存、事务管理、自动注入等支持 示例代码 https://github.com/Essviv/spring/tree/master/src/test/java/com/cmcc/syw/controller 参考文献 spring集成测试 spring-test的概念 spring-test的官方文档 IBM","categories":[],"tags":[]},{"title":"IO中的阻塞、非阻塞、同步和异步的区别","slug":"IO/java基础/IO中的阻塞、非阻塞、同步和异步的区别","date":"2016-11-22T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/22/IO/java基础/IO中的阻塞、非阻塞、同步和异步的区别/","link":"","permalink":"http://yoursite.com/2016/11/22/IO/java基础/IO中的阻塞、非阻塞、同步和异步的区别/","excerpt":"","text":"IO中的阻塞、非阻塞、同步和异步的区别IO相关概念 阻塞：在发起IO操作之后，线程被阻塞，直到相应的IO操作完成才会返回 非阻塞： 在发起IO操作之后，线程不会被阻塞并且立即返回 同步： 在发起IO操作之后，在没有得到结果之前，调用都不会返回（注意，这里不返回不代表就一定阻塞了，应用也可以处于非阻塞状态），调用一旦返回了，IO操作的结果也就得到了。换句话说，就是由调用者（或应用）主动等待IO操作的结果， Reactor模式就属性这种模式 异步： 在发起IO操作之后，应用程序直接返回，并不等待IO操作的结果，而是由被调用者（通常是系统）在IO操作完成后，通过通知、回调等方式告知应用程序。 Proactor就属性这种模式 从上面的定义可以看出，同步和异步的区别在于IO的调用方是否需要主动地等待数据，在同步操作中，应用需要主动将数据从系统内核空间拷贝到应用空间，并且在这个过程中会出现block状态；而异步操作中，应用调用完IO操作后，就可以执行其它的操作了，系统在将数据拷贝到应用空间完成后，通过回调和通知等方式告知应用，应用再开始对这些数据进行相应的处理. IO模型IO模型大体上可分为以下五类： 1.阻塞式IO(BIO) 2.非阻塞式IO(NIO) 3.多路复用 4.信息驱动（不常用，略） 5.异步IO(AIO) 它们之间的比较： IO模型图: 感觉这里的IO多路复用更应该属于“同步阻塞”，但不知道为什么这里被划分为异步阻塞 一个IO操作其实分成了两个步骤：发起IO请求和实际的IO操作。同步IO和异步IO的区别就在于第二个步骤是否阻塞，如果实际的IO读写阻塞请求进程，那么就是同步IO。阻塞IO和非阻塞IO的区别在于第一步，发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 参考文献概念比较1： IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）概念比较2： 大话同步/异步、阻塞/非阻塞AIO简介： AIO简介BIO, NIO和AIO的理解： BIO, NIO和AIO的理解这篇文章很详细地阐述了同步与异步、阻塞与非阻塞的区别: http://www.jianshu.com/p/55eb83d60ab1","categories":[],"tags":[]},{"title":"Runnable, Callable, Future, FutureTask的区别","slug":"多线程/Runnable, Callable, Future, FutureTask的区别","date":"2016-11-22T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/22/多线程/Runnable, Callable, Future, FutureTask的区别/","link":"","permalink":"http://yoursite.com/2016/11/22/多线程/Runnable, Callable, Future, FutureTask的区别/","excerpt":"","text":"Runnable, Callable, Future, FutureTask的区别 Runnable是thread用来执行时指定的对象，它只包含有一个方法，run方法，这个方法没有返回值，且不会抛出异常信息. Callable和runnable一样，都是executor用来执行时指定的对象，它也包含一个方法，call， 但call方法是有返回值的，且会抛出exception异常. Future是executorService在提交完任务后，用于表示未完成的任务的一个对象，它可以用来取消任务、获取任务状态以及获取任务运行结果等操作. FutureTask是runnableFuture的一个子接口，而runnableFuture实现了Runnable以及Future接口， 同时，FutureTask还包含有callable或者runnable的实例，可以说，FutureTask是一个集合体，它既可以用在thread的运行中，也可以用在executorService的运行中，还可以用来控制任务的执行和取消等操作.","categories":[],"tags":[]},{"title":"netty总览图-核心组件","slug":"IO/netty/netty总览图-核心组件","date":"2016-11-22T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/22/IO/netty/netty总览图-核心组件/","link":"","permalink":"http://yoursite.com/2016/11/22/IO/netty/netty总览图-核心组件/","excerpt":"","text":"netty总览图-核心组件netty核心概念关系图 Bytebuf: 对java nio中ByteBuffer的抽象 每个Channel代表了一种能够用于IO操作的实体，例如socket， 文件等等 每个channel都有一个pipeline, pipeline的作用是处于和这个channel相关的所有IO事件 每个pipeline中有一系列的channelHandler， 每个channelHandler是真实处理IO事件的地方， 并把这个事件沿着Pipeline传播. ChannelHandler可大体分为两类，一类是InboundHandler，也就是处理进来的消息的处理器，另一种则是OutboundHandler， 是用来处理出去的消息的. 每个channelHandler都有一个相关联的channelHandlerContext， 这个上下文的作用就是让channelHandler可以和它的pipeline以及其它的channelHandler之间进行交互. 每个channel都会被注册到EventLoop中, eventLoop会负责处理该channel的所有IO事件， 通常情况下，每个eventLoop会管理多个channel这里可以类比下NIO的编程，这里的EventLoop类似于带有selector组件的ServerSocketChannel， 它负责多个channel的IO事件. EventLoopGroup是eventLoop的集合，它管理着所有的eventloop的生命周期 参考文献 NIO教程 http://tutorials.jenkov.com/java-nio/index.html netty教程 netty源码解析系列","categories":[],"tags":[]},{"title":"JVM的内存回收","slug":"JVM/JVM的内存回收","date":"2016-11-18T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/18/JVM/JVM的内存回收/","link":"","permalink":"http://yoursite.com/2016/11/18/JVM/JVM的内存回收/","excerpt":"","text":"JVM的内存回收JVM的内存回收需要处理以下几个问题： 哪些内存是需要回收的 什么时候回收 怎么回收 1. 哪些内存需要回收对于第1个问题而言，在JAVA中，GC回收的主要对象是堆内存，这部分内存用于存储对象实例，当实例对象不再需要时，则需要对这部分内存进行回收. 2. 如何判断对象已死？判断对象是否已死解决的是“什么时候回收”的问题。目前判断对象已死的方法主要有两种， 一种是引用计数法，一种是根搜索算法; 引用计数法引用计数法就是记录每个对象的引用数量， 每次GC进行之前，依次轮询所有对象的引用计数，对于那些计数归零的对象，就成为GC的目标对象. 这个算法逻辑简单，但有个致命的缺点，没办法解决循环引用的问题，如下图所示, 图1表示正常的循环引用回收，图2表示循环引用的情况. 根搜索算法（也被称为是可达性分析）根搜索算法通过定义一系列的根对象，在GC开始之前，依次通过这些根对象往下标记，被引用到的对象会被标记为被引用，对于那些没有被标记引用的对象，就成为GC的目标对象. 从这个算法的描述中可知，这个算法依赖于预先定义的一系列根对象(GC Root). 在JVM的实现中，定义了以下对象为GC Root: 虚拟机栈中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中JNI引用的对象 3. GC回收算法回收算法解决的是“如何回收”的问题. 当JVM根据一定的算法获取到哪些对象不再被引用时，就需要有一定的算法对它们进行回收， 目前比较常见的GC算法有以下几种: 标记-擦除算法（Mark-Sweep)： 首先标记所有不再需要的对象，然后一次性对这些对象进行擦除，被回收的内存可以被继续使用. 这种算法的优点是实现简单，缺点是标记和擦除算法的效率都不高，且容易产生内存碎片 标记-压缩算法(Mark-Compact): 首先标记所有不再需要的对象，然后将存活的对象移向一边，最后将剩余内存清空. 复制算法(Copying)：首先将内存分成相等的两部分，每次只使用其中一部分，当这部分内存被用完时，就将剩余的存活对象复制到另一部分上，然后将这部分内存直接擦除. 这个算法的优点是实现简单，运行高效，但致命的缺点是内存浪费严重（50%）。因此只适用于对于存活率比较低的场景，这种情况下，每次需要复制的对象就比较少，拷贝的效率就会很高. 分代收集算法： 严格来讲，这个不能称为是一种GC回收算法，它只是对不同的场景将前面三种回收算法做一种组合实现而已，不过由于它针对不同场景的特点选用了不同的回收算法，因此也是最有效的方式. 目前大部分的JVM实现中都会将堆内存进一步细分为 新生代 和老生代. 新生代的特点是存在大量的“朝生夕死”的对象，因此它特别适合使用“复制”算法. 具体实现”复制”算法时，又进一步将新生代细分为Eden区和两部分Survivor(from, to)区，两者的比例默认为8:1， 每次只使用Eden区和其中一块Survivor(from)区（意味着只浪费了10%的新生代内存空间）当Eden区内存用完时，会将Eden区以及Survivor(from)区中存活的对象复制到另一个Survivor(to)区，然后将Eden区与Survivor(from)区的内存擦除. 另外，由于我们无法保证每次回收时都只有不超过10%的对象存活，当Survivor(to)区不足以保存存活对象时，需要额外的空间进行担保（通常是老生代），如果触发了担保机制，那么这些存活的对象会直接进入老生代. 从新生代使用的”复制“算法的描述中可以看出，”复制“算法特殊适合于对象存活率较低的情况，而且，如果不想浪费50%的内存空间，就需要有额外的空间进行担保. 老生代的对象存活率较高，且没有额外的空间进行担保，因此不适合使用”复制“算法. 目前的JVM实现中，老生代基本上都是使用”标记-整理“算法. 4. GC收集器（GC算法的具体实现）不同的GC收集器是针对特定的内存区域（新生代、老生代）， 使用不同的GC回收算法（见第3节）， 以不同的模式运行（并发式、独占式），在运行的过程中会产生不同的线程数（单线程、多线程）. 以下关于不同的GC回收器的学习也会从这几个方面进行阐述. 新生代串行收集器: 新生代、复制算法、独占式、单线程(+XX:UseSerialGC(串+串)) 老生代串行收集器: 老生代、标记-压缩算法 、独占式、单线程(+XX: UseSerialGC（串+串）， +XX:UseParNewGC（并+串）) 并行收集器: 新生代、复制算法、独占式、多线程(+XX:UseParNewGC（并+串）, +XX:UseConcMarkSweepGC（并+CMS）) 参考文献 http://www.cnblogs.com/smyhvae/p/4744233.html http://coderbee.net/index.php/jvm/20131031/547 https://www.ibm.com/developerworks/cn/java/j-lo-JVMGarbageCollection/ https://plumbr.eu/java-garbage-collection-handbook http://www.cnblogs.com/highriver/archive/2013/04/17/3016992.html","categories":[],"tags":[]},{"title":"JVM的内存模型","slug":"JVM/JVM的内存模型","date":"2016-11-16T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/16/JVM/JVM的内存模型/","link":"","permalink":"http://yoursite.com/2016/11/16/JVM/JVM的内存模型/","excerpt":"","text":"JVM的内存模型JVM中的内存管理1. 程序计数器这部分内存是线程私有的，它是用来记录当前线程执行的字节码的位置. JVM在这部分内存中没有定义任何错误类型 2. 虚拟机栈这部分内存也是线程私有的，线程每调用一个方法，都会往相应的虚拟机本中push一个栈桢（栈桢的内容包括局部变量，操作栈，方法出口信息等），当方法返回时，虚拟机栈就会pop相应的栈桢. 在线程执行的过程中，方法的调用和返回对应了虚拟机栈中的入栈和出栈的过程. JVM在这部分定义了两种异常： StackOverflowException: 当栈深度超过JVM规定的深度时，就会引起栈溢出异常 由于方法的局部变量表在编译的时候就已经确定，因此栈桢的大小也就确定了，如果JVM无法申请足够的内存创建栈桢时，则会抛出OutOfMemoryException 3. 堆内存这部分内存是JVM中最大的一部分内存，它是所有线程共享的，同时也是GC管理的主要区域；它主要是用于存储对象实例. 堆内存可进一步细分为： Eden区(E区）, Survivor From（S1区）, Survivor To(S2区）, Old区（O区），将内存分代管理主要是为方便GC管理，这部分会在后续和GC算法进一步阐述 JVM在这部分定义了OOM异常，如果在这部分无法申请新的内存来存储实例时，则会抛出OOM异常 4. 方法区方法区主要用于存储已加载的类信息，静态变量，常量以及JIT生成的代码等内容. 它也被称为是Non-Heap区（非堆） 5. 常量池Class文件中除了类的版本、方法表等内容外，还包括了在编译时期就已经确定的常量表，常量表的这部分内容在类加载后就被存储于常量池中 6. 本地方法栈本地方法栈与虚拟机栈是非常类似的，但是它存储的是native方法的信息. 同样，在本地方法栈中，JVM也定义了两种异常信息，OOM与StackOverflowException, 其含义与虚拟机栈类似. 内存分代管理与回收算法 参考文献 InfoQ 内存模型1 内存模型2（这个文章里有关于内存模型的明晰的示意图） 内存模型3 内存模型4 内存模型5 内存模型6 内存模型7","categories":[],"tags":[]},{"title":"JVM的虚拟机栈","slug":"JVM/JVM的虚拟机栈","date":"2016-11-16T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/16/JVM/JVM的虚拟机栈/","link":"","permalink":"http://yoursite.com/2016/11/16/JVM/JVM的虚拟机栈/","excerpt":"","text":"JVM的虚拟机栈虚拟机栈是线程的私有内存，栈中的元素被称为栈帧， 方法被调用时，就会往线程的虚拟机栈中压入新的栈帧，栈帧中包含有方法调用的相关信息，如局部变量表（包含方法参数和方法体内定义的局部变量）、操作数栈、方法返回信息等；栈顶的栈帧代表了当前正在被调用的方法，当方法返回时，栈顶的栈帧元素被弹出，从方法的调用到返回对应了栈顶元素的入栈到出栈的过程. 栈桢结构虚拟机栈中是由栈桢组成的，方法的每次调用到退出的过程对应了栈桢在虚拟机栈中的入栈到出栈的过程，当前正在被执行的方法对于栈顶的栈桢，也被称为是当前栈桢. 栈桢中包含了方法运行所需要的全部信息，主要包括局部变量表、操作数栈、动态链接以及方法出口信息等， 如下图所示 1. 局部变量表局部变量表包括方法参数以及在方法内部定义的局部变量（由于栈桢元素是线程私有的，因此如果某个方法只依赖于方法参数和局部变量的话，那它一定是线程安全的） 局部变量表是以变量槽（slot）为最小单位，JVM并没有指明每个slot占用的内存大小; 在方法执行的时候，虚拟机是通过局部变量表完成方法参数值到参数变量的传递，对于非静态方法(static）来讲，第一个槽存放的是this变量，然后是方法参数的值，接着是方法内部定义的局部变量的值 2. 操作数栈通常会听到有人说“JAVA是基于栈的执行引擎”, 这里说的栈就是操作数栈. 它是一个“后进先出” （LIFO）的栈结构， 栈中的元素为操作数，虚拟机的操作指令都是针对操作数栈中的元素来进行的。在方法刚开始执行的时候，操作数栈是空的，随着方法的执行，虚拟机会将操作指令的操作数入栈，然后执行相应的指令，再从栈中读取数据（出栈）. 例如, iadd指令是对两个操作数进行加操作，在执行这条指令的时候，虚拟机会将要相加的两个操作数进行入栈操作，然后执行iadd, iadd会将栈顶的两个元素进行相加后再将结果进行入栈. 3. 动态链接动态链接的作用是，每个栈帧都会包含指向运行时常量池中当前栈帧对应的方法的引用. 持有方法的引用 是为了支持动态链接（？） 4. 返回地址当方法被执行时，返回的方式有两种： 一种是正常的操作指令退出， 一种是因为程序异常（不管是虚拟机异常还是未捕获异常）导致的程序退出; 不管是何种方式退出， 程序都会返回到调用该方法的位置；程序退出的过程相当于栈桢出栈的过程，因此可能的操作包括恢复上层方法的局部变量表和操作数栈、把返回值压入上层调用方法的操作数栈中以及恢复程序计数器的值等","categories":[],"tags":[]},{"title":"Reflection in Action","slug":"JVM/反射/Reflection in Action","date":"2016-11-16T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/16/JVM/反射/Reflection in Action/","link":"","permalink":"http://yoursite.com/2016/11/16/JVM/反射/Reflection in Action/","excerpt":"","text":"Reflection in ActionReflectionReflection is the ability of a running programme to examine itself and its software environment, and to change what it does depending on what it finds. MetadataTo perform this self-examination, a program needs to have a representation of itself. This information we call metadata. In an object-oriented world, metadata is organized into objects, called metaobjects. The runtime self-examination of the metaobjects is called introspection. In general, there are three techniques that a reflection API can use to facilitate behavior change: direct metaobject modification operations for using metadata (such as dynamic method invocation) intercession, in which code is permitted to intercede in various phases of program execution. Java supplies a rich set of operations for using metadata and just a few important interces- sion capabilities. In addition, Java avoids many complications by not allowing direct metaobject modification. class object getXXXs, getXXX, getDeclaredXXXs, getDeclaredXXX: 带declared的，是获取当前类声明的所有方法, 不论是public, protected, default还是private, 不带declared的，是获取类所有的public方法，包括在超类中声明的方法. 针对原生类型、接口以及数组，java也提供了相应的方法来表示相应的类型， class类也提供了isPrimitive, isInterface, isArray等方法来鉴别 原生类型(包括void)：通过形如int.class来表示, 可以通过class对象的isPrimitive来判断某个class是否代表原生类型. 接口类型: 通过形如Collection.class来表示，可以通过class对象的isInterface来判断某个class是否代表接口类型 数组类型: 通过形如int[].class或者int[][].class来表示，可以通过class对象的isArray来进行甄别，另外，数组中的元素类型可以通过getComponentType来获取. 类的层级关系也可以通过反射得到，在class类中声明了getSuperClass, getInterface, isAssignableFrom以及isInstance方法 getSuperClass返回的是当前类对象的直接父类 getInterfaces: 如果当前的类对象是Class，则返回它实现的接口类，如果当前的类对象是接口，则返回的是它的直接父接口列表 isAssignableFrom: X.isAssignableFrom(Y)意味着 a X field can be assigned from a Y field，也就是说 X和Y是同一个类 X是Y的超类 X是Y的超接口 isInstance: 反射中的instanceOf操作符，它的作用和instanceOf是一样的 field object getFields, getField, getDeclaredField, getDeclareFields：使用方法与区别与class中对应的方法一样，带declared的方法都是获取当前对应中的所有方法，不管它是private/default/protected/public. field继承自AccessObject, 同时实现了Member接口, member接口中定义了以下四个方法 getName: 获取成员的名称 getModifiers: 获取成员的修饰符, private/default/protected/public, static, final等等 getDeclaringClass: 获取声明当前成员的类 isSynthetic: 标识当前成员是否是由编译器引入的. Modifier类提供了判断修饰符的方法， 它提供了11种修饰符的判断，同时，它的toString方法会输出修饰符的文字. 可以通过modifier判断Member接口返回的getModifier值，得到对应的修饰符信息 当某个属性为数组对象时，特别是原生对象的数组时，不能将它转化为Object[]. JAVA中提供了Array工具类对数据对象进行操作，它的主要操作包括getLength, get/set以及newInstance. 动态加载和反射构造动态加载主要是通过Class.forName来完成的，结合返回构造的方式(Class.newInstance)，可以在运行时期指定相应的类名，从而动态地改变应用的功能. 动态加载与反射构造还可以应用于设计模式中，例如外观模式、抽象工厂等模式中，通过动态加载，可以实现外观的动态加载，从而为程序提供动态地变更外观实现提供可能；与此类似，抽象工厂结合动态加载，可以实现动态地构建不同的对象；关于这部分内容，具体可参见书本的第六章.动态加载中，Class.forName需要接受类的全限定名，针对数组，均是由左方括号([)开始，并结合单个类型代码来指定，具体可查阅JVM文档. 反射构造可以通过两种方式来完成，Class.newInstance和构造函数对象Constructor. Class.newInstance: 相当于调用了类的无参数构造函数，也就是说X.class.newInstance = new X(); Contructor: 构造函数也是类的元数据对象，可以通过Class.getConstructor()并指定参数类型来获取指定的构造函数, 对于非静态的内部类，第一个参数必须显示指定为外部类的类对象(具体见javadoc) Java中的动态代理Proxy, InvacationHandler(implementation and delegation) 查看堆栈状态new Throwable()会带有一系列的堆栈对象，每个堆栈对象StackTraceElement中会含有相应的类名、方法名、文件名以及行号信息.","categories":[],"tags":[]},{"title":"JVM分析工具","slug":"JVM/JVM分析工具","date":"2016-11-14T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/14/JVM/JVM分析工具/","link":"","permalink":"http://yoursite.com/2016/11/14/JVM/JVM分析工具/","excerpt":"","text":"JVM分析工具JVM常用的分析工具包括： jps, stat, jmap, jstack, jinfo, jvisualvm jps列出当前运行的JVM进程，可以通过参数-m -l列出其main方法以及线程ID等信息 jinfojinfo主要是用于查询与设置当前JVM进程的参数值的，它可以实现运行时查看和修改JVM参数的功能. jstat列出某个JVM进程的内存统计信息，它的子命令包括（具体可查阅参考文献1， 每个子命令的输出字段的含义可查阅参考文献2） -gc: 堆内存的GC相关信息的统计数据 -class: 类加载信息的统计数据 -gcnew: 新生代的内存统计数据 -gcold: 老生代的内存统计数据 -gccapacity: 堆内存的大小统计数据， 包括新生代、老生代等 -gcnewcapacity: 新生代的大小统计数据 -gcoldcapacity: 老生代的大小统计数据 -gccause: 引起gc操作的原因的统计数据 -gcutil: gc的统计数据 jstack查看JVM的堆栈信息 jmap查看JVM的堆快照信息，也可以通过-dump命令将当前的堆快照信息输出到指定的文件. jvisualvm从这个名字可以看出，它其实就是将之前五个命令的输出进行了可视化显示. 应该来讲，只要掌握了前五个命令的使用，自然就掌握了jvisualvm的使用. 参考文献 Oracle document center JVM常用工具分析","categories":[],"tags":[]},{"title":"代理模式","slug":"设计模式/代理模式","date":"2016-11-10T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/10/设计模式/代理模式/","link":"","permalink":"http://yoursite.com/2016/11/10/设计模式/代理模式/","excerpt":"","text":"代理模式代理模式的关键点在于代理类与被代理类在客户端看来是“相同”的，不管这种“相同”是通过继承还是接口来定义 代理模式的UML 代理模式的实现 静态代理: 静态代理指的是在程序运行前，代理类与委托类之间的代理关系就已经确定了，它们的字节码在运行就已经存在 动态代理：动态代理是指在程序运行的过程中，动态地生成指定类的代理类，并增加一些额外的实现逻辑， 根据实现方式，动态代理又可以进一步分为 继承方式实现(cglib) 与 接口方式实现(JAVA原生支持） 动态代理的实现1. JAVA原生方式JAVA原生方式实现动态代理主要涉及到两个类, Proxy和InvocationHandler， 每一个proxy实例都有一个关联的invocationHandler，所有对proxy实例的调用最终都会委托到相关联的invocationHandler的invoke方法中调用, 具体的调用时序图如下： JAVA原生方式实现的动态代理示例代码如下： 从上面的示例代码中可以看出，原生方式实现的动态代理必须有一个前提： 委托类必须实现接口，否则无法通过这种方式进行代理. 2. CgLib的方式实现动态代理cglib使用asm动态生成字节码，因此在使用cglib时需要增加asm的依赖. 另外，cglib使用了继承的方式来实现动态代理，因此 委托类不能声明成final，否则无法通过cglib进行代理 cglib实现动态代理主要涉及到两个类，Enhancer和MethodInterceptor. Enhancer可以同时支持继承超类和实现接口两种方式来生成动态代理类， 通过设置超类和回调接口(Callback)可以实现与Proxy与InvocationHandler一样的效果. MethodInterceptor是最常用的回调接口， 它可以认为是一种AroundAdvice， 也是拦截器的一种实现， 它只有一个方法intercept， 可以在这个方法中定义相应的代理实现逻辑. 除此之外，cglib还提供了CallbackFilter接口，它是用来匹配方法与callback接口的接口，可以为不同的方法定义不同的回调实现. cglib实现的动态代理示例代码如下： 总结 JAVA原生的动态代理实现是基于接口的方式，因此被代理的类必须实现接口，否则无法通过这种方式实现动态代理 CgLib是通过继承方式实现动态代理，因此被代理的类不能是final. 另外，由于它是在字节码层面进行动态代理，因此依赖于asm库 不管是JAVA原生方式还是cglib方式实现动态代理，可以看出实现的思路均是通过回调或者拦截的方式修改被代理方法的实现逻辑来完成的（implementation and delegation） 动态代理模式也是实现框架最重要的一种模式，它也是spring框架中aop, 事务控制等特性的基础 代理模式与装饰器模式的区别在于，代理模式关注于对象的访问，而装饰器模式更关注动态地增加功能；另外，代理模式中代理类与被代理类的代理关系一般在编译时就已经确定，而装饰器模式的目标对象可以在运行时指定. 参考文献 http://wiki.jikexueyuan.com/project/java-reflection/java-dynamic.html 示例代码 代理模式与装饰器模式的区别","categories":[],"tags":[]},{"title":"外观模式","slug":"设计模式/外观模式","date":"2016-11-09T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/09/设计模式/外观模式/","link":"","permalink":"http://yoursite.com/2016/11/09/设计模式/外观模式/","excerpt":"","text":"外观模式","categories":[],"tags":[]},{"title":"适配器模式","slug":"设计模式/适配器模式","date":"2016-11-09T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/09/设计模式/适配器模式/","link":"","permalink":"http://yoursite.com/2016/11/09/设计模式/适配器模式/","excerpt":"","text":"适配器模式适配器模式的实现可分为两种， 一种称为类适配器，一种称为对象适配器 1. 类适配器UML图如下， 适配器通过实现目标接口（Target)，并通过组合的方式获取被适配对象（adaptee）的实例，从而实现适配，这种方式相对来讲更加灵活： 它不仅可以适配被适配接口(adaptee)，还可以适配被适配接口的子类（adaptee的子类）; 如果子类有需要，可以重写被适配接口的某些方法，依然可以被正常适配 虽然UML图中只给出了一个被适配对象，事实上适配器可以适配多个接口，从而实现目标接口. 适配器模式的关键点不在于它适配了多少个接口，而在于它通过提供适配器，将原来旧的接口适配到了新的接口上，与数量无关. 推荐使用这种方式实现适配器模式 2. 对象适配器UML图如下，这是适配器模式的第二种实现，它是通过实现目标接口(target)，并继承被适配对象(adaptee2)实现的. 它的优缺点如下： 实现方式简单，可以很方便地对被适配对象(adaptees)的方法进行重写. 只能适配被适配器对象(adaptee2)，无法适配被适配器对象的子类；如果子类对被适配器进行了重写，则需要重新实现适配器对象(adapter) 不推荐使用这种方式实现适配器模式 参考文献 http://design-patterns.readthedocs.io/zh_CN/latest/structural_patterns/adapter.html https://sourcemaking.com/design_patterns/adapter 示例代码示例代码","categories":[],"tags":[]},{"title":"参观者模式","slug":"设计模式/参观者模式","date":"2016-11-07T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/07/设计模式/参观者模式/","link":"","permalink":"http://yoursite.com/2016/11/07/设计模式/参观者模式/","excerpt":"","text":"参观者模式双重分派机制参考文献 http://blog.csdn.net/lovelion/article/details/7433567 http://blog.csdn.net/chenssy/article/details/12029633","categories":[],"tags":[]},{"title":"享元模式","slug":"设计模式/享元模式","date":"2016-11-07T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/11/07/设计模式/享元模式/","link":"","permalink":"http://yoursite.com/2016/11/07/设计模式/享元模式/","excerpt":"","text":"享元模式","categories":[],"tags":[]},{"title":"反射-类信息","slug":"JVM/反射/反射-类信息","date":"2016-10-30T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/30/JVM/反射/反射-类信息/","link":"","permalink":"http://yoursite.com/2016/10/30/JVM/反射/反射-类信息/","excerpt":"","text":"反射-类信息反射可以在运行时获取类、方法以及属性的信息 通过反射，可以在运行时获取类各方面的信息，包括类名信息、包信息、修饰符、超类信息、接口信息、方法信息、属性信息、构造函数以及注解等信息. 类名信息getName, getSimpleName(不包含包名信息), getCanonicalName, getTypeName getName返回的名称一般是用于Class.forName，用来动态地加载类信息 getCanonicalName返回的名称一般是用于import操作中，也可用于日志输出 getSimpleName不包含包信息 包信息包信息被封装在Package类中，通过这个类可以获取包信息，比如包名等 修饰符修饰符是用一个整形数字来表示，可以通过Modifier来进行解析， 包括private/protected/public, native, static, synchronized等信息 超类信息通过getSuperClass获取，获取的也是Class对象，同样可以通过反射机制获取其相应的信息 接口信息通过getInterfaces获取，注意这里只获取这个类显式声明实现的接口，对于它的超类中声明实现的接口信息，通过这个方法是无法返回的, 需要通过递归的方式来获取完整的接口信息. 方法信息通过getMethods或者getDeclaredMethods来获取，前者获取类所有的方法，包括在超类中声明的方法，后者则只会获取在当前类中声明的方法， 包括public/protected/default/private. 构造函数信息 通过getConstructors或者getDeclaredConstructors()来获取，其中getConstructor获取到所有public构造函数列表，而getDeclaredConstructors则可以获取到所有的构造函数，包括public/protect/default/private的构造函数 如果知道构造函数的参数类型及顺序， 可以通过getContructor来获取指定的构造函数. 可以通过构造函数对象初始化该类的实例, newInstance 属性信息 getFields或者getDeclaredFields来获取，两者的区别与前面相同 通过getField方法，可以指定属性名称来获取相应的属性Field对象. 获取到Field对象后，可以对get/set设置相应实例的值 注解信息getAnnotations或者getDeclaredAnnotations来获取，两者的区别与前面的相同. 注意，这里只能获取到那些Retention = RetentionPolicy.RUNTIME的注解，也就是只能获取到运行时注解. ##参考信息 http://stackoverflow.com/questions/15202997/what-is-the-difference-between-canonical-name-simple-name-and-class-name-in-jav","categories":[],"tags":[]},{"title":"反射-属性信息","slug":"JVM/反射/反射-属性信息","date":"2016-10-30T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/30/JVM/反射/反射-属性信息/","link":"","permalink":"http://yoursite.com/2016/10/30/JVM/反射/反射-属性信息/","excerpt":"","text":"反射-属性信息Field代表了属性信息，可以是类属性，也可以是实例属性 属性中定义了以下方法： getType: 获取属性的类型信息 实现了Member接口： Member接口中定义了getName, getDeclaringClass, getModifier和isSynthetic方法 get/set方法：在实例上获取/设置这个属性的值 getByte/getInt/getFloat/getString: 获取属性的值，返回指定的类型，同样，也有一系列的set方法","categories":[],"tags":[]},{"title":"反射-构造函数","slug":"JVM/反射/反射-构造函数","date":"2016-10-30T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/30/JVM/反射/反射-构造函数/","link":"","permalink":"http://yoursite.com/2016/10/30/JVM/反射/反射-构造函数/","excerpt":"","text":"反射-构造函数构造函数与方法信息都有个共同的基类，Executable类，这个类中定义了以下几个构造函数和方法共有的方法： getParameterTypes: 按构造函数（或方法）参数的声明顺序，返回相应的参数类型信息 getParameterCount: 返回构造函数（或方法）参数的个数 getParameters: 返回构造函数（或方法）的参数列表 getDeclaringClass: 返回声明这个对象的类信息 getExceptionTypes: 返回构造函数（或方法）声明的异常信息 构造函数代表了类的构造函数信息，其最主要的作用是可以通过它在运行时构建类的实例： newInstance: 接受可变参数，代表构造函数的参数","categories":[],"tags":[]},{"title":"反射-方法信息","slug":"JVM/反射/反射-方法信息","date":"2016-10-30T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/30/JVM/反射/反射-方法信息/","link":"","permalink":"http://yoursite.com/2016/10/30/JVM/反射/反射-方法信息/","excerpt":"","text":"反射-方法信息方法与构造函数一样，都是继承自Executable类，关于Executable类提供的方法信息可以参见构造函数 方法特有的方法信息： invoke: 传入相应的参数，并在实例上调用该方法 getReturnType: 返回返回参数的类型信息","categories":[],"tags":[]},{"title":"JAVA中的引用类型","slug":"JVM/JAVA中的引用类型","date":"2016-10-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/25/JVM/JAVA中的引用类型/","link":"","permalink":"http://yoursite.com/2016/10/25/JVM/JAVA中的引用类型/","excerpt":"","text":"JAVA中的引用类型 强引用(Strong Reference)是指平时经常用到的引用类型，如果某个对象存在强引用，那么它将不会被GC回收 软引用(Soft Reference)是指那些有用但不是必需的对象，它经常被用作缓存，当JVM内存充足时，它不会被GC回收，但如果内存不足时，它会被回收；它可以和引用队列(ReferenceQueue)进行关联，当软引用被回收时，它就进入关联的引用队列 弱引用(Weak Reference)是指不是必需的引用，在GC开始的时候，不管内存是否充足，它都将被回收；它可以和引用队列（ReferenceQueue)相关联，当弱引用被回收时，它就被加入到相关联的引用队列中 虚引用(Phantom Reference) 并不影响对象的生命周期，如果一个对象和虚引用关联，那么就跟没有和引用关联一样，它随时可能被GC回收，它必须和引用队列相关联，当GC某个对象时，如果发现它还有虚引用，则会把它加入到相应的引用队列中，可以通过判断虚引用是否出现在这个引用队列中，来确定该对象是否被回收 参考文献 参考文献","categories":[],"tags":[]},{"title":"常见的Web安全问题","slug":"安全/常见的Web安全问题","date":"2016-10-14T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/14/安全/常见的Web安全问题/","link":"","permalink":"http://yoursite.com/2016/10/14/安全/常见的Web安全问题/","excerpt":"","text":"常见的WEB安全问题1. XSS跨站脚本攻击， 解决方式就是对所有的用户进行转义编码，永远不要相信用户的输入 2. CSRF跨站请求伪造，原理图如下. 解决方式是给每个请求增加攻击者无法猜测的随机量，在接收到请求时，先校验这个随机量的有效性. 3. 固定session攻击会话固定攻击. 攻击者先登录欲攻击的网站，获取该会话的session, 并把获取到的session做为登录链接的一个参数，将构造的链接发给无辜的用户，用户点击链接登录成功后，在会话过期之前，攻击者就可以使用该session进行操作，获取登录用户的所有权限.解决方式是登录成功后，给用户重新发放一个session，废弃登录时的session. 4. 越权访问 横向越权：访问链接中带有一些ID标识，比如prdId = 5等等，攻击者就可以通过更改ID编码访问其它用户的资料，解决方式是尽量避免从前端获取相应的ID，如果无法避免，必须在后台增加相应的判断 纵向越权： 攻击者获取需要更高访问权限的链接，直接访问，解决方式是通过安全框架进行统一权限控制 5. 文件存储文件存储最主要的问题有几个， 限制文件大小，避免允许用户上传不限大小的文件 用户上传文件的目录要关闭相应的执行权限，毕竟用户上传的文件内容是不可知的 6. 短信轰炸、邮件轰炸对于短信验证码这类会给用户下发短信、邮件等操作，允许通过验证码、限制次数等方式进行人机实验，确定发起操作的不是攻击脚本，避免攻击者通过脚本短时间内发起大量操作，给用户造成困扰. 7. XFS跨框架攻击，通过iFrame的隐藏特性，在正常操作的背后隐藏相应的iFrame，执行一些不可告人的操作, 防御的措施是在web服务器上配置相应的策略","categories":[],"tags":[]},{"title":"Flyway","slug":"工具/Flyway","date":"2016-10-11T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/11/工具/Flyway/","link":"","permalink":"http://yoursite.com/2016/10/11/工具/Flyway/","excerpt":"","text":"FlywayFlyway的命令包括： migrate: 将DB升级到最新的版本 clean: 清空数据库中所有的数据，注意这条命令只能在集成环境或测试环境中执行，严禁在生产环境中执行 info: 显示当前所有升级脚本的执行情况，显示pending、fail或者是success validate: 检查当前的升级脚本的情况，包括： 之前升级过的脚本是否被修改 所有的升级脚本是否被执行 baseline: 对于已经有数据（包括结构和数据）的数据库而言，可以通过baseline命令进行基线处理，在定义了基线后，后续所有的升级都只会针对大于基线版本的升级脚本进行另外，值得注意的是，定义成基线版本的那个脚本不会被运行，原因是基线版本就是定义目前数据库中已经有的数据和结构，不需要再次执行. 当需要新建另一个数据库时，可以直接运行基本版本生成相应的数据. repair: 在升级出现错误的情况下，flyway会在meta-table中产生一条错误信息，通过repair命令可以将这条消息删除，然后继续运行相应版本的升级脚本 flyway支持多种运行模式，可以通过commandLine、java、maven插件等方式运行. 参考文档 https://flywaydb.org/documentation/","categories":[],"tags":[]},{"title":"HashMap","slug":"集合/HashMap","date":"2016-10-10T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/10/集合/HashMap/","link":"","permalink":"http://yoursite.com/2016/10/10/集合/HashMap/","excerpt":"","text":"HashMap哈希表使用的是 数组加链表的方式存储元素，具体如下： 通过hash(key)与数组长度运算，hash(key)&amp;(length-1), 获取新元素在数组中的位置 如果数组中该位置上没有元素，那么新元素直接被放置在这个位置 如果数组中该位置上已经有元素，那么新元素被放在以这个元素开始的链表上, 进入第2步 遍历链表上的所有元素，比较hash值与key值，只有两者都相同的情况下，才认为找到相应的位置，如果该位置上有值了，则可以选择替换或不进行操作； 如果一直到链表的末尾都没有找到相应的元素，则在链表末尾添加相应的元素. 查找哈希表中的元素时，过程与上面大致相同，不做进一步描述.","categories":[],"tags":[]},{"title":"maven学习笔记","slug":"工具/maven学习笔记","date":"2016-10-04T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/04/工具/maven学习笔记/","link":"","permalink":"http://yoursite.com/2016/10/04/工具/maven学习笔记/","excerpt":"","text":"Maven学习笔记构件的坐标groupId, artifactId, version 管理依赖通过构件的坐标来唯一指定123456&lt;dependency&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;&lt;/artifactId&gt; &lt;version&gt;&lt;/version&gt; //快照版本： SNAPSHOT &lt;scope&gt;&lt;/scope&gt; //可取值包括: compile, runtime, provided, test, system(不常用) &lt;/dependency&gt; 间接依赖 ===&gt; 依赖冲突 路径最短 1A ==&gt; B ==&gt; C(V1.0)（selected) A ==&gt; D ==&gt; E ==&gt; C(V2.0) 声明优先 1A ==&gt; B ==&gt; C(V1.0) (selected) A ==&gt; D ==&gt; C(V2.0) 生命周期和阶段（抽象概念， 具体实现由指定的插件目标决定）生命周期 clean: pre-clean, clean, post-clean default: process-resources, compile, process-test-resources, test-compile, test, package, install, deploy site: pre-site, site, post-site, site-deploy 阶段同一个周期内的阶段按顺序执行，不同周期的阶段没有先后顺序关系 mvn clean: 执行clean周期的pre-clean, clean两个阶段 mvn clean test: 执行clean周期的pre-clean， clean， 然后再执行default周期的process-resources, compile……test mvn clean package: 执行clean周期的pre-clean, clean，然后再执行default周期的process-resources … package 插件生命周期定义的阶段均为抽象的概念，具体的操作由插件目标来实现，maven为一些关键的阶段定义了默认的插件目标 默认绑定： maven-clean-plugin: clean ===&gt; clean 自定义绑定： 将指定坐标的构件的A目标绑定到test阶段 123456789101112131415&lt;plugin&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;&lt;/artifactId&gt; &lt;version&gt;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;&lt;/id&gt; &lt;phase&gt;test&lt;/phase&gt; //指定绑定的阶段 &lt;goals&gt; &lt;goal&gt;A&lt;/goal&gt; //定义插件的目标 &lt;/goals&gt; &lt;/execution&gt; &lt;/excutions&gt;&lt;/plugin&gt; 聚合和继承聚合: 一次性构建多个项目123456789&lt;groupId /&gt;&lt;artifactId /&gt;&lt;version /&gt;&lt;packaging&gt;POM&lt;/packaging&gt; //聚合项目的打包方式必须为POM&lt;modules&gt; &lt;module /&gt; //第一个被聚合的项目 &lt;module /&gt; //第二个被聚合的项目&lt;/modules&gt; 继承继承的目的是为了减少重复的配置，子项目会从父项目中继承相应的配置，如果有必要，子项目也可以重写一些配置属性 1234567891011121314父POM： &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;packaging&gt;POM&lt;packaging&gt; //父工程的打包方式也必须为POM子POM &lt;artifactId /&gt; &lt;parent&gt; &lt;groupId /&gt; &lt;artifactId /&gt; &lt;version /&gt; &lt;relativePath&gt;&lt;/relativePath&gt; //父POM的路径 &lt;/parent&gt; 多环境构建： 基于属性, filter和profile实现, 通过${}访问属性 自定义属性: JAVA POM属性: ${project.baseDir}, ${project.artifactId}, ${project.build.sourceDirectory} filter资源过滤： 解析资源文件中的Maven属性 123456&lt;resources&gt; &lt;resource&gt; &lt;directory /&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt;&lt;/resources&gt; profile针对不同的环境采用不用的属性, 命令行激活， 如： -Pdev 12345678910111213141516171819&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;properties&gt; &lt;msg&gt;hello world&lt;/msg&gt; &lt;/properties&gt; &lt;activation&gt; &lt;activateByDefault&gt;true&lt;/activateByDefault&gt; //默认启用的profile &lt;/activation&gt; &lt;/profile&gt; &lt;profile&gt; &lt;id&gt;prod&lt;/id&gt; &lt;properties&gt; &lt;msg&gt;Holy god!&lt;/msg&gt; &lt;/properties&gt; &lt;/profile&gt;&lt;/profiles&gt; 约定大于配置（Convention Over Configuration）1234src/main/java, src/main/resources, src/test/java, src/test/resources 参考文献 maven实战 机械工业出版社 许晓斌","categories":[],"tags":[]},{"title":"WSDL","slug":"SOA/WSDL","date":"2016-10-04T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/04/SOA/WSDL/","link":"","permalink":"http://yoursite.com/2016/10/04/SOA/WSDL/","excerpt":"","text":"WSDLWSDL中包含以下几类元素 抽象元素： types: WS定义的类型，通过这种方式，WS可以最大限度的实现平台的中立性 message: WS的消息， 可以理解成传统函数中的输入输出参数 portType: WS执行的操作, 可以理解成传统函数库的一个模块或一个类， 也可以认为是接口定义 具体定义元素： binding: WS使用的通信协议， 定义消息的格式和通信细节，注意这里只是定义了协议与通信细节，并没有与具体的地址绑定 service: WS定义的服务，它将之前的绑定与实际的地址相关联，完成服务接口的完整定义. WSDL的结构12345678910111213&lt;definitions&gt; &lt;types /&gt; &lt;message /&gt; &lt;portType /&gt; &lt;binding /&gt; &lt;service /&gt;&lt;/definitions&gt; binding元素在这个例子中，portType元素把定义了端口的名称， 还定义了四个操作的名称. 相对于传统的函数库来讲，MathInterfce是函数库，而Add是输入参数为AddMessage，而输出参数为AddMessageResponse的函数. 其它的操作与此类似. 而service元素将MathInterface接口绑定到了http://localhost/math/math.asmx 这个地址. 参考文档 微软关于WSDL的说明 IBM关于WSDL中绑定类型的说明 示例代码 http://blog.csdn.net/onlyqi/article/details/7013893 cxf、axis2与spring-ws的比较 客户端自动生成代码调用 自行编码调用 疑问 怎么发布？: 1)jaxws:endpoint 2) java-ws发布 怎么获取WSDL: 直接在WS地址的后面加上?wsdl即可 绑定类型与编码类型 客户端方式与SOAP方式的区别 JAVA自带API与Axis2, cxf的api使用","categories":[],"tags":[]},{"title":"maven","slug":"工具/maven","date":"2016-10-04T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/04/工具/maven/","link":"","permalink":"http://yoursite.com/2016/10/04/工具/maven/","excerpt":"","text":"Maven生命周期maven中将整个项目构建抽象成一系列的生命周期，具体每个周期的实现交由插件实现，这点可以参照设计模式中“模板方法”的实现. maven的生命周期分为三套：clean， default以及site， 分别对应于清理、构建以及建立项目站点三个环节. 不同的生命周期又可以进一步划分为不同的阶段(phase)，在同一个生命周期内，后面的阶段依赖于前面的阶段. 不同的生命周期不会相互影响. e.g. clean周期包括pre-clean, clean和post-clean三个阶段，如果调用了clean:clean阶段，则clean:pre-clean和clean:clean都会被调用. 生命周期的不同阶段maven中三个不同的生命周期又可以进一步划分为不同的阶段. clean: pre-clean, clean, post-clean default: 校验，初始化、编译、测试、打包、集成测试、安装、部署 validate, initialize, generate-source, process-source, generate-resource, process-resource, compile, process-classes, generate-test-source, process-test-source, generate-test-resource, process-test-resource, test-compile, process-test-classes, test, prepare-package, package, pre-integration-test, integration-test, post-integration-test, verify, install, deploy site: pre-site, site, post-site, site-deploy 常见的插件 http://www.infoq.com/cn/news/2011/04/xxb-maven-7-plugin/","categories":[],"tags":[]},{"title":"Jaxws和jaxrs","slug":"SOA/Jaxws和jaxrs","date":"2016-10-02T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/10/02/SOA/Jaxws和jaxrs/","link":"","permalink":"http://yoursite.com/2016/10/02/SOA/Jaxws和jaxrs/","excerpt":"","text":"Jaxws和jaxrsjaxws这是一个api规范，需要提供相应的运行时实现. 不过J2se中提供了jaxws的参考实现（jaxws-ri) @WebService, @WebMethod wsimport自动生成客户端代码 调用服务端的方法 实现了jaxws规范的框架包括： cxf, axis2 jaxrs用于更方便地创建RESTful服务的api, 它也是一个api规范，也需要提供运行时实现，j2se中并没有提供相应的实现，不过可以自行根据需要添加. jersey是jaxrs的参考实现. @Path, @GET/@PUT/@POST/@DELETE @PathParam, @QueryParam, @HeaderParam, @CookieParam, @MatrixParam @Consumes, @Produces, @ApplicationPath @Context, @NotNull, @Email 实现了jaxrs规范的框架包括： cxf, jersey","categories":[],"tags":[]},{"title":"mockito","slug":"测试/mockito","date":"2016-09-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/25/测试/mockito/","link":"","permalink":"http://yoursite.com/2016/09/25/测试/mockito/","excerpt":"","text":"mockito 调用自己的方法: doReturn(sth).when(xxx).somemethod() 返回列表中的元素: when(obj.method()).thenAnswer(AdditionalAnswers.returnsElementsOf()); 抛出异常: doThrow().when(someObject).someMethod(). 创建mock对象的方式: @InjectMock, @Mock 在verify的时候 次数可以通过以下的方式指定 ：times()/never()/atLeastOnce()/atLeast()/atMost() 另外，执行的顺序，可以通过InOrder来指定, 也可以通过createStrictMock来创建顺序有关的mock对象 12345InOrder obj = inorder(obj); obj.verify(methodA); obj.verify(methodB); * 执行的时间可以通过timeout来指定 自定义mock操作可以通过Answer接口来实现 部分mock(partially mock)可以通过spy或者doCallRealMethod来实现.1doCallRealMethod().when().someMethod() 参考文献 教程： http://www.w3ii.com/en-US/mockito/default.html 部分mock: http://heipark.iteye.com/blog/1496603 中文教程: http://blog.csdn.net/bboyfeiyu/article/details/52127551 http://blog.csdn.net/sdyy321/article/details/38757135 示例代码 https://github.com/Essviv/spring/tree/master/src/test/java/com/cmcc/syw/service/impl","categories":[],"tags":[]},{"title":"hashMap和concurrentHashMap的区别","slug":"多线程/hashMap和concurrentHashMap的区别","date":"2016-09-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/24/多线程/hashMap和concurrentHashMap的区别/","link":"","permalink":"http://yoursite.com/2016/09/24/多线程/hashMap和concurrentHashMap的区别/","excerpt":"","text":"hashMap和concurrentHashMap的区别 线程安全concurrentHashMap是线程安全的，而hashMap不是 同步机制hashMap执行操作时并不会执行同步，但可以通过Collections.synchronizedMap(hashMap)来得到一个与Hashtable等同的对象，对于这个对象的所有操作都会获取整个map的锁concurrentHashMap将整个map分成16个部分（默认），每次进行操作时，都只会获取其中一个部分的锁，从而同时允许多个线程进行操作 NULL值concurrentHashMap的键和值都不允许是null, 而hashMap可以有一个null键 性能hashmap的性能最好，因为它不会执行任何同步操作，而concurrentHashMap的性能略差一些","categories":[],"tags":[]},{"title":"WebService相关概念","slug":"SOA/WebService相关概念","date":"2016-09-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/24/SOA/WebService相关概念/","link":"","permalink":"http://yoursite.com/2016/09/24/SOA/WebService相关概念/","excerpt":"","text":"WebService相关概念webService的分类, 大体上可以分成以下三类： SOAP+WSDL(jaxws规范) REST(jaxrs规范) XML-RPC WebService的实现方式包括SOAP、REST和XML-RPC. XML-RPC也是我们通常说的RPC，已逐渐被SOAP替代 WebService与SOA的关系SOA全称是Service Oriented Architecture， 面向服务的架构. 因此可以这么理解，这是一种软件组织方式，不同的组件使用webService对外提供相应的服务，供其它组件进行调用. WS可以认为是SOA的一种实现方式，反过来说，SOA并不一定需要通过WS来实现. WebServices are self describing services that will perform well defined tasks and can be accessed through the web. Service Oriented Architecture (SOA) is (roughly) an architecture paradigm that focuses on building systems through the use of different WebServices, integrating them together to make up the whole system. 开源框架 jaxrs的实现: jersey, cxf, RESTeasy jaxws的实现： cxf, axis2 备注 axws和jaxrs都是规范，它依赖于具体的实现，在j2se中包含了jaxws的参考实现(jaxws RI), 不包含jaxrs的参考实现(jersey) 在jaxws的情景中，每一个服务都会对应于一个service， 而相应的代理被称为PORT， 在进行客户端编码时，一般是通过wsimport自动生成相应的代码，然后先获取相应的service，再由service获取相应的port，然后调用相应的方法即可. 以下这段话可能会对理解这些概念有所帮助. 在部署的时候，都是由web.xml为入口进入到webapp中，再由相应的servlet进行匹配（CxfServlet），再根据定义的服务地址进行处理. 参考文献 关于jax-ws和jax-rs的描述 SOA与WebService XML-RPC与SOAP http://www.cnblogs.com/lanxuezaipiao/archive/2013/05/11/3072436.html J2ee turtorial","categories":[],"tags":[]},{"title":"JAVA中的多线程基础知识","slug":"多线程/JAVA中的多线程基础知识","date":"2016-09-21T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/21/多线程/JAVA中的多线程基础知识/","link":"","permalink":"http://yoursite.com/2016/09/21/多线程/JAVA中的多线程基础知识/","excerpt":"","text":"JAVA中的多线程基础知识Thread 继承Thread对象 实现Runnable接口 线程的Sleep和Interrupt当需要挂起当前线程时，可使用Sleep方法进行挂起， 在挂起期间，可以使用interrupt方法进行中断，对于中断的处理取决于程序的实现，线程的中断状态通过Thread对象内部的成员变量进行标识，可以使用isInterrupt和interrupted两个方法进行判断，这两个方法的区别可以参阅javadoc的说明。 Join当调用join方法时，当前的线程会进入等待状态，直到被调用join方法的线程执行结束，例如，调用t.join()后，当前线程会一直等待，直接进程t执行完成，当然，等待的过程也可以通过interrupt来进行中断。 如果在主线程中执行以下的操作，有一点值得注意的是，当执行t1.join时，主线程会等待t1线程执行完毕，但这个操作并不影响t2线程的继续执行. 换句话说，执行t1.join之后，主线程等待，而t2线程仍然在执行. 1234t1.start();t2.start();t1.join();t2.join(); 锁类型对象锁和类锁是独立的， 在方法上加synchonized获取的是对象锁，而在静态方法上加synchronized获取的是类锁，两者可以同时被不同的线程获取到; 另外，对方法加synchronized可以认为是对代码块加synchronized的一种简便方式，具体请参阅“synchronized关键字解析”一文 如果某个线程已经拥有了某个锁，那么其它的线程就不能再拥有这个锁;但是这个线程本身可以再次获取这个锁，也就是重入锁机制(reentrant lock) 原子操作以下两种操作可以认为是原子操作，另外cocurrency包中也提供了一些原子类型 对所有引用类型及大部分原生类型的读写操作是原子性的 对所有声明为volatile的变量的读写是原子性的（包括double, long) 多线程中常见的问题 死锁： 线程A获取了对象M的锁，并试图获取对象N的锁；同时，线程B获取了对象N的锁，又试图获取对象M的锁；根据锁机制，线程A和B会同时被阻塞进入等待状态，并且这种等待是不会停止的，因此称为死锁 饥饿： 如果某个线程（A）长期的占用某个对象锁，而另一个线程（B）又需要频繁地调用这个对象的另一个同步方法，那么很有可能这个线程（B）会被经常地阻塞，这种状态就称作饥饿 活锁： 线程A需要响应线程B的事件，而线程B又需要响应线程A的事件，这样它们两个线程虽然没有被阻塞，却一直忙于响应对方的事件从而没办法往下继续 wait操作调用了对象的wait操作之后，当前线程（A）就自动释放对象锁，并处于等待状态；当另一个线程（B）获取到该对象锁并调用notifyAll通知正在等待该锁的所有线程，当前线程（A）会重新获得对象锁，并进行一些相应的操作，相应的例子可参阅这里的“生产者-消费者”的实现 不可变对象 不对外提供任何set方法 所有的成员变量都声明成private和final， 声明成private是不允许从外部进行访问和修改，声明成final是不允许从内部进行修改 将类声明成final，以此来防止子类继承并重写类方法 如果成员变量中有引用，应在构造函数中避免直接存储外部提供的引用，应该使用深拷贝的方式来创建新的对象并存储; 同样，当返回内部引用对象时，应避免直接返回内部对象，而应该通过拷贝对象返回 具体可参考http://docs.oracle.com/javase/tutorial/essential/concurrency/imstrat.html","categories":[],"tags":[]},{"title":"2016学习计划","slug":"2016学习计划","date":"2016-09-13T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/13/2016学习计划/","link":"","permalink":"http://yoursite.com/2016/09/13/2016学习计划/","excerpt":"","text":"2016年学习计划 2月份-3月份： 学习缓存技术，包括redis, memcached和ehcached 4月份： 学习JAVA中的多线程技术，可以通过官方文档和教程相结合的方式来学习，在学习的过程中，注意理解其原理，结合git上一些实际的项目源码进行分析 5月份： 学习NIO技术，并学习netty和mina两大框架的使用，通过学习其源码，加深对NIO的理解和实现和使用，备选的教程包括官方文档，教程以及源码 6月份： 学习MQ的使用，包括rabbitMQ和kafka 7月份： 学习tomcat和nginx，进一步了解tomcat和nginx的机制和使用 8月份： 学习jQuery和bootstrap的使用，争取能够解决一般的前端JS和脚本问题 9月份： 学习FreeMarker和Velocity，并使用它们完成一些模板功能 10-12月份： 学习struts，并在实际的项目中使用 学习各种技术的时候，注意要“知其然知其所以然”,在学习阶段完成后，最好能结合实际的例子做个项目，或者结合git上相应的项目源码，进一步加深对这门技术的理解。 haproxy, tomcat, nginx","categories":[],"tags":[]},{"title":"消息发布的确认","slug":"消息队列/rabbitMQ/消息发布的确认","date":"2016-09-13T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/13/消息队列/rabbitMQ/消息发布的确认/","link":"","permalink":"http://yoursite.com/2016/09/13/消息队列/rabbitMQ/消息发布的确认/","excerpt":"","text":"消息发布的确认在RMQ中，消息发布的确认有两种方式，一种是AMQP标准的方式，一种是RMQ扩展的方式 1. 事务机制通过事务的机制来保证每次消息的发布是成功的，如示例代码所示, 但这个方法最大的不足是它必须等待broker处理完后才能继续，导致这种方式的效率非常低，大概比没有启用事务要低250%左右。 123channel.txSelect();channel.basicPublish();channel.txCommit(); //or channel.txRollback(); 2. 确认机制除了事务机制外，RMQ还提供了一种确认机制，它通过broker的确认来实现。通过将通道切换成confirm模式来进行确认，如下代码所示，它的效率要比事务机制高得多 123channel.addConfirmListener();channel.confirmSelect();channel.basicPublish(); 3. 确认机制和事务机制不能同时使用4. 消息确认的时机对于无法路由到任何队列的消息来讲，它将直接进行确认，对于可以路由的消息来讲，直到所有的队列都接收到消息后才会进行消息确认；对于持久化队列来讲，接收到消息意味着持久化已经完成或者已经被全部消费；对于镜像队列而言，接收到消息意味着所有的镜像队列都已经收到相应的消息，如下图所示.由于RMQ内部是通过批量序列化的方式来进行持久化的，因此对于持久化队列而言，消息确认的延迟可能会非常长 参考文档 消息发布的确认 消息发布确认的介绍","categories":[],"tags":[]},{"title":"策略模式","slug":"设计模式/策略模式","date":"2016-09-10T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/09/10/设计模式/策略模式/","link":"","permalink":"http://yoursite.com/2016/09/10/设计模式/策略模式/","excerpt":"","text":"策略模式策略模式最重点的特点是完成一个任务的方法有多种策略，客户端可以根据需要选择合适的策略来进行. 不同的策略间可以相互替换, 典型的应用包括排序、搜索等实现 它的UML图如下： 策略模式与模板模式的区别从模板模式的UML图中可以看出，模板模式定义了方法的模板实现 ，而将一些修改化的实现方式通过抽象方法的方式委托给相应的子类来实现.","categories":[],"tags":[]},{"title":"mybatis中的批量操作","slug":"数据库/mybatis中的批量操作","date":"2016-08-30T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/08/30/数据库/mybatis中的批量操作/","link":"","permalink":"http://yoursite.com/2016/08/30/数据库/mybatis中的批量操作/","excerpt":"","text":"mybatis中的批量操作1. 语法mybatis中关于批量操作的支持主要来自foreach标签，foreach支持的属性包括以下几点： collection: 集合的变量名称， 类似于JAVA的for循环中的集合 item: 循环变量的名称，类似于JAVA的for循环中的临时变量名称，后续可以这个变量名称访问到每次循环的变量对象 index: 循环变量的下标 open: 循环开始前，在开始增加的表达式 close: 循环结束后，在末尾增加的表达式 seperator: 每个循环之间要增加的表达式 12345678910111213141516举例： where id in &lt;foreach collection=&quot;records&quot; item=&quot;record&quot; open=&quot;(&quot; close=&quot;)&quot; seperator=&quot;,&quot;&gt; #&#123;record.id&#125; &lt;/foreach&gt;解析这段表达式得到的结果为： where id in (id1, id2, id3,……)在这里，open属性指定在循环开始前增加左括号, close属性指定在循环结束后增加右括号，而seperator指定在每次循环间增加逗号，因此形如上面的表达式 2. 操作利用mybatis的foreach标签可以实现批量操作，主要分为： 2.1 批量插入123456789101112131415161718192021例子1： insert into table(col1, col2, col3) values &lt;foreach collection=&quot;records&quot; item=&quot;record&quot; seperator=&quot;,&quot;&gt; (#&#123;record.col1&#125;, #&#123;record.col2&#125;, #&#123;record.col3&#125; ) &lt;/foreach&gt;解析为： insert into table(col1, col2, col3) values (record1.col1, record1.col2, record1.col3), (record2.col1, record2.col2, record2.col3), (record3.col1, record3.col2, record3.col3)注意这里的左右括号不能通过open和close属性进行指定，因为open和close属性指定的是在整个循环开始和结束后要增加的表达式，而批量插入操作需要在每次循环表达式前后加上括号，错误代码示例如下： 12345678910111213141516171819例子2： insert into table(col1, col2, col3) values &lt;foreach collection=&quot;records&quot; item=&quot;record&quot; seperator=&quot;,&quot; open=&quot;(&quot; close=&quot;)&quot;&gt; #&#123;record.col1&#125;, #&#123;record.col2&#125;, #&#123;record.col3&#125; &lt;/foreach&gt;解析为： insert into table(col1, col2, col3) values (record1.col1, record1.col2, record1.col3, record2.col1, record2.col2, record2.col3, record3.col1, record3.col2, record3.col3) 2.1.1 批量插入自动生成ID在使用mybatis时，如果是插入单条记录时，可以使用useGeneratedKeys和keyProperty属性来获取新增记录的自增ID，如果在批量插入时也需要获取所有新增记录的自增ID，则需要满足以下两点: mybatis的版本不低于3.3.1， 从这个版本开始，mybatis才支持批量插入记录时获取记录的自增ID 入参的集合名称必须取名为“collection”、“list”、“array” 1234567891011&lt;insert id=&quot;batchInsert&quot; useGeneratedKeys=&quot;true&quot; keyProperty=&quot;id&quot;&gt;insert into table(col1, col2, col3) values &lt;foreach collection=&quot;list&quot; item=&quot;record&quot; seperator=&quot;,&quot;&gt; (#&#123;record.col1&#125;, #&#123;record.col2&#125;, #&#123;record.col3&#125; ) &lt;/foreach&gt;&lt;/insert&gt; 2.2 批量更新利用mybatis来实现记录的批量更新操作时，需要区分两种情况： 所有记录更新成相同的值 所有记录更新成不同的值 2.2.1 更新成相同的值这种情况相对来讲比较简单，只要对单个更新操作稍微做点修改即可完成, 示例代码如下： 123456789101112131415161718192021222324update table set col1 = #&#123;value1&#125;, col2 = #&#123;value2&#125;where id in &lt;foreach collection=&quot;ids&quot; item=&quot;id&quot; open=&quot;(&quot; close=&quot;)&quot; seperator=&quot;,&quot;&gt; #&#123;id&#125; &lt;/foreach&gt;====&gt; update table set col1=#&#123;value1&#125;, col2=#&#123;value2&#125;where id in (id1, id2, id3...) 2.2.2 更新成不同的值这种情况下，对于不同的记录需要对某些字段更新不同的值. 这里主要利用了mysql中的case/when/then子句的功能来实现， case/when/then的语法可参见这里. 利用mybatis实现这个操作的语法如下： 1234567891011121314151617181920212223242526272829303132333435update table set col1 = CASE &lt;foreach collection=&quot;records&quot; item=&quot;record&quot; close=&quot;ELSE `col1` END&quot;&gt; WHEN id = #&#123;record.id&#125; THEN #&#123;record.col1&#125; &lt;/foreach&gt;where id in &lt;foreach collection=&quot;records&quot; item=&quot;record&quot; open=&quot;(&quot; close=&quot;)&quot; seperator=&quot;,&quot;&gt; #&#123;record.id&#125; &lt;/foreach&gt;====&gt;update table set col1 = CASE WHEN id = #&#123;record1.id&#125; THEN #&#123;record1.col1&#125; WHEN id = #&#123;record2.id&#125; THEN #&#123;record2.col1&#125; WHEN id = #&#123;record3.id&#125; THEN #&#123;record3.col1&#125; ELSE `col1` ENDwhere id in (id1, id2, id3) 这里有几个需要说明的地方： close属性的值： 由于 case的语法指定，如果某个记录不符合任何一个when的情况，会默认把它的相应属性设置为null， 因此这里需要加上else子句，来保证在不符合任何一个when的情况，保留原字段的值 where子句： 指定更新的范围是指定的记录范围 参考文档 http://zacard.net/2016/02/18/mybatis3-multiple-rows-write-bace-id/","categories":[],"tags":[]},{"title":"Cookie的属性","slug":"Web/Cookie的属性","date":"2016-08-26T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/08/26/Web/Cookie的属性/","link":"","permalink":"http://yoursite.com/2016/08/26/Web/Cookie的属性/","excerpt":"","text":"Cookie的属性Cookie在WEB开发中具有重要的意义，它有以下的属性，现解析如下： secure： 当secure=true时，表示该cookie只能通过安全通道（即https）传输给服务器端， 当通道为http时，这个cookie不会由浏览器传输回服务器端; 通过这个标识，能保证cookie不会经由不安全的通道发往服务器端，意即在传输的过程中是保证安全的；但是，在浏览器端仍然可以看到cookie的值，如果需要也可以对cookie进行加密处理 path: 表示在访问哪些路径时，客户端必须把这个cookie传输回服务端. 如果设置了cookie的path属性为/contextPath/A， 那么当访问/contextPath/A以及所有它的子目录时，浏览器都会自动带上这个cookie; 相反地，如果访问的是/contextPath/B, 那么浏览器就不会带上这个cookie domain: 功能和path类似，但它指定的是域名范围的地址 expire: 设置cookie的有效期","categories":[],"tags":[]},{"title":"Spring学习之IoC","slug":"spring-ioc-2","date":"2016-07-12T13:45:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/07/12/spring-ioc-2/","link":"","permalink":"http://yoursite.com/2016/07/12/spring-ioc-2/","excerpt":"","text":"配置细节 由于配置文件中提供的参数类型都是string类型的，而实际的参数类型有可能是任何类型，因此两者之间必然存在相应的转化，这是通过ConversionService来完成的. p命名空间可以很方便地通过属性来指定属性值 c命名空间可以用来很方便地通过属性指定构造函数的参数值 depends-on可以用来显式地指定依赖关系，在使用bean之前，它所依赖的所有bean都必须完成初始化操作. 如果需要表达对多个bean的依赖关系，可以通过逗号，分号以及空格分隔. 1234&lt;bean id=&quot;beanOne&quot; class=&quot;ExampleBean&quot; depends-on=&quot;manager,accountDao&quot; /&gt;&lt;bean id=&quot;manager&quot; class=&quot;ManagerBean&quot; /&gt;&lt;bean id=&quot;accountDao&quot; class=&quot;x.y.jdbc.JdbcAccountDao&quot; /&gt; 默认情况下，容器在初始化时，会默认初始化所有的singleton对象，这样如果这些对象的配置有问题，在容器初始化的时候就会被及时发现，而不是等到容器运行了很长时间后才被发现。如果希望容器在需要使用单例对象时才生成相应的单例对象，可以设置它的lazy-initialized属性，这样，容器就只会在这个对象第一次被请求的时候才去生成这个对象. 前面提到过，在spring框架中，每个bean对象的生命周期是不一样的，这个生命周期通过scope属性来表示，可以是singleton, prototype以及其它一些选择. 之前也提到过这个问题，如果在singleton对象中需要引用另一个prototype的对象，这个对象的注入时机是在singleton对象初始化的时候完成，也就是说，这个注入只会完成一次. 如果希望单例对象在每次请求时都获取新的prototype对象，spring提供了两种方式： 实现ApplicationContextAware接口： 通过这个接口，单例对象可以获取到ApplicationContext对象，即容器对象，这样当它需要prototype对象时，只需要通过容器获取一次即可. 但这个方法让业务层代码感知到容器的存在，有一定的侵入性. 需要通过方法注入的方式来完成: 通过指定lookup-method属性，可以将某个在容器中托管的对象映射给方法，也就是说，方法的返回对象被指定的容器对象替换. Spring框架是通过CGLib动态代理的方式来实现相应的机制的. 具体的示例代码如下所示： 12345678910111213141516171819202122public abstract class CommandManager &#123; public Object process(Object commandState) &#123; // grab a new instance of the appropriate Command interface Command command = createCommand(); // set the state on the (hopefully brand new) Command instance command.setState(commandState); return command.execute(); &#125; // okay... but where is the implementation of this method? protected abstract Command createCommand();&#125;&lt;!-- a stateful bean deployed as a prototype (non-singleton) --&gt;&lt;bean id=&quot;command&quot; class=&quot;fiona.apple.AsyncCommand&quot; scope=&quot;prototype&quot;&gt; &lt;!-- inject dependencies here as required --&gt;&lt;/bean&gt;&lt;!-- commandProcessor uses statefulCommandHelper --&gt;&lt;bean id=&quot;commandManager&quot; class=&quot;fiona.apple.CommandManager&quot;&gt; &lt;lookup-method name=&quot;createCommand&quot; bean=&quot;command&quot;/&gt;&lt;/bean&gt; Bean的范围对象的Scope属性定义了该对象的生命周期，关于这个之前已经说过很多，本节再对scope属性进行详细的描述. 总得来讲，对象的scope属性有以下几种： Scope 描述 Singleton 单例对象，容器中只存在一个对象，这是spring对象默认的scope值 prototype 原型对象，每次请求容器时，容器都将生成新的对象实例 request 单次请求范围有效，也就是说在每次请求时，容器都会生成新的对象实例，这个范围只在web应用中有效 Session 会话范围有效，也就是说在开始新的会话时，容器会生成新的对象实例，并在会话期间保持不变，同样，这个范围也只在web应用中有效 application 在servletContext范围有效，注意这里是servletContext范围唯一，而单例对象是在容器范围内唯一，这个范围也只在web应用中有效 globalSession 略 webSocket 略 singleton单例对象在容器范围保持唯一，它的示意图如下，也就是说，当容器请求单例对象时，总是返回同一个对象. 同时它也是spring中默认的scope值. prototype原型对象，从它的命名上就可以看出，它是作为对象的原型来使用，相当于是创建对象的模板. 当向容器请求原型对象时，容器总是返回一个新的原型对象，示意图如下. 和其它scope类型不同的是，spring容器并没有完整地管理prototype对象的生命周期，当容器创建好原型对象后，它就将原型对象完全交付给客户端，而不管它后续的所有生命周期。这也就意味着，虽然spring容器会对所有对象执行初始化操作(postConstruct或者initialize)，但它并不一定会执行相应的回收操作（preDestroy或者destructor)，原型对象的析构操作必须由客户端自行完成. 如果需要由容器来完成相应的回收操作，那么必须通过实现BeanPostProcessor接口来完成. 自定义生命周期Spring框架中提供了许多扩展点，供用户在需要的时候扩展实现自定义的行为，其中一个就是生命周期函数. 生命周期函数的扩展可以通过以下方式来进行： 实现InitializingBean和DisposobleBean接口：spring团队不建议使用这种方式来扩展，因为它将业务代码和框架代码进行不必要的耦合 使用@PostConstruct与@PreDestroy注解：这是推荐的作法，在容器构造完对象后，会调用postConstruct注解的方法进行初始化操作；同样地，在容器析构对象之前，会调用preDestroy注解的方法执行相应的操作. 这种方式对应于XML配置时的init-method属性和destroy-method属性. 参考文献官方文档：官方文档","categories":[],"tags":[{"name":"spring Ioc","slug":"spring-Ioc","permalink":"http://yoursite.com/tags/spring-Ioc/"}]},{"title":"Spring学习之IoC","slug":"spring-ioc-1","date":"2016-07-05T13:45:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/07/05/spring-ioc-1/","link":"","permalink":"http://yoursite.com/2016/07/05/spring-ioc-1/","excerpt":"","text":"简介Spring框架中提供了很全面的功能，其中最基础的模块当属于容器的IoC以及AOP功能，以及它的事务管理机制. 在接下来的几篇文章中，将就这三个方面展开详细的阐述. IoCIoC也可以称为是DI，也就是依赖注入的意思，它的主要作用是由容器生成应用程序所需要的bean对象，并维护它们的生命周期以及相互间的依赖关系，而不是由bean自身来管理和维护它与其它对象之间的依赖关系. Spring中提供了BeanFactory接口来管理对象，同时也提供了ApplicationContext接口来提供更丰富的功能，包括AOP、事件发布、消息源处理等等功能，它是BeanFactory的子接口，但功能更丰富，因此,ApplicationContext接口也就是我们平时所说的容器. Spring容器的主要作用可以通过下图展示，在下图中，一系列的对象以及相应的配置文件在容器中完成组装，在容器初始化完成后，一个完整可用的应用程序就形成了，它内部的各种对象间的依赖关系也全部建立，而这些都是由容器根据配置文件自动组装完成的. 配置元数据配置信息也称为元数据， 从上述的描述可知，spring容器要自动完成bean对象的组装，需要接受相应的配置信息. 在spring框架中，配置信息的形式可以有很多种，包括： 传统的xml配置文件，多个配置文件可以通过import语句导入，注意这里使用的路径总是相对于目前的xml文件而言，也就是说它只接受相对路径，前置的”/“会被忽略 注解方式 代码方式(spring3.0引入). 示例以下的代码完整地展示了容器的配置、初始化及使用过程，大体上所有使用容器的步骤都可以分为这几个部分： 12345678910111213141516171819202122232425262728//配置信息，services.xml&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;bean id=&quot;...&quot; class=&quot;...&quot;&gt; &lt;!-- collaborators and configuration for this bean go here --&gt; &lt;/bean&gt; &lt;!-- more bean definitions go here --&gt;&lt;/beans&gt;//初始化及使用ApplicationContext context = new ClassPathXmlApplicationContext(new String[] &#123;&quot;services.xml&quot;, &quot;daos.xml&quot;&#125;);// retrieve configured instancePetStoreService service = context.getBean(&quot;petStore&quot;, PetStoreService.class);// use configured instanceList&lt;String&gt; userList = service.getUsernameList(); Bean对象的定义Bean对象在spring容器是通过BeanDefinition来定义的，它可以认为是容器创建Bean对象时的“菜单”，里面记录了完整的类名、类的依赖关系、生命周期方法以及其它属性等信息，有了这些信息，容器就可以在合适的时机初始化相应的对象. 具体来讲，bean对象的定义包括以下内容： 属性名 描述 class 类的完整名称 name 类的名称，可以作为类的标识，可以指定多个，用逗号分开 scope 作用域，这个会在后续的章节中进行详细阐述 constructor arguments 构造方法参数 properties 各种属性 autowiring mode 自动注入模式 lazy-initializing mode 延迟初始化模式，如果bean对象是延迟初始化模式，那么在容器初始化的时候，并不会去初始化这个bean，直到后续有请求需要用到这个bean时才会执行初始化操作 initializing method 初始化方法 destruction method 析构方法 除此之外，spring还允许注册在容器外生成的对象，这种方式是通过ApplicationContext来获取BeanFactory，然后再调用registerBeanDefinition方法来完成对象的注册. 对象标识对象的标识可以由一个ID或者多个name组合而成，同时也可以通过alias属性为bean对象设置别名. 在需要引用对象的地方，可以通过相应的标识进行引用.（ref或者idref) 如果在定义bean对象的时候没有显式地指定名称，那么spring框架将会默认给它提供一个，默认的对象名将遵循java规范，将类名的第一个字母小写，而后遵循camelCase原则. 12&lt;bean name=&quot;nameA,nameB,nameC&quot; id=&quot;idA&quot; /&gt;&lt;alias name=&quot;nameA&quot; alias=&quot;aliasA&quot; /&gt; 对象的初始化对象的初始化有两种方式, 在这两种方式中，如果需要提供相应的构造函数参数，都可以通过来指定： 直接定义bean对象. 这种方法有点类似于使用new操作符，spring框架直接调用对象的构造函数完成对象的初始化. 通过工厂方法来完成. 这种方法又可以进一步细分为两种： 通过静态类的工厂方法. 通过class与来指定工厂方法，工厂方法的返回值类型即为创建对象的实际类型，注意返回的类型并不一定需要与静态类的类型保持一致 通过实例的工厂方法. 这种情况下不需要指定class，而是通过与来指定工厂方法，同样地，工厂方法的返回值类型即为创建对象的实际类型. 123456789101112//静态类工厂方法&lt;bean id=&quot;clientService&quot; class=&quot;examples.ClientService&quot; factory-method=&quot;createInstance&quot;/&gt;//实例的工厂方法&lt;bean id=&quot;serviceLocator&quot; class=&quot;examples.DefaultServiceLocator&quot; /&gt;&lt;!-- the bean to be created via the factory bean --&gt;&lt;bean id=&quot;clientService&quot; factory-bean=&quot;serviceLocator&quot; factory-method=&quot;createClientServiceInstance&quot;/&gt; 依赖注入构造函数注入这种方式就是指依赖关系是通过构造函数的参数传入的，在使用的时候，可以使用进行指定. 在不引起歧义的情况下，容器会自动根据提供的参数类型传给构造函数，比如当所有参数的类型都不一样时，就不存在歧义 当存在类型转化或者会引起歧义的情况时，可以通过type指定具体类型以及index来指定参数在构造函数中的参数位置来进一步确定参数间的匹配. 123456789101112131415161718192021222324252627282930//没有歧义，不需要再额外指定参数&lt;beans&gt; &lt;bean id=&quot;foo&quot; class=&quot;x.y.Foo&quot;&gt; &lt;constructor-arg ref=&quot;bar&quot;/&gt; &lt;constructor-arg ref=&quot;baz&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;bar&quot; class=&quot;x.y.Bar&quot;/&gt; &lt;bean id=&quot;baz&quot; class=&quot;x.y.Baz&quot;/&gt;&lt;/beans&gt;//参数的匹配有歧义，需要额外指定匹配参数,不指定的话，则无法知道“7500000”与“42”哪个参数该匹配到yearspublic class ExampleBean &#123; // Number of years to calculate the Ultimate Answer private int years; // The Answer to Life, the Universe, and Everything private String ultimateAnswer; public ExampleBean(int years, String ultimateAnswer) &#123; this.years = years; this.ultimateAnswer = ultimateAnswer; &#125;&#125;&lt;bean id=&quot;exampleBean&quot; class=&quot;examples.ExampleBean&quot;&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;7500000&quot;/&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;42&quot;/&gt;&lt;/bean&gt; setter方法注入setter方法注入就是通过调用相应属性的set方法来完成，没有太多的内容，不作阐述. 如何选择既然有两种方式可以完成依赖关系的注入，那么肯定就会有这样的疑问，什么情况下该用哪种方式来完成依赖呢? 这里引用一个描述来回答这个问题： 使用构造函数来注入那些必需的依赖关系，而对于那些不强制要求的依赖关系，使用setter依赖来完成 参考文献官方文档:官方文档","categories":[],"tags":[{"name":"spring Ioc","slug":"spring-Ioc","permalink":"http://yoursite.com/tags/spring-Ioc/"}]},{"title":"ServletContext与ApplicationContext的区别","slug":"spring-differerce-between-application-context-and-servlet-context","date":"2016-07-02T14:25:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/07/02/spring-differerce-between-application-context-and-servlet-context/","link":"","permalink":"http://yoursite.com/2016/07/02/spring-differerce-between-application-context-and-servlet-context/","excerpt":"","text":"Spring中的概念在阅读Spring源码或相关的文献时，经常会遇到WebApplicationContext, ApplicationContext, ServletContext以及ServletConfig等名词，这些名词都很相近，但适用范围又有所不同，对理解源码及spring内部实现造成混淆，因此有必要对这些概念进行一些比较. 为了后续比较的方便，首先我们先来澄清这几个名词的概念 ServletContext: 这个是来自于servlet规范里的概念，它是servlet用来与容器间进行交互的接口的组合，也就是说，这个接口定义了一系列的方法，servlet通过这些方法可以很方便地与自己所在的容器进行一些交互，比如通过getMajorVersion与getMinorVersion来获取容器的版本信息等. 从它的定义中也可以看出，在一个应用中(一个JVM)只有一个ServletContext, 换句话说，容器中所有的servlet都共享同一个ServletContext. ServletConfig: 它与ServletContext的区别在于，servletConfig是针对servlet而言的，每个servlet都有它独有的serveltConfig信息，相互之间不共享. ApplicationContext: 这个类是Spring实现容器功能的核心接口，它也是Spring实现IoC功能中最重要的接口，从它的名字中可以看出，它维护了整个程序运行期间所需要的上下文信息， 注意这里的应用程序并不一定是web程序，也可能是其它类型的应用. 在Spring中允许存在多个applicationContext，这些context相互之间还形成了父与子，继承与被继承的关系，这也是通常我们所说的，在spring中存在两个context,一个是root context，一个是servlet applicationContext的意思. 这点后面会进一步阐述. WebApplicationContext: 其实这个接口不过是applicationContext接口的一个子接口罢了，只不过说它的应用形式是web罢了. 它在ApplicationContext的基础上，添加了对ServletContext的引用，即getServletContext方法. 如何配置ServletContext从前面的论述中可以知道, ServletContext是容器中所有servlet共享的配置，它在应用中是全局的 根据servlet规范的规定，可以通过以下配置来进行配置，其中Context-Param指定了配置文件的位置，ContextLoaderListener定义了context加载时的监听器，因此，在容器启动时，监听器会自动加载配置文件，执行servletContext的初始化操作. 12345678&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:conf/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; ServletConfigServletConfig是针对每个Servlet进行配置的，因此它的配置是在servlet的配置中，如下所示， 配置使用的是init-param, 它的作用就是在servlet初始化的时候，加载配置信息，完成servlet的初始化操作 123456789101112&lt;servlet&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/mvc-dispatcher-servlet.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/servlet&gt; 关于applicationContext的配置，简单来讲，在servletContext中配置的context-param参数, 会生成所谓的root application context, 而每个servlet中指定的init-param参数中指定的对象会生成servlet application context, 而且它的parent就是servletContext中生成的root application context, 因此在servletContext中定义的所有配置都会被继承到servlet中， 这点在后续的源码阐述中会有更直观的体现. 源码分析首先先来看ServletContext中的配置文件的加载过程. 这个过程是由ContextLoaderListener对象来完成的，因此我们找到相应的源码，去掉一些日志及不相关的源码后如下： 第一步是判断是否存在rootApplicationContext，如果存在直接抛出异常结束 第二步是创建context对象，并在servletContext中把这个context设置为名称为ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE的属性. 到这里其实已经解释了ApplicationContext与servletContext的区别，它不过是servletContext中的一个属性值罢了，这个属性值中存有程序运行的所有上下文信息 由于这个applicationContext是全局的应用上下文信息，在spring中就把它取名为’root application context’. 12345678910111213141516public WebApplicationContext initWebApplicationContext(ServletContext servletContext) &#123; if (servletContext.getAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE) != null) &#123; &#125; try &#123; // Store context in local instance variable, to guarantee that // it is available on ServletContext shutdown. if (this.context == null) &#123; this.context = createWebApplicationContext(servletContext); &#125; servletContext.setAttribute(WebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, this.context); return this.context; &#125;&#125; 接着再来看DispatcherServlet的源码，作为servlet，根据规范它的配置信息应该是在Init方法中完成，因此我们找到这个方法的源码即可知道servletConfig以及servlet application context的初始化过程: 第一步是从servletConfig中获取所有的配置参数， ServletConfigPropertyValues的构造函数中会遍历servletConfig对象的所有初始化参数，并把它们一一存储在pvs中 第二步就是开始初始servlet，由于dispatcherServlet是继承自FrameworkServlet，因此这个方法在FrameworkServlet中找到，可以看到，在initServletBean中又调用了initWebApplicationContext方法，在这个方法中，首先获取到rootContext， 接着就开始初始化wac这个对象，在创建这个wac对象的方法中，传入了rootContext作为它的parent，也就是在这里，两者之间的父子关系建立，也就形成了我们平时常说的继承关系. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677@Overridepublic final void init() throws ServletException &#123; // Set bean properties from init parameters. PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); // Let subclasses do whatever initialization they like. initServletBean();&#125;//遍历获取servletConfig的所有参数public ServletConfigPropertyValues(ServletConfig config, Set&lt;String&gt; requiredProperties) throws ServletException &#123; while (en.hasMoreElements()) &#123; String property = (String) en.nextElement(); Object value = config.getInitParameter(property); addPropertyValue(new PropertyValue(property, value)); if (missingProps != null) &#123; missingProps.remove(property); &#125; &#125;&#125;//初始化webApplicationContextprotected final void initServletBean() throws ServletException &#123; try &#123; this.webApplicationContext = initWebApplicationContext(); &#125;&#125;//具体的初始化操作实现protected WebApplicationContext initWebApplicationContext() &#123; WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // A context instance was injected at construction time -&gt; use it wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; // The context has not yet been refreshed -&gt; provide services such as // setting the parent context, setting the application context id, etc if (cwac.getParent() == null) &#123; // The context instance was injected without an explicit parent -&gt; set // the root application context (if any; may be null) as the parent cwac.setParent(rootContext); &#125; configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; // No context instance was injected at construction time -&gt; see if one // has been registered in the servlet context. If one exists, it is assumed // that the parent context (if any) has already been set and that the // user has performed any initialization such as setting the context id wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // No context instance is defined for this servlet -&gt; create a local one //就是在这个方法中，servlet application context与root application context的继承关系正式建立 wac = createWebApplicationContext(rootContext); &#125; if (this.publishContext) &#123; // Publish the context as a servlet context attribute. String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); &#125; return wac;&#125;//就是在这个方法中，servlet application context与root application context的继承关系正式建立protected WebApplicationContext createWebApplicationContext(WebApplicationContext parent) &#123; return createWebApplicationContext((ApplicationContext) parent);&#125; 参考文献 Servlet与JSP： Head first to Servlet and JSP JavaDoc: DispatcherServlet FrameworkServlet ContextLoaderListener","categories":[],"tags":[{"name":"Spring ServletContext ServletConfig ApplicationContext","slug":"Spring-ServletContext-ServletConfig-ApplicationContext","permalink":"http://yoursite.com/tags/Spring-ServletContext-ServletConfig-ApplicationContext/"}]},{"title":"SpringMVC学习(6)","slug":"spring-mvc-config","date":"2016-07-02T11:37:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/07/02/spring-mvc-config/","link":"","permalink":"http://yoursite.com/2016/07/02/spring-mvc-config/","excerpt":"","text":"配置SpringMVC在之前的阐述中提到，springMVC框架中提供了很多特殊的对象来实现整个MVC流程的处理（详见概述）. 这一节详细阐述如何分别通过代码和配置的方式在springMVC中自定义这些对象的行为. 启用SpringMVC框架启用SpringMVC框架的方法很简单，如下所示，只需要一行注解代码或者一行配置即可完成. 启用了springMVC框架后，spring自动完成了以下这些事情： 注册RequestMappingHandlerMapping对象 注册RequestMappingHandlerAdapter对象 注册ExceptionHandlerExceptionResolver对象 设置了HttpMessageConverters，具体的实现包括： ByteArrayHttpMessageConverter StringHttpMessageConverter ResourceHttpMessageConverter SourceHttpMessageConverter FormHttpMessageConverter MappingJackson2HttpMessageConverter: 如果在类路径中找到ackson 2，则自动注册这个类 1234567//代码方式@Configuration@EnableWebMVCpublic class WebConfig&#123;&#125;//配置方式, dispatcherServlet.xml&lt;mvc:annotation-driven /&gt; 类型转换和格式化操作如果需要自定义类型转换和格式化，可以通过重写相应的方法或者配置conversionService来完成 1234567891011121314151617181920212223242526272829303132//代码方式@Configuration@EnableWebMVCpublic class WebConfig extends WebMvcConfigurerAdapter&#123; @Override public void addFormatters(FormatterRegistry registry) &#123; // Add formatters and/or converters &#125;&#125;//配置方式&lt;mvc:annotation-driven conversion-service=&quot;conversionService&quot;/&gt;&lt;bean id=&quot;conversionService&quot; class=&quot;org.springframework.format.support.FormattingConversionServiceFactoryBean&quot;&gt; &lt;property name=&quot;converters&quot;&gt; &lt;set&gt; &lt;bean class=&quot;org.example.MyConverter&quot;/&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=&quot;formatters&quot;&gt; &lt;set&gt; &lt;bean class=&quot;org.example.MyFormatter&quot;/&gt; &lt;bean class=&quot;org.example.MyAnnotationFormatterFactory&quot;/&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name=&quot;formatterRegistrars&quot;&gt; &lt;set&gt; &lt;bean class=&quot;org.example.MyFormatterRegistrar&quot;/&gt; &lt;/set&gt; &lt;/property&gt;&lt;/bean&gt; 拦截器可以通过重写addInterceptors方法或者配置元素来对所有请求或者部分请求设置拦截器： 1234567891011121314151617181920212223//代码方式@Overridepublic void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new LocaleInterceptor()); registry.addInterceptor(new ThemeInterceptor()).addPathPatterns(&quot;/**&quot;).excludePathPatterns(&quot;/admin/**&quot;); registry.addInterceptor(new SecurityInterceptor()).addPathPatterns(&quot;/secure/*&quot;);&#125;//配置方式&lt;mvc:interceptors&gt; &lt;bean class=&quot;org.springframework.web.servlet.i18n.LocaleChangeInterceptor&quot;/&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;mvc:exclude-mapping path=&quot;/admin/**&quot;/&gt; &lt;bean class=&quot;org.springframework.web.servlet.theme.ThemeChangeInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/secure/*&quot;/&gt; &lt;bean class=&quot;org.example.SecurityInterceptor&quot;/&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 内容协商管理器在介绍SpringMVC的RESTful支持(视图解析)时曾提过，spring框架可以支持按照请求后缀名或者消息头返回相应格式的内容，实现的主要方法就是配置ContentNegotiatingViewResolver. 而将请求的后缀或消息头转化成相应的媒体类型就是由ContentNegotiateManager来完成，配置这个管理器的方法同样也有通过代码和配置文件两种方式： springMVC中, 可以根据需要给RequestMappingHandlerMapping、RequestMappingHandlerAdapter以及ExceptionHandlerExceptionResolver提供相同的内容协商器，也可以根据需要提供各自的协商器，具体根据需要进行确定. 12345678910111213141516//代码方式@Overridepublic void configureContentNegotiation(ContentNegotiationConfigurer configurer) &#123; configurer.mediaType(&quot;json&quot;, MediaType.APPLICATION_JSON);&#125;//配置方式&lt;mvc:annotation-driven content-negotiation-manager=&quot;contentNegotiationManager&quot;/&gt;&lt;bean id=&quot;contentNegotiationManager&quot; class=&quot;org.springframework.web.accept.ContentNegotiationManagerFactoryBean&quot;&gt; &lt;property name=&quot;mediaTypes&quot;&gt; &lt;value&gt; json=application/json xml=application/xml &lt;/value&gt; &lt;/property&gt;&lt;/bean&gt; 在没有使用SpringMVC环境的场景时： 如果需要使用RequestMappingHandlerMapping, 必须生成一个ContentNegotiationManager实例并把它配置到处理器映射对象中，以完成请求的映射 如果需要使用RequestMappingHandlerAdapter或者ExceptionHandlerExceptionResolver， 也必须生成一个ContentNegotiationManager的实例来完成内容类型的协商 视图控制器视图控制器的作用很简单，它不提供相应的业务处理逻辑，只是将请求映射成相应的视图，它的配置如下: 12345678//代码方式@Overridepublic void addViewControllers(ViewControllerRegistry registry) &#123; registry.addViewController(&quot;/&quot;).setViewName(&quot;home&quot;);&#125;//配置方式&lt;mvc:view-controller path=&quot;/&quot; view-name=&quot;home&quot;/&gt; 资源配置资源配置主要是针对静态资源的处理，具体是由ResourceHttpRequestHandler类进行处理，它可以根据请求头里的信息决定返回304还是200. 配置信息主要是配置哪些路径的请求交由这个类进行处理，以及资源的位置. 具体如下: 12345678//代码方式@Overridepublic void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/resources/**&quot;).addResourceLocations(&quot;/public-resources/&quot;);&#125;//配置方式&lt;mvc:resources mapping=&quot;/resources/**&quot; location=&quot;/public-resources/&quot;/&gt; 消息转化器消息转化器的作用是将请求消息体和响应消息体的内容与对应的类型之间进行转换，配置样例如下: 12345678910111213141516171819202122//代码方式@Overridepublic void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; Jackson2ObjectMapperBuilder builder = new Jackson2ObjectMapperBuilder() .indentOutput(true) .dateFormat(new SimpleDateFormat(&quot;yyyy-MM-dd&quot;)) .modulesToInstall(new ParameterNamesModule()); converters.add(new MappingJackson2HttpMessageConverter(builder.build())); converters.add(new MappingJackson2XmlHttpMessageConverter(builder.xml().build()));&#125;//配置方式&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.springframework.http.converter.json.MappingJackson2HttpMessageConverter&quot;&gt; &lt;property name=&quot;objectMapper&quot; ref=&quot;objectMapper&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.http.converter.xml.MappingJackson2XmlHttpMessageConverter&quot;&gt; &lt;property name=&quot;objectMapper&quot; ref=&quot;xmlMapper&quot;/&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 参考文献官方文档：官方文档","categories":[],"tags":[{"name":"SpringMVC config","slug":"SpringMVC-config","permalink":"http://yoursite.com/tags/SpringMVC-config/"}]},{"title":"Spring学习(5)","slug":"spring-mvc-code-based-servlet-container-config","date":"2016-07-02T09:24:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/07/02/spring-mvc-code-based-servlet-container-config/","link":"","permalink":"http://yoursite.com/2016/07/02/spring-mvc-code-based-servlet-container-config/","excerpt":"","text":"基于代码的Servlet容器配置大部分情况下，使用Spring是采用基于配置的方式，通常情况下是通过web.xml文件对容器的行为进行配置. 从servlet3.0开始，spring提供了基于代码的配置，以下是个例子: 1234567891011121314public class MyWebApplicationInitializer implements WebApplicationInitializer &#123; @Override public void onStartup(ServletContext container) &#123; XmlWebApplicationContext appContext = new XmlWebApplicationContext(); appContext.setConfigLocation(&quot;/WEB-INF/spring/dispatcher-config.xml&quot;); ServletRegistration.Dynamic registration = container.addServlet(&quot;dispatcher&quot;, new DispatcherServlet(appContext)); registration.setLoadOnStartup(1); registration.addMapping(&quot;/&quot;); &#125;&#125; WebApplicationInitializer的实现会自动被检测到并且被用于初始化容器的配置. 除此之外, spring还提供了一种AbstractDispatcherServletInitializer抽象类来提供更简洁的方式对容器进行初始化, 这也是spring推荐的使用方式. 使用AbstractDispatcherServletInitializer进行初始化还可以分为使用代码和使用XML配置文件的方式： 注意：这里说的是初始化DispatcherServlet的配置，而上面说的是利用代码方式初始化Servlet容器，两者不要混淆. 123456789101112131415161718192021222324252627282930313233343536373839404142//使用代码方式配置public class MyWebAppInitializer extends AbstractAnnotationConfigDispatcherServletInitializer &#123; @Override protected Class&lt;?&gt;[] getRootConfigClasses() &#123; return null; &#125; @Override protected Class&lt;?&gt;[] getServletConfigClasses() &#123; return new Class[] &#123; MyWebConfig.class &#125;; &#125; @Override protected String[] getServletMappings() &#123; return new String[] &#123; &quot;/&quot; &#125;; &#125;&#125;//使用XML方式配置public class MyWebAppInitializer extends AbstractDispatcherServletInitializer &#123; @Override protected WebApplicationContext createRootApplicationContext() &#123; return null; &#125; @Override protected WebApplicationContext createServletApplicationContext() &#123; XmlWebApplicationContext cxt = new XmlWebApplicationContext(); cxt.setConfigLocation(&quot;/WEB-INF/spring/dispatcher-config.xml&quot;); return cxt; &#125; @Override protected String[] getServletMappings() &#123; return new String[] &#123; &quot;/&quot; &#125;; &#125;&#125; 另外，AbstractDispatcherServletInitializer抽象类还提供了关于filter，isAsyncSupport等配置的支持，具体可以参阅其文档，这里就不做过多阐述. 查阅AbstractDispatcherServletInitializer的源码可以发现，它是典型的模板模式的实现，将对WebApplicationInitializer的实现进行抽象，但是整个方法的实现过程没有太多变化. 12345678910111213141516171819202122232425262728293031323334public void onStartup(ServletContext servletContext) throws ServletException &#123; super.onStartup(servletContext); registerDispatcherServlet(servletContext);&#125;protected void registerDispatcherServlet(ServletContext servletContext) &#123; String servletName = getServletName(); Assert.hasLength(servletName, &quot;getServletName() may not return empty or null&quot;); WebApplicationContext servletAppContext = createServletApplicationContext(); Assert.notNull(servletAppContext, &quot;createServletApplicationContext() did not return an application &quot; + &quot;context for servlet [&quot; + servletName + &quot;]&quot;); DispatcherServlet dispatcherServlet = new DispatcherServlet(servletAppContext); ServletRegistration.Dynamic registration = servletContext.addServlet(servletName, dispatcherServlet); Assert.notNull(registration, &quot;Failed to register servlet with name &apos;&quot; + servletName + &quot;&apos;.&quot; + &quot;Check if there is another servlet registered under the same name.&quot;); registration.setLoadOnStartup(1); registration.addMapping(getServletMappings()); registration.setAsyncSupported(isAsyncSupported()); Filter[] filters = getServletFilters(); if (!ObjectUtils.isEmpty(filters)) &#123; for (Filter filter : filters) &#123; registerServletFilter(servletContext, filter); &#125; &#125; customizeRegistration(registration);&#125; 参考文献官方文档： 基于代码配置Servlet容器","categories":[],"tags":[{"name":"springMVC code-based container config","slug":"springMVC-code-based-container-config","permalink":"http://yoursite.com/tags/springMVC-code-based-container-config/"}]},{"title":"SpringMVC学习(4)","slug":"spring-mvc-fileUpload-exceptionHandle","date":"2016-06-29T10:54:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/29/spring-mvc-fileUpload-exceptionHandle/","link":"","permalink":"http://yoursite.com/2016/06/29/spring-mvc-fileUpload-exceptionHandle/","excerpt":"","text":"文件上传Spring中对于文件上传的支持是通过MultipartResolver接口来实现的，默认情况下，spring关于文件上传的支持是关闭的，如果需要，需要在上下文中配置相应的multipartResolver对象，并指定它的名字为“multipartResolver”. spring框架中自带了两种multipartResolver的实现， 配置的样例如下： CommonsMultipartResolver: 使用这个解析器需要引用o.s.web.multipart包 StandardServletMultipartResolver: spring内部提供的multipartResolver的标准实现 1234567&lt;bean id=&quot;multipartResolver&quot; class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;!-- one of the properties available; the maximum file size in bytes --&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;100000&quot;/&gt;&lt;/bean&gt; 在配置了multipartResolver之后，后续所有的请求都会被解析，如果判断出该请求是文件上传请求，则会调用multipartResolver的resolveMultipart方法，将request包装成MultipartHttpServletRequest. 在表单中使用multipartResolver在表单中使用multipartResolver分为两个步骤， 示例代码如下, 可以看出，在控制器的实现中基本没有什么变化，只是将原来的HttpServletRequest包装成了MultipartHttpServletRequest，这步是在请求进入DispatcherServlet的doDispatch方法时被调用的（后续会对DispatcherServlet的doDispatch方法做源码分析)： 指定表单的enctype为“multipart/form-data” 在控制器相应的处理方法中，通过MultipartFile或者MultipartHttpServletRequest获取相应的请求对象，并获取上传的文件 1234567891011121314151617181920212223242526272829//表单内容&lt;html&gt; &lt;body&gt; &lt;form action=&quot;/form&quot; enctype=&quot;multipart/form-data&quot; method=&quot;post&quot;&gt; &lt;input type=&quot;input&quot; name=&quot;name&quot; /&gt; &lt;input type=&quot;file&quot; name=&quot;file&quot; /&gt; &lt;input type=&quot;submit&quot; name=&quot;submit&quot; value=&quot;submit&quot;/&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt;//控制器实现@Controllerpublic class FileUploadController &#123; @PostMapping(&quot;/form&quot;) public String handleFormUpload(@RequestParam(&quot;name&quot;) String name, @RequestParam(&quot;file&quot;) MultipartFile file) &#123; if (!file.isEmpty()) &#123; byte[] bytes = file.getBytes(); // store the bytes somewhere return &quot;redirect:uploadSuccess&quot;; &#125; return &quot;redirect:uploadFailure&quot;; &#125;&#125; 在编码客户端中使用MultipartResolver通过代码向服务端上传文件时，绝大部分的逻辑和上述通过表单上传是一致的，但是通过代码上传文件时，可以一次性上传多个文件内容，并且上传的文件内容格式可以互不相同，下面的代码展示了这样的上传样例: 12345678910111213141516171819202122232425POST /someUrlContent-Type: multipart/mixed--edt7Tfrdusa7r3lNQc79vXuhIIMlatb7PQg7VpContent-Disposition: form-data; name=&quot;meta-data&quot;Content-Type: application/json; charset=UTF-8Content-Transfer-Encoding: 8bit&#123; &quot;name&quot;: &quot;value&quot;&#125;--edt7Tfrdusa7r3lNQc79vXuhIIMlatb7PQg7VpContent-Disposition: form-data; name=&quot;file-data&quot;; filename=&quot;file.properties&quot;Content-Type: text/xmlContent-Transfer-Encoding: 8bit... File Data ...//控制器的实现@PostMapping(&quot;/someUrl&quot;)public String onSubmit(@RequestPart(&quot;meta-data&quot;) MetaData metadata, @RequestPart(&quot;file-data&quot;) MultipartFile file) &#123; // ...&#125; 在这种情况下，可以通过@RequestPart或者@RequestParam来指定获取哪部分的数据，并且可以通过HttpMessageConverter自动完成相应的类型转换，其它的逻辑与表单完全一致. 异常处理在springMVC中，当请求在被处理的过程中，如果出现了异常情况，异常情况会交由HandlerExcetionResolver接口处理. 在SpringMVC框架中，可以通过三种方式进行异常处理: 实现HandlerExceptionResolver接口: 这个接口定义了resolveException方法来处理在发生异常的情况下返回的ModelAndView对象 使用SimpleMappingExceptionResolver实现: 这个实现提供了异常类型到视图类的映射关系 使用@ExceptionHandler注解: 这个注解的作用是当异常产生的时候，调用有这个注解的方法来处理异常. 它可以指定要处理的异常类型，默认为方法参数的异常类型. @ExceptionHandler可以只作用于某个Controller类中的方法，也可以通过ControllerAdvise注解作用于多个控制器类. 在需要使用多个HandlerExceptionResolver的场合，可以通过HandlerExceptionResolverComposite类进行组合. @ExceptionHandler注解在SpringMVC中，使用ExceptionHandlerExceptionResolver来处理@ExceptionHandler注解的异常信息. 它特别适合于不需要返回错误视图，只是简单地返回错误代码及错误说明的场景, 例如在RESTful接口的使用中，就可以使用这个注解对异常信息进行处理. 以下是示例代码， 当请求/exception地址时会抛出运行时异常，这个运行时异常会被@ExceptionHandler注解的方法处理，最终返回响应码为400，响应信息为“Bad Request.”: 123456789101112131415@RequestMapping@Controllerpublic class ExceptionHandlerController &#123; @RequestMapping(&quot;/exception&quot;) public String throwException() &#123; throw new RuntimeException(); &#125; @ExceptionHandler(value = RuntimeException.class) @ResponseBody public String handleException(HttpServletResponse response) &#123; response.setStatus(HttpServletResponse.SC_BAD_REQUEST); return &quot;Bad request.&quot;; &#125;&#125; 标准的spring异常在Spring进行处理的过程中，可能会产生许多类型的异常信息，使用SimpleMappingExceptionResolver可以将这些异常信息映射成相应的错误视图，但是有的时候可能希望将这些异常信息映射成相应的错误代码, spring中提供了DefaultHandlerExceptionResolver来实现这个功能. DefaultHandlerExceptionResolver默认被注册到spring中, 它当一些标准的spring异常类型转换成相应的错误代码，如BindException转化成400， HttpMediaTypeNotSupportedException转化成415等等，具体的对应关系可以查阅参考文献. @ResponseStatus在Spring中自定义异常类时，可以使用@ResponseStatus注解, 它可以指定相应的错误代码和错误信息，当处理器抛出这种类型的异常信息时，spring会自动将它转化成相应的错误代码和错误信息. 在spring内部，使用ResponseStatusExceptionResolver进行这个注解的处理，默认情况下spring就会注册这个解析器 除此之外，Servlet也支持将异常信息通过错误码和异常类型映射成相应的处理器进行处理. 参考文献官方文档: 官方文档","categories":[],"tags":[{"name":"SpringMVC fileUpload exceptionHandle","slug":"SpringMVC-fileUpload-exceptionHandle","permalink":"http://yoursite.com/tags/SpringMVC-fileUpload-exceptionHandle/"}]},{"title":"SpringMVC学习(3)","slug":"spring-mvc-handler-mapping","date":"2016-06-28T12:43:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/28/spring-mvc-handler-mapping/","link":"","permalink":"http://yoursite.com/2016/06/28/spring-mvc-handler-mapping/","excerpt":"","text":"处理器映射在前面关于SpringMVC处理流程的描述中提到，当用户向服务器发起请求时，spring将请求通过handlerMapping映射成具体的handler，并带上一系列的拦截器，后续的处理由该处理器和拦截器来完成. 在使用注解@RequestMapping的时候，spring会自动使用RequestMappingHandlerMapping这个类来进行处理，它会自动找出所有controller类中使用@RequestMapping注解的地方. 在springMVC中，所有继承自AbstractHandlerMapping的实现都有以下这些属性: 属性名 描述 interceptors 和当前handler关联的所有拦截器列表，请求会先由interceptor进行拦截处理，最后到达handler defaultHandler 默认的处理器，当没有匹配到合适的拦截器时使用 order 用于多个handlerMapping之间的排序时使用 alwaysUseFullPath 如果设置为true,那么在查找的时候会使用完整的路径进行匹配；否则只使用相对路径进行匹配. e.g. 如果某个servlet使用/testing/*， 这个值设置为true， 那么匹配的将是/testing/viewPage.html, 否则就是/viewPage.html 拦截器spring中使用拦截器机制在对请求进行处理之前执行一些特定的操作， 这些拦截器由HandlerInterceptor来定义， 这个接口定义了三个方法，如果想自定义拦截器，可以通过继承HandlerInterceptorAdapter类来实现： preHandler: 在请求到达处理器之前被调用，返回boolean类型的值，如果为true，意味着请求通过，交由后续的拦截器或处理器进行一步处理；如果为false，说明请求不通过，或响应已由本拦截器输出 postHandler: 在处理器返回相应的响应结果之后被调用 , 用于做一些后续的处理操作 afterCompletion: 在整个请求结束之后被调用 以下这段代码展示了如何配置interceptor信息： 1234567&lt;beans&gt; &lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerMapping&quot;&gt; &lt;property name=&quot;interceptors&quot;&gt; &lt;bean class=&quot;example.MyInterceptor&quot;/&gt; &lt;/property&gt; &lt;/bean&gt;&lt;beans&gt; 视图解析几乎所有的MVC框架都会提供相应的视图解析机制，而SpringMVC中使用ViewResolver和View类来完成相应的功能， 其中ViewResolver是用于将控制器返回的逻辑视图解析成相应的视图, 而View类则完成数据的准备和实际的渲染工作. SpringMVC中自带了一些视图解析器的实现, 以下是相应的说明，后续还给出了视图解析器的配置样例: 视图解析器名称 描述 AbstractCachingViewResolver 视图解析器的抽象实现，完成视图缓存的功能，所有需要缓存视图的视图解析器都可以从这个类派生 XmlViewResolver 由XML文件中的配置来定义逻辑视图到实际视图的对应关系，默认的xml文件位于/WEB-INF/views.xml中 UrlBasedViewResolver 基于URL路径的视图解析机制，如果实际的视图资源与逻辑视图之间有严格的对应关系，或者逻辑视图是实际视图的某个特定部分时，就可以使用这个解析器. 这个解析器还可以定义路径的前后缀信息 InternalResourceViewResolver 工具类，它是UrlBasedViewResolver和InternalResourceView的组合 VelocityViewResolver、FreeMarkerViewResolver 两者都是基于模板语言的视图解析器，都继承自UrlBasedViewResolver，对应的视图类分别是VelocityView和FreeMarkerView ContentNegotiatingViewResolver 这个解析器自己并不解析视图，而是将解析工作委托给配置好的视图解析器列表来进行解析，可以根据请求地址的后缀或者Accept头信息返回相应的视图内容 123456&lt;bean id=&quot;viewResolver&quot; class=&quot;org.springframework.web.servlet.view.UrlBasedViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot;/&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;&lt;/bean&gt; 解析器级联SpringMVC中支持将解析器级联，通过Order接口对各个解析器进行排序，当有请求进来时则对它一一进行解析. 以下的样例代码展示了同时配置多个视图解析器: 123456789101112131415&lt;bean id=&quot;jspViewResolver&quot; class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;viewClass&quot; value=&quot;org.springframework.web.servlet.view.JstlView&quot;/&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;excelViewResolver&quot; class=&quot;org.springframework.web.servlet.view.XmlViewResolver&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;property name=&quot;location&quot; value=&quot;/WEB-INF/views.xml&quot;/&gt;&lt;/bean&gt;&lt;!-- in views.xml --&gt;&lt;beans&gt; &lt;bean name=&quot;report&quot; class=&quot;org.springframework.example.ReportExcelView&quot;/&gt;&lt;/beans&gt; 重定向视图在视图解析器解析得到相应的视图类之后，由视图类进行后续的处理. 对于InternalResourceView来讲，内部使用RequestDispatcher进行相应的forward操作; 对于FreeMarkerView及VelocityView来讲，由它们直接输入相应的视图内容. 但是有时候可能会需要将视图重定向，比如经典的“Post-Redirect-Get”场景. 在Spring中提供了两种方式来进行重定向操作： 使用RedirectView: 这种方式直接由处理器返回RedirectView的实例，因此重定向的操作直接由这个类完成，但这种做法将重定向操作与控制器紧密耦合，即控制器感知到业务在进行重定向操作，不推荐 使用redirect:前缀: 这种做法在返回的逻辑视图前加上相应的前缀，对于控制器层而言，它和其它的逻辑视图一样；但对于UrlBasedViewResolver来讲，带有这个前缀的逻辑视图意味着要进行重定向操作. 在重定向过程中，可以RedirectAttribute将当前请求的一些属性传递给目标URL. 使用forward：前缀: 使用这个前缀时，UrlBasedViewResolver会在内部生成InternalResourceView，并把逻辑视图中除了该前缀外的其它部分包装到view中，然后调用RequestDispatcher.forward方法进行处理. ContentNegotiatingViewResolver（CNVR)正如前面所说的那样，CNVR自己并不进行视图解析工作，它把这部分工作委托给相应的ViewResolver进行. CNVR支持以下两种策略进行解析： 根据请求的后缀进行解析，比如fred.xml返回xml视图，而fred.pdf返回pdf视图 根据请求的Accept头信息, 但是这种方式在使用浏览器时比较局限，比如在使用FireFox浏览器时，它的Accept头总是被设置为Accept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8 CNVR获取视图的整个过程如下： CNVR中的每个视图解析器都提供了一些View对象，这些View对象都提供了相应的ContentType属性，这个属性表示该View类能提供的内容类型。在视图解析的第一个步骤，就是将请求的内容类型（Content-Type)，与这些view的ContentType进行匹配，由第一个满足条件的View来进行处理 如果没有视图解析器（关联的View类）能够提供与请求类型相兼容的类型，那么交由CNVR的defaultViews中提供的view进行匹配，由第一个满足匹配条件的view进行处理 以下给出了CNVR的配置样例， 当请求.html时，视图解析器就会根据text/html进行查找，InternalResourceViewResolver提供的View满足这个要求，因此就由这个解析器进行后续的处理; 当请求.atom类型时，视图解析器根据application/atom+xml类型进行查找，匹配到BeanNameViewResolver; 当请求的是.json类型时，视图解析器无法提供相应类型的内容，于是CNVR开始匹配defaultViews属性中匹配的view类，匹配到MappingJackson2JsonView. 123456789101112131415161718&lt;bean class=&quot;org.springframework.web.servlet.view.ContentNegotiatingViewResolver&quot;&gt; &lt;property name=&quot;viewResolvers&quot;&gt; &lt;list&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.BeanNameViewResolver&quot;/&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.InternalResourceViewResolver&quot;&gt; &lt;property name=&quot;prefix&quot; value=&quot;/WEB-INF/jsp/&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.jsp&quot;/&gt; &lt;/bean&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;defaultViews&quot;&gt; &lt;list&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.json.MappingJackson2JsonView&quot;/&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;content&quot; class=&quot;com.foo.samples.rest.SampleContentAtomView&quot;/&gt; 如果CNVR中没有配置相应的viewResolvers属性，那么它将使用上下文中配置的所有viewResolver. 参考文档 官方参考文档: 官方文档","categories":[],"tags":[{"name":"SpringMVC handlerMapping","slug":"SpringMVC-handlerMapping","permalink":"http://yoursite.com/tags/SpringMVC-handlerMapping/"}]},{"title":"SpringMVC学习(2)","slug":"spring-mvc-controller","date":"2016-06-21T13:00:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/21/spring-mvc-controller/","link":"","permalink":"http://yoursite.com/2016/06/21/spring-mvc-controller/","excerpt":"","text":"ControllerController提供了service接口中定义的业务的访问途径，它接受用户的输入，并将用户请求委托给service层进行处理，最终将处理结果以视图的形式返回给用户。从Spring2.5以来，提供了注解的方式来实现controller，这些注解包括@Controller, @RequestMapping, @RequestParam, @ModelAttribute，使用这些注解，程序中并不需要实现特定的接口，极大地方便了程序的实现。 @Controller和@RequestMapping@Controller注解意味着被注解类的角色为controller，即MVC中的“C”角色. 配合@RequestMapping注解，SpringMVC会自动扫描这些注解并将相应的请求地址映射到这些类和方法中，举个例子： 123456789101112131415161718192021222324252627@Controller@RequestMapping(\"/appointments\")public class AppointmentsController &#123; @RequestMapping(method = RequestMethod.GET) public Map&lt;String, Appointment&gt; get() &#123; return appointmentBook.getAppointmentsForToday(); &#125; @RequestMapping(path = \"/&#123;day&#125;\", method = RequestMethod.GET) public Map&lt;String, Appointment&gt; getForDay(@PathVariable @DateTimeFormat(iso=ISO.DATE) Date day, Model model) &#123; return appointmentBook.getAppointmentsForDay(day); &#125; @RequestMapping(path = \"/new\", method = RequestMethod.GET) public AppointmentForm getNewForm() &#123; return new AppointmentForm(); &#125; @RequestMapping(method = RequestMethod.POST) public String add(@Valid AppointmentForm appointment, BindingResult result) &#123; if (result.hasErrors()) &#123; return \"appointments/new\"; &#125; appointmentBook.addAppointment(appointment); return \"redirect:/appointments\"; &#125;&#125; 这个代码片断中多次使用了@RequestMapping注解. 第一次是对类进行注解，它意味着类中所有的处理方法路径都是相对于这个路径而言的，在这个例子中，也就是都是相对于/appointments路径. 第二个注解出现在get方法上，同时还增加了method参数，这意味着这个方法在原来类注解的基础上，增加了请求方法的限制，这里只能匹配get方法的请求. 第三个注解出现在getForDay中，它也只能匹配get方法，同时它还使用了URI模板，后续会对URI模板进行深入的探讨. 除此之外，getNewForm方法还对请求路径做了进一步的限制，它只匹配/appointments/new，并且只匹配get方法. 另外，类层级的@RequestMapping注解并不是必须的，如果类没有被它注解，意味着方法中所有的路径都是绝对路径，而不是相对路径. 在Spring3.1之前，@RequestMapping默认先由DefaultAnnotationHandlerMapping处理，再由DefaultAnnotationHandlerAdapter进一步缩小匹配的范围. 在Spring3.1以后，spring提供了新的实现，RequestMappingHandlerMapping和RequestMappingHandlerAdapter，处理器的选择由RequestMappingHandlerMapping直接完成. Spring4.3以后，对@RequestMapping提供了几个变体版本，针对不同的请求方法，提供了相应的mapping注解，比如@GetMapping, @PostMapping, @PutMapping等等，有需要可以查阅相关的文档，这里不做进一步详述. URI模板在使用RequestMapping注解的过程中，可以使用URI模板很方便地获取部分URL地址. URI模板中的变量可以通过@PathVariable注解来获取，如下所示, 当请求/owners/fred地址时，ownerId的值就会被设置成fred. 当@PathVariable注解被用于Map类型的变量时，所有的URI模板变量会自动被填充到map变量中: 123456@GetMapping(\"/owners/&#123;ownerId&#125;\")public String findOwner(@PathVariable String ownerId, Model model) &#123; Owner owner = ownerService.findOwner(ownerId); model.addAttribute(\"owner\", owner); return \"displayOwner\";&#125; 除了变量，还可以在URI模板中使用正则表达式. 具体的语法为{varName: regex}，其中第一部分为定义的变量，第二部分为正则表达式 如下所示，当请求/spring-web/spring-web-3.0.5.jar时，symbolicName会被设置成spring-web, version被映射成3.0.5，而extension被映射成.jar: 1234@RequestMapping(\"/spring-web/&#123;symbolicName:[a-z-]+&#125;-&#123;version:\\\\d\\\\.\\\\d\\\\.\\\\d&#125;&#123;extension:\\\\.[a-z]+&#125;\")public void handle(@PathVariable String version, @PathVariable String extension) &#123; // ...&#125; 当某个请求地址匹配多个URI模板时，匹配的规则按以下进行: 拥有较少URI变量和通配符的URI模板被优先匹配，比如/hotels/{hotel}/有1个变量和1个通配符，因此它比/hotels/{hotel}/*优先匹配 如果变量和通配符数量相等，路径长的被优先匹配，比如/foo/bar比/foo/优先匹配 如果变量和通配符数量相等且长度相等，拥有较长通配符个数的优先匹配， 比如/hotels/{hotel}比/hotels/*优先匹配 除此之外，还有两条例外: /**拥有最低的匹配次序 前置的通配符拥有更低的匹配次序，例如/public/path3/{a}/{b}/{c}比/public/**优先匹配 矩阵变量Spring还支持在URI请求中带上相应的变量，这些变量就被称为矩阵变量(matrix variable). 在spring中，矩阵变量可以有以下三种形式： /cars;color=red;year=2012 /cars;color=red,green,blue;year=2012 /cars;color=red;color=green;color=blue;year=2012 如果要在请求中访问矩阵变量的值，可以使用@MatrixVariable注解来获取. 这个注解还支持指定相应的变量名和矩阵变量名来进一步精确地获取其中的值. @RequestMapping注解方法@RequestMapping注解被用在方法上时，支持许多类型的输入参数，包括以下这些： 类型 描述 类型转换 Request和Response对象 可以选择特定的请求和响应类型，比如HttpServletRequest和HttpServletResponse N/A Session对象 HttpSession类型，表示会话对象 N/A InputStream, OutputStream, Reader, Writer 表示请求和响应的输入输出流对象 N/A HttpMethod 表示当前请求方法对象 N/A 注解参数 包括@PathVariable, @MatrixVariable, @RequestBody, @RequestParam, @RequestAttribute, @RequestHeader注解, 请求中的属性或参数都会被自动转化成注解所标注的参数类型 其中@RequestBody注解使用的是HttpMessageConverter接口来进行转换，其它的注解则由 WebDataBinder和Formatter来完成||HttpEntity|包括请求参数和请求头信息|类型转换由HttpMessageConverter完成||Model, Map, ModelMap|增强视图的模型内容|N/A| 支持的返回类型包括： 类型 描述 类型转换 ModelAndView 包括返回的模型和视图对象 N/A Model, Map 返回的模型对象，视图类由默认的RequestToViewNameTranslator来进行解析得到 N/A View 返回的视图对象，模型对象可以由参数中的Model类型的参数来提供 N/A String 返回的逻辑视图，可以由相应的ViewResolver接口解析得到实际的视图 N/A void 这种情况下，响应的内容由程序直接通过Response对象输出，或者由RequestToViewNameTranslator解析得到 N/A HttpEntity 包括响应头和响应消息体的内容 消息体的内容由HttpMessageConverter完成类型转换 HttpHeaders 表示当前响应不包含消息体，只有响应头 N/A @RequestBody和@ResponceBody@RequestBody和@ResponceBody分别代表了请求和响应的消息体，当参数被RequestBody注解，或者响应的对象被ResonseBody注解时，Spring会自动根据convesionService的配置转化成相应的对象或消息体. 事实上，在spring中消息体到对象或者由对象到响应消息体之间的转换全部由HttpMessageConverter完成，包括RequestBody、ResponseBody以及HttpEntity中的消息体. RequestMappingHandlerAdapter自动注册了以下几个converters,如果需要可以自行定义和配置converter ByteArrayHttpMessageConverter StringHttpMessageConverter FormHttpMessageConverter SourceHttpMessageConverter @RestController当需要在spring中实现RESTful类型的接口时，可以使用@RestController. 它是@Controller和@ResponseBody两个注解的组合，可以很方便地实现RESTful风格的接口 方法参数与类型转换请求中的参数都是字符串类型的，当它们转换成其它类型时，就需要使用类型转换，这些参数包括请求参数，请求头，cookie信息以及path variable变量. 在Spring中，提供了WebDataBinder来实现相应的类型转换. 通过注册Formatter接口的实现来自定义类型转换的过程. 类型转换的过程可以通过两种方式进行： 使用@InitBinder注解 使用WebBindingInitializer @InitBinder注解使用@InitBinder注解可以将方法定义为类型转换的实现，它定义了将请求参数转换成相应类型的具体方法，通过registerCustomEditor或者addCustomFormatter来实现注册. 它接受的参数同@RequestMapping基本相同，而且不能有返回值. 这种方式只在声明@InitBinder的controller类中有用. 123456789101112131415@Controllerpublic class MyFormController &#123; @InitBinder protected void initBinder(WebDataBinder binder) &#123; SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); dateFormat.setLenient(false); binder.registerCustomEditor(Date.class, new CustomDateEditor(dateFormat, false)); //spring4.2后支持的方式 binder.addCustomFormatter(new DateFormatter(&quot;yyyy-MM-dd&quot;)); &#125; // ...&#125; WebBindingInitializer通过WebBindingInitializer可以将类型转换的配置放到RequestMappingAdapter， 通过修改默认的配置自定义类型转换的过程. 123456&lt;bean class=&quot;org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter&quot;&gt; &lt;property name=&quot;cacheSeconds&quot; value=&quot;0&quot;/&gt; &lt;property name=&quot;webBindingInitializer&quot;&gt; &lt;bean class=&quot;org.springframework.samples.petclinic.web.ClinicBindingInitializer&quot;/&gt; &lt;/property&gt;&lt;/bean&gt; 异步请求处理从Servlet3.0规范开始就定义了异步请求处理的方法. Spring框架也从3.2开始支持3.0规范. 在Spring中，可以用两种方式来实现对请求的异步处理： Callable: 请求到达服务器后，服务器直接返回callable对象，并且servlet的处理线程退出，从而可以接着处理后续进来的请求。后续的处理由threadPoolTaskExecutor中的线程接管，当工作线程处理完成后，结果通过callable回调给servlet容器的线程，再由这个线程返回到客户端. DeferredResult： 整个处理的过程与Callable基本一致，唯一不同的是，Callable的结果由容器管理的工作线程完成后交还给servlet容器响应. 但deferredResult的响应结果则由其它的任意线程进行设置，通过它的setResult方法，任何方法都可以将处理的结果设置到deferredResult中 12345678910111213141516171819202122232425262728293031@RequestMapping(&quot;query&quot;)@ResponseBodypublic Callable&lt;Person&gt; asyn() &#123; return () -&gt; &#123; //延迟两秒响应 Thread.sleep(2000); return buildPerson(); &#125;;&#125;@RequestMapping(&quot;queryDefered&quot;)@ResponseBodypublic DeferredResult&lt;Person&gt; asynDeferredResult() &#123; DeferredResult&lt;Person&gt; deferredResult = new DeferredResult&lt;&gt;(5000L, buildPerson(&quot;Essviv&quot;)); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; deferredResult.setResult(buildPerson()); &#125; &#125;).start(); return deferredResult;&#125; 异步处理流程异步请求处理的整个过程包括： 控制器返回callable或者deferredResult SpringMVC开始在工作线程中处理请求，对于Callable，工作线程由容器管理；对于deferredResult，工作线程由任意线程实现 DispatcherServer及所有的过滤器线程，但是response仍然打开着，等待进一步响应 Callable返回相应的响应并把响应重新分发给dispatchServlet DispatcherServlet被重新唤起，并将callable的内容输出到response中 在异步处理的时候，如果出现异常，异常处理的方式与同步处理表现一致. 在DeferredResult中，也可以通过setErrorResult来设置异常信息 配置方法那么，在spring中如何配置异步处理请求呢? 简单来讲，分为两步： 设置servlet的版本为3.0 在servlet及相应的filter配置中加上true 在controller中返回callable或者deferredResult 参考文献Controller的官方文档： 官方文档","categories":[],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"}]},{"title":"SpringMVC学习(1)","slug":"spring-introduction","date":"2016-06-21T13:00:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/21/spring-introduction/","link":"","permalink":"http://yoursite.com/2016/06/21/spring-introduction/","excerpt":"","text":"Spring MVCSpringMVC是Spring框架中自带的MVC实现，它的实现是围绕DispatcherServlet来展开的（后续会对DispatcherServlet的源码进行解读)。DispatcherServlet是一种前端控制器的实现，所有的请求进行一些预处理后，通过它分发给相应的处理器，处理器处理后的结果再返回给前端控制器，由它派发给相应的视图解析器，最后完成响应. SpringMVC中默认的处理器是由@Controller和@RequestMapping标签来实现的. Spring3.0之后，还增加了对RESTful风格的请求的支持. DispatcherServlet正如前方所说的那样，DispatcherServlet是一种前端控制器的实现，它的结构如图所示, 同时它也是标准的JavaEE的servlet实现. 在SpringMVC中，ApplicationContext是有作用域范围的， 每一个DispatcherServlet都有它的webApplicationContext，这些webApplicationContext都继承了root webApplicationContext中定义的所有bean. 通常情况下, rootApplicationContext中包含了基础的bean定义，这些bean在所有的servlet以及context中共享, rootApplicationContext通常由web.xml中的contextConfigLocation定义. DispatcherServlet中特殊的Bean对象在DispatcherServlet中定义了很多特殊类型的对象用来完成处理请求、渲染视图等操作，并且SpringMVC中还定义一系列默认实现，这些默认实现都配置在org.springframework.web.servlet包下的DispatcherServlet.properties文件中. 一旦程序中配置了其中某种类型的实现，这些默认的实现都将会被替代. 下表给出了DispatcherServlet依赖的一些bean类型，这些bean类型也构成了SpringMVC的处理框架, 如下图所示. 类型 描述 HandlerMapping HandlerMapping将请求映射成相应的处理器和一系列的前置、后置处理器 HandlerAdapter HandlerAdapter将各种各样的Handler适配成统一的接口供DispatcherServlet调用，它是一种适配器模式的应用 HandlerExceptionResolver 它将处理过程中的异常映射成视图，以允许一些更精确的异常处理 ViewResolver 它将逻辑视图转化成实际的视图类型","categories":[],"tags":[{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"}]},{"title":"XStream学习(3)","slug":"xstream-converter","date":"2016-06-15T13:24:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/15/xstream-converter/","link":"","permalink":"http://yoursite.com/2016/06/15/xstream-converter/","excerpt":"","text":"转换器我们从最简单的例子开始吧,创建一个简单的对象，设置别名，然后输出： 12345678910111213141516171819202122package com.thoughtworks.xstream.examples;public class Person &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125;//设置别名xStream.alias(&quot;person&quot;, Person.class);//输出结果&lt;person&gt; &lt;name&gt;Guilherme&lt;/name&gt;&lt;/person&gt; 创建转换器接下来我们要开始创建Person类的转换器，转换器要实现Converter接口. 这个转换器必须要完成以下三个功能： 支持Person类的转换 将Person类转换成XML（序列化） 将XML转换成Person（反序列化） 以下是Person类转换器的代码，在序列化的过程中，可以使用startNode/endNode来声明新的节点;在反序列化的过程中，可以使用moveDown/moveUp在结点树中遍历；在创建完转换器的代码后，可以将它注册到XStream中，并观察它的输出: 123456789101112131415161718192021222324252627282930313233343536373839package com.thoughtworks.xstream.examples;import com.thoughtworks.xstream.converters.Converter;import com.thoughtworks.xstream.converters.MarshallingContext;import com.thoughtworks.xstream.converters.UnmarshallingContext;import com.thoughtworks.xstream.io.HierarchicalStreamReader;import com.thoughtworks.xstream.io.HierarchicalStreamWriter;public class PersonConverter implements Converter &#123; public boolean canConvert(Class clazz) &#123; return clazz.equals(Person.class); &#125; public void marshal(Object value, HierarchicalStreamWriter writer, MarshallingContext context) &#123; Person person = (Person) value; writer.startNode(&quot;fullname&quot;); writer.setValue(person.getName()); writer.endNode(); &#125; public Object unmarshal(HierarchicalStreamReader reader, UnmarshallingContext context) &#123; Person person = new Person(); reader.moveDown(); person.setName(reader.getValue()); reader.moveUp(); return person; &#125;&#125;//注册转换器xStream.registerConverter(new PersonConverter());//输出结果&lt;person&gt; &lt;fullname&gt;Guilherme&lt;/fullname&gt;&lt;/person&gt; 另一种转换器如果只是想把某个对象转化成字符串，那么有一种简单的实现方式, 实现AbstractSingleValueConverter接口. 1234567891011121314151617181920package com.thoughtworks.xstream.examples;import com.thoughtworks.xstream.converters.basic.AbstractSingleValueConverter;public class PersonConverter extends AbstractSingleValueConverter &#123; public boolean canConvert(Class clazz) &#123; return clazz.equals(Person.class); &#125; public Object fromString(String str) &#123; Person person = new Person(); person.setName(string); return person; &#125;&#125;//输出结果&lt;person&gt;Guilherme&lt;/person&gt; 参考文献XStream的官方转换器文档: 官方文档","categories":[],"tags":[{"name":"XStream 转换器","slug":"XStream-转换器","permalink":"http://yoursite.com/tags/XStream-转换器/"}]},{"title":"XStream学习(2)","slug":"xstream-annotation","date":"2016-06-14T13:15:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/14/xstream-annotation/","link":"","permalink":"http://yoursite.com/2016/06/14/xstream-annotation/","excerpt":"","text":"注解在上一章中，我们学习了如何使用XStream的别名机制来更好的控制输出，但是有时候这种控制略显烦琐，因此XStream也提供了相应的注解支持. 本章主要展示了如何使用XStream提供的注解机制来更简单有效地控制相应的输出. 首先我们先定义消息模式，并输出最基本的XML格式： 1234567891011121314151617181920212223242526package com.thoughtworks.xstream;public class RendezvousMessage &#123; private int messageType; public RendezvousMessage(int messageType) &#123; this.messageType = messageType; &#125;&#125;//输出代码package com.thoughtworks.xstream;public class Tutorial &#123; public static void main(String[] args) &#123; XStream stream = new XStream(); RendezvousMessage msg = new RendezvousMessage(15); System.out.println(stream.toXML(msg)); &#125;&#125;//输出结果&lt;com.thoughtworks.xstream.RendezvousMessage&gt; &lt;messageType&gt;15&lt;/messageType&gt;&lt;/com.thoughtworks.xstream.RendezvousMessage&gt; 类别名和属性别名注解XStream提供了@XStreamAlias注解，它同时提供了alias方法和aliasField方法提供的功能，但XStream并不会自动读取这个注解，必须通过processAnnotation方法显式地指定XStream读取这个注解. 12345678910111213141516171819202122@XStreamAlias(&quot;message&quot;)class RendezvousMessage &#123; @XStreamAlias(&quot;type&quot;) private int messageType; public RendezvousMessage(int messageType) &#123; this.messageType = messageType; &#125;&#125;//输出代码public static void main(String[] args) &#123; XStream stream = new XStream(); xstream.processAnnotations(RendezvousMessage.class); RendezvousMessage msg = new RendezvousMessage(15); System.out.println(stream.toXML(msg)); &#125;//输出结果&lt;message&gt; &lt;type&gt;15&lt;/type&gt;&lt;/message&gt; 隐式集合注解现在让我们来试试如何使用注解完成隐式集合的功能，首先为消息模式添加列表属性，最终得到的输出结果为: 12345678910111213141516171819202122232425@XStreamAlias(&quot;message&quot;)class RendezvousMessage &#123; @XStreamAlias(&quot;type&quot;) private int messageType; private List&lt;String&gt; content; public RendezvousMessage(int messageType, String ... content) &#123; this.messageType = messageType; this.content = Arrays.asList(content); &#125; &#125;//输出结果&lt;message&gt; &lt;type&gt;15&lt;/type&gt; &lt;content class=&quot;java.util.Arrays$ArrayList&quot;&gt; &lt;a class=&quot;string-array&quot;&gt; &lt;string&gt;firstPart&lt;/string&gt; &lt;string&gt;secondPart&lt;/string&gt; &lt;/a&gt; &lt;/content&gt;&lt;/message&gt; 这显然不是我们想要的输出结果，XStream提供了@XStreamImplicit注解来提供和addImplicitCollection对应的功能.在加上这个注解后，输出的结果变成了: 12345678910111213141516171819202122@XStreamAlias(&quot;message&quot;)class RendezvousMessage &#123; @XStreamAlias(&quot;type&quot;) private int messageType; @XStreamImplicit private List&lt;String&gt; content; public RendezvousMessage(int messageType, String... content) &#123; this.messageType = messageType; this.content = Arrays.asList(content); &#125;&#125;//输出结果&lt;message&gt; &lt;type&gt;15&lt;/type&gt; &lt;a class=&quot;string-array&quot;&gt; &lt;string&gt;firstPart&lt;/string&gt; &lt;string&gt;secondPart&lt;/string&gt; &lt;/a&gt;&lt;/message&gt; 离期望的输出近了一点，但还不是最终的样子，我们期望那个’a‘不要出现，同时期望能够给每个元素取个名字，比如part. XStream为XStreamImplicit注解了相应的属性设置，itemFieldName可以用来设计集合中每个元素的名称，将它设置为part后的输出结果为: 12345678910111213141516171819202122@XStreamAlias(&quot;message&quot;)class RendezvousMessage &#123; @XStreamAlias(&quot;type&quot;) private int messageType; @XStreamImplicit(itemFieldName=&quot;part&quot;) private List&lt;String&gt; content; public RendezvousMessage(int messageType, String... content) &#123; this.messageType = messageType; this.content = Arrays.asList(content); &#125;&#125;//输出结果&lt;message&gt; &lt;type&gt;15&lt;/type&gt; &lt;part&gt;firstPart&lt;/part&gt; &lt;part&gt;secondPart&lt;/part&gt;&lt;/message&gt; 变换器注解首先我们先为消息模型增加个时间字段，并查看下输出的结果: 123456789101112131415161718192021222324@XStreamAlias(&quot;message&quot;)class RendezvousMessage &#123; @XStreamAlias(&quot;type&quot;) private int messageType; @XStreamImplicit(itemFieldName=&quot;part&quot;) private List&lt;String&gt; content; private Date time; public RendezvousMessage(int messageType, String... content) &#123; this.messageType = messageType; this.content = Arrays.asList(content); &#125;&#125;//输出结果&lt;message&gt; &lt;type&gt;15&lt;/type&gt; &lt;part&gt;firstPart&lt;/part&gt; &lt;part&gt;secondPart&lt;/part&gt; &lt;time&gt;2016-06-14 14:02:08.305 UTC&lt;/time&gt;&lt;/message&gt; 如果我们希望将time属性输出成系统的毫秒数该怎么办呢？XStream为我们提供了@XStreamConverter注解来解决这个问题，它的作用是自定义某个类的转化器.另外，@XStreamConverter注解还可以通过注解的属性给相应的转化器输入构造函数，具体可以查看注解的javadoc说明.以下的例子首先定义了Date类型的转化器DateConverter，并设置time使用该转化器，最后的输出结果为: 12345678910111213141516171819202122232425262728293031323334public class DateConverter implements Converter &#123; @Override public void marshal(Object source, HierarchicalStreamWriter writer, MarshallingContext context) &#123; Date date = (Date) source; writer.setValue(String.valueOf(date.getTime())); &#125; @Override public Object unmarshal(HierarchicalStreamReader reader, UnmarshallingContext context) &#123; String value = null; reader.moveDown(); value = reader.getValue(); reader.moveUp(); return new Date(NumberUtils.toLong(value)); &#125; @Override public boolean canConvert(Class type) &#123; return type.equals(Date.class); &#125;&#125;@XStreamConverter(DateConverter.class)private Date time;//输出结果&lt;message&gt; &lt;type&gt;15&lt;/type&gt; &lt;part&gt;firstPart&lt;/part&gt; &lt;part&gt;secondPart&lt;/part&gt; &lt;time&gt;1465913336042&lt;/time&gt;&lt;/message&gt; 属性注解在有些场景下，可能希望将对象的某个字段显示成结点的属性而不是子结点，XStream提供了@XStreamAsAttribute来实现属性注解的功能. 1234567@XStreamAlias(&quot;type&quot;)@XStreamAsAttributeprivate int messageType;//输出结果&lt;message type=&quot;15&quot;&gt;&lt;/message&gt; 在另一些场景下，可能希望将对象的某个字段作为结点的值,而将其它全部字段作为结点的属性输出. XStream提供了ToAttributedValueConverter类来实现这个功能，它指定某个字段作为结点的值，其它字段作为属性输出但是，这里有个前提，由于结点的属性只能是字符类型，因此那些作为属性输出的字段必须能够转化成字符类型. 如果某个字段不能完成这种转换，必须通过显式指定SingleValueConverter接口实现. 1234567891011121314151617181920212223242526@XStreamAlias(&quot;message&quot;)@XStreamConverter(value=ToAttributedValueConverter.class, strings=&#123;&quot;content&quot;&#125;)class RendezvousMessage &#123; @XStreamAlias(&quot;type&quot;) private int messageType; //这里的list并不能自动转化成字符类型，因此必须显式地指定转化器，否则输出会报错 @XStreamConverter(SomeImplementOfSingleValueConverter.class) private List&lt;String&gt; content; @XStreamConverter(value=BooleanConverter.class, booleans=&#123;false&#125;, strings=&#123;&quot;yes&quot;, &quot;no&quot;&#125;) private boolean important; @XStreamConverter(SingleValueCalendarConverter.class) private Calendar created = new GregorianCalendar(); public RendezvousMessage(int messageType, boolean important, String... content) &#123; this.messageType = messageType; this.important = important; this.content = Arrays.asList(content); &#125;&#125;//输出&lt;message type=&quot;15&quot; important=&quot;no&quot; created=&quot;1154097812245&quot;&gt;This is the message content.&lt;/message&gt; 忽略字段有些时候，对象的部分属性并不需要输出到XML中，这时候可以通过XStream提供的@XStreamOmitField注解来完成忽略输出的功能. 自动检测注解在上面的描述中我们提到，在设置了类的注解后，调用代码中必须显式地调用processAnnotation方法来通知XStream来处理这些注解，这种处理有时候略显麻烦. XStream还提供了自动检测注解的方法autodetectAnnotations. 但是，如果调用了processAnnotation方法，那么自动检测的功能就自动被关闭了. 另外，XStream自动检测还有一些弊端，具体可以查阅“参考文献”. 参考文献XStream官方注解文档: 注解文档","categories":[],"tags":[{"name":"XStream 注解","slug":"XStream-注解","permalink":"http://yoursite.com/tags/XStream-注解/"}]},{"title":"XStream学习(1)","slug":"xstream-alias","date":"2016-06-14T13:15:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/06/14/xstream-alias/","link":"","permalink":"http://yoursite.com/2016/06/14/xstream-alias/","excerpt":"","text":"XStream概览XStream是用来将对象与XML进行互相转换的第三方库. 它的主要特点包括： 简单易用. 无需映射. 性能. 简洁的XML. … 错误信息的输出. 支持与JSON格式的转换. 别名假设我们定义了以下的XML格式, 需要通过XStream进行读写： 123456789101112&lt;blog author=&quot;Guilherme Silveira&quot;&gt; &lt;entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/entry&gt; &lt;entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/entry&gt;&lt;/blog&gt; 首先，我们需要根据XML格式定义以下两个对象模型: 12345678910111213141516171819202122232425262728293031323334353637383940package com.thoughtworks.xstream;public class Blog &#123; private Author writer; private List entries = new ArrayList(); public Blog(Author writer) &#123; this.writer = writer; &#125; public void add(Entry entry) &#123; entries.add(entry); &#125; public List getContent() &#123; return entries; &#125;&#125;package com.thoughtworks.xstream;public class Author &#123; private String name; public Author(String name) &#123; this.name = name; &#125; public String getName() &#123; return name; &#125;&#125;package com.thoughtworks.xstream;public class Entry &#123; private String title, description; public Entry(String title, String description) &#123; this.title = title; this.description = description; &#125;&#125; 接着就可以通过XStream来进行XML格式的输出： 12345678910public static void main(String[] args) &#123; Blog teamBlog = new Blog(new Author(&quot;Guilherme Silveira&quot;)); teamBlog.add(new Entry(&quot;first&quot;,&quot;My first blog entry.&quot;)); teamBlog.add(new Entry(&quot;tutorial&quot;, &quot;Today we have developed a nice alias tutorial. Tell your friends! NOW!&quot;)); XStream xstream = new XStream(); System.out.println(xstream.toXML(teamBlog));&#125; 最终我们得到的输出是这样的, 似乎看上去和我们想要的结果有点不一样： 1234567891011121314151617&lt;com.thoughtworks.xstream.Blog&gt; &lt;writer&gt; &lt;name&gt;Guilherme Silveira&lt;/name&gt; &lt;/writer&gt; &lt;entries&gt; &lt;com.thoughtworks.xstream.Entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/com.thoughtworks.xstream.Entry&gt; &lt;com.thoughtworks.xstream.Entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/com.thoughtworks.xstream.Entry&gt; &lt;/entries&gt;&lt;/com.thoughtworks.xstream.Blog&gt; 类别名首先我们需要把类似于’com.thoughtworks.xstream.Blog’这样的包名给去掉，XStream给我们提供了alias方法来实现类的别名功能: 12xstream.alias(&quot;blog&quot;, Blog.class);xstream.alias(&quot;entry&quot;, Entry.class); 现在再看来看看我们的输出, 现在它变成了以下这样，离我们期望的结果似乎近了一点： 1234567891011121314151617&lt;blog&gt; &lt;writer&gt; &lt;name&gt;Guilherme Silveira&lt;/name&gt; &lt;/writer&gt; &lt;entries&gt; &lt;entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/entry&gt; &lt;entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/entry&gt; &lt;/entries&gt;&lt;/blog&gt; 属性别名现在我们希望把writer属性改成author，同样地，XStream提供了aliasField来实现类属性的别名设置, 现在的输出变成了： 12345678910111213141516171819xstream.aliasField(&quot;author&quot;, Blog.class, &quot;writer&quot;);&lt;blog&gt; &lt;author&gt; &lt;name&gt;Guilherme Silveira&lt;/name&gt; &lt;/author&gt; &lt;entries&gt; &lt;entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/entry&gt; &lt;entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/entry&gt; &lt;/entries&gt;&lt;/blog&gt; 隐式集合再来看看entries的输出，XStream引入了“隐式集合”的概念，当遇到不需要显示集合根结点的情况时，可以将它映射成隐式集合.在上述的例子中，entries是个列表，默认输出时，会将entries作为集合的根结点，有时候这种输出并不是我们期望的，那么如何将它去掉呢？XStream中提供了addImplicitCollection方法来解决这个问题， 再次输出的结果为： 1234567891011121314151617xstream.addImplicitCollection(Blog.class, &quot;entries&quot;);&lt;blog&gt; &lt;author&gt; &lt;name&gt;Guilherme Silveira&lt;/name&gt; &lt;/author&gt; &lt;entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/entry&gt; &lt;entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/entry&gt;&lt;/blog&gt; 属性别名有时候我们可能会希望将类的某个字段输出成结点的属性，而不是它的子结点，比如在上述的例子中，可以将writer字段输出成属性. XStream提供了useAttributeFor方法将类的字段作为结点属性输出 12stream.useAttributeFor(Blog.class, &quot;writer&quot;);xstream.aliasField(&quot;author&quot;, Blog.class, &quot;writer&quot;); 但是，如果要将writer输出成结点的属性，还需要完成一个转换. 那就是Author类如何转换成String值，因为结点的属性值只能是字符类型. XStream提供了SingleValueConverter接口来实现类与字符类型之间的转换功能. 因此只要能提供一个converter能将author转化成字符类型，就可以顺利将writer字段作为结点的属性输出. 定义完转化器之后，将它注册到XStream中，就可以得到相应的输出: 12345678910111213141516171819202122232425262728293031class AuthorConverter implements SingleValueConverter &#123; public String toString(Object obj) &#123; return ((Author) obj).getName(); &#125; public Object fromString(String name) &#123; return new Author(name); &#125; public boolean canConvert(Class type) &#123; return type.equals(Author.class); &#125;&#125;//将新定义的转化器注册到XStream中xstream.registerConverter(new AuthorConverter());//输出&lt;blog author=&quot;Guilherme Silveira&quot;&gt; &lt;entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/entry&gt; &lt;entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/entry&gt;&lt;/blog&gt; 包名别名有的时候我们可以希望为包名设置别名，XStream提供了aliasPackage来设置包名的别名 1234567891011121314151617181920xstream.aliasPackage(&quot;my.company&quot;, &quot;org.thoughtworks&quot;);//输出&lt;my.company.xstream.Blog&gt; &lt;author&gt; &lt;name&gt;Guilherme Silveira&lt;/name&gt; &lt;/author&gt; &lt;entries&gt; &lt;my.company.xstream.Entry&gt; &lt;title&gt;first&lt;/title&gt; &lt;description&gt;My first blog entry.&lt;/description&gt; &lt;/my.company.xstream.Entry&gt; &lt;my.company.xstream.Entry&gt; &lt;title&gt;tutorial&lt;/title&gt; &lt;description&gt; Today we have developed a nice alias tutorial. Tell your friends! NOW! &lt;/description&gt; &lt;/my.company.xstream.Entry&gt; &lt;/entries&gt;&lt;/my.company.xstream.Blog&gt; 参考文献XStream别名官方文档: 别名","categories":[],"tags":[{"name":"XStream 别名","slug":"XStream-别名","permalink":"http://yoursite.com/tags/XStream-别名/"}]},{"title":"培训","slug":"培训","date":"2016-05-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/25/培训/","link":"","permalink":"http://yoursite.com/2016/05/25/培训/","excerpt":"","text":"培训数据库开发单一的数据源或集群的代理异构数据源： hibernate, spring-data(for redis, hbase) 提供统一的API 动态生成操作 优化性能 异步并行加载框架 热点缓存 恶意访问的拦截、值班表扫描的拦截 按需加载 淘宝技术魔方技术架构 数据源层（DB， 日志） —&gt; 计算层（hadoop集群+hdfs，云梯； nosql集群，银河）—-&gt; 存储层 —-&gt; 查询层 —-&gt; 服务层 TimeTunnel: 读日志增量的工具 DataX: 异构数据源之间导入导出数据的工具（构架+插件） 数据库的优化（开发人员） DB的优化原理 优化器模块 执行计划（oracle会对这部分进行缓存，mysql不会对这部分缓存) 学会查看执行计划和性能损耗 innodb_buffer_pool_size对性能有决定性的影响 查询优化 索引的利弊 索引的限制： 索引的计算、函数、数据类型转换均会导致索引失效. 重量级的关键字的执行原理 join order by group by:先排序再分组 distinct: 先分组再从每组中取一条记录 表机制 innodb行锁是通过事务来实现的 行锁升级为表锁的情况: 索引失效、间隙锁、死锁 mysql的新技术 源码安装使用ICC编译器 HandlerSocket（绕过mysql的优化器模块) percona: percona数据库， xtrdb存储引擎（可完全替代innodb）， xtrbackup（可完全替代hotbackup） 数据库的表设计 范式与非范式结合使用: 适当冗余数据避免join, 常用字段与不常用字段垂直拆分(大字段)缓存架构(b/s结构的项目中有哪些缓存可以做) 页面缓存： OSCache, EHCache, 页面静态化(IO, xml+xslt) 页面局部缓存: 标签类的技术， OSCache, EHCache, ESI(标准, JAVA的实现称为JESI） 浏览器的缓存 优缺点(慎用) html5(最被看好) 更丰富的标签、表单 canvas webStorage webSocket 反射代理服务器的缓存 七层的软件负载均衡器: nginx（通过配置可直接缓存） nginx(专做负载均衡) + squid(专做缓存) 数据库的前端缓存 热点数据的缓存 热点规则: 支持可配置 key规则： 一级缓存、二级缓存 数据失效： 绝对失效、相对失效、事件失效（消息中间件, JMS） 经常反复创建并销毁的对象 享元模式: 建议使用软引用. 前端的一些缓存: jquery中的data方法 分布式的缓存服务器 Redis 提供了更丰富的数据类型: list, set, hashes, string, hset(不仅仅是cache， 更多的是服务端的计算) 支持持久化: rdb持久化、aof持久化、无持久化（吞吐量最高） rdb持久化: 快照文件 持久化所需要的空间 恢复需要多长时间 对QPS的影响 aof持久化: 日志文件 支持主备机制: master-slaver master做读写，slave做持久化 master做写，slave做读 master宕机，slave接管（keepalived） 消息中间件(非企业级): pubsub 在线扩容、容量规划等内容 memcached 内存存储：内存要求高，CPU要求低 集中式缓存：多个实例之间不通讯，存在单点问题 SOCKET通讯，NIO编程： 网络带宽、Socket的连接数都是制约性能的因素 内存分配: 全部预先分配，不存在内存碎片，不需要回收； slab, chunk, page(每次分配的内存大小，分配完内存后会按一定的大小划分成slab） MemcachedClient官方 淘宝memcached: xplatform asf google的MSM(memcached session manager) 新浪memcachedb(memcached的持久化) GemFile BKD服务层 webService(http+xml) RPC(tcp/ip) 基础的RPC操作 性能优化 io模型( 同步阻塞IO， 同步非阻塞IO， 多路复用IO(nio, 事件机制)， 异步IO) 线程模型 netty, mina 服务的透明化：服务框架中间件(dubbo) 服务的治理： 自动注册与治理、配置、升降级 —-&gt; 微服务 dubbo(dubboX) + 其它第三方开源项目, spring-cloud(更完整), spring-boot总结 B/S架构的目标 架构师的理论体系 非功能性需求（从需求到架构） 构架的的总体分层（前端、后台） 数据的优化( java, jvm, tomcat…) 合理的缓存架构","categories":[],"tags":[]},{"title":"markdown","slug":"doc/markdown","date":"2016-05-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/25/doc/markdown/","link":"","permalink":"http://yoursite.com/2016/05/25/doc/markdown/","excerpt":"","text":"#Markdown ##段落和换行 段落前后要有一个以上的空行 换行可以通过在行尾添加两个以上的空格来实现 这是一个新的段落 这个地方不会出现断行 但这个地方会出现断行 ##标题 Setext: ===和— atx: #号 ##引用引用可以通过“&gt;”来实现 这是个引用的段落这还是个引用的段落 ##列表列表可以通过加号、星号和减号来实现 加号选项1 加号选项2 星号选项1 星号选项2 星号选项3 加号选项3 减号选项1 减号选项2 减号选项3 ##代码代码可以通过反单引号(`)来实现 public static void main(String[] args) throws IOException { final String markdownFilename = &quot;/markdown.txt&quot;; final String htmlFilename = &quot;C:\\\\Users\\\\Lenovo\\\\Desktop\\\\markdown.html&quot;; File file = new File(Markdown.class.getResource(markdownFilename).getFile()); String html = new PegDownProcessor(Extensions.ALL).markdownToHtml(FileUtils.readFileToString(file)); FileUtils.writeStringToFile(new File(htmlFilename), html); System.out.println(&quot;OK&quot;); } ##分隔线分隔线可以通过在一行中超过三个以上的星号，减号或底线来完成 下面会有一条横线， 是用星号画出来的 下面也会有一条横线，是用底线画出来的 下面还会有一条横线，是用减号画出来的##链接 [an example](hyperlink)百度 谷歌 [an example][id][id]: hyperlink百度 谷歌 ##图片图片引用的格式： ![alt text](path) ##强调强调可以通过在文字前后加星号或者下划线来完成 某某很帅某某真的很帅","categories":[],"tags":[]},{"title":"JAXB","slug":"SOA/JAXB","date":"2016-05-23T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/23/SOA/JAXB/","link":"","permalink":"http://yoursite.com/2016/05/23/SOA/JAXB/","excerpt":"","text":"JAXB架构 标签 @XmlRootElement: 根元素 @XmlElement: 用于setter方法 @XmlAttribute: 表示将这个字段作为元素的属性 @XmlType: propOrder属性的名称 @XmlJavaTypeAdapter: 用于复杂的java类型，或者当Jaxb输出的格式不是理想的输出时，就可以通过自定义adapter的方式定义输出的内容与格式 @XmlAccessorType: 指定用于生成XML的元素有哪些，默认是public_member，可以指定的包括field, property, public_member, none. 代码 JaxbContext: 提供了jaxb的API入口，不管是marshaller和unmarshaller，均可以由它产生 Marshaller/Unmarshaller: 可以认为是编码与解码器，由jaxbContext生成，可以设置相应的属性（如格式化输出等）,并且提供了由java对象与xml表达之间的转换操作. XmlAdapter: 这个接口提供了java类与xml格式输出的自定义输出接口，marshal与unmarshal分别定义了输入输出.XmlAdapter接口定义了在内存中如何通过valueType来表示boundType. 参考文献 https://www.javacodegeeks.com/2015/04/%E7%94%A8%E4%BA%8Ejava%E5%92%8Cxml%E7%BB%91%E5%AE%9A%E7%9A%84jaxb%E6%95%99%E7%A8%8B.html 示例代码 https://github.com/Essviv/spring/tree/master/src/main/java/com/cmcc/syw/jaxb","categories":[],"tags":[]},{"title":"RabbitMQ的安装","slug":"消息队列/rabbitMQ/RabbitMQ的安装","date":"2016-05-17T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/17/消息队列/rabbitMQ/RabbitMQ的安装/","link":"","permalink":"http://yoursite.com/2016/05/17/消息队列/rabbitMQ/RabbitMQ的安装/","excerpt":"","text":"RabbitMQ的安装 安装RMQ后，需要启用后台管理插件 rabbitmq-plugins list: 查看插件列表 rabbitmq-plugins enable plugins-name: 启用某个插件 rabbitmq-plugins disable plugins-name: 禁用某个插件 后台管理默认的端口为15672， 默认的用户名为guest, 密码为guest， 并且它具有所有的操作权限 可以通过rabbmitmqctl命令来操作用户，并给指定的用户赋相应的角色和权限","categories":[],"tags":[]},{"title":"JS基础语法","slug":"前端/JS基础语法","date":"2016-05-13T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/13/前端/JS基础语法/","link":"","permalink":"http://yoursite.com/2016/05/13/前端/JS基础语法/","excerpt":"","text":"JS基础语法 区分大小写 数据类型 Number: 不区分整形和浮点数，统一用Number来表示 字符串： 可以用双引号(“”)或者单引号(‘’)来表示字符串 可以通过’\\’进行转义操作 操作： toLowerCase, toUpperCase, 字符操作， indexOf(查找第一个出现的位置）， subString 布尔类型： true or false， 布尔运算包括&amp;&amp;， ||， ！ null和undefined： null表示空值， 而undefined表示未定义的值 数组： 可以通过[]或者Array对象来生成 操作： indexOf, slice, concat, join, push/pop, shift/unshift, sort, reverse 对象: JS中的对象就是一组“键-值”对，因此它并不是严格的面向对象的语言 使用JSON格式来定义对象 可以动态的增加或修改属性的值 可以通过delete obj.prop操作来删除相应的属性 可以通过 prop in obj 来判断某个对象是否有某个属性，但这个判断会将判断继承的属性，如果只想判断它本身的属性，需要使用hasOwnProperty方法 变量 JS中的变量是通过”var”来定义的, 如果在声明变量时，没有使用var关键字，那么声明的变量默认就是全局变量，这是一种很不好的声明习惯. 可以在JS中声明使用strict模式来避免这个问题 流程控制 条件判断: if/else if/else 循环： for, for-in, while, do-while 在循环时，可以对对象或者数组（这种情况下，数组的索引值被当作属性来看） Map/Set reference: http://www.liaoxuefeng.com/wiki/001434446689867b27157e896e74d51a89c25cc8b43bdb3000/00143450082788640f82a480be8481a8ce8272951a40970000练习地址： http://www.w3resource.com/javascript-exercises/ 函数 arguments： 函数调用者实际传入的参数列表，可以比声明的参数多，也可以比声明的参数少，类型也可以不同 变量声明前置： JS中会把变量的声明全部前置到函数的开始，也就是说，它允许变量的使用在声明之前，这个特性会导致函数中表现出奇怪的行为 变量的作用域： 不在任何函数中声明的变量就属于全局变量，全局变量默认默认被绑定到“window”的属性上; 没有定义作用域的函数都属于全局变量， 因此也可以通过window.functionName()来进行调用 方法： 给JS对象绑定了某些函数，就称为这个对象的方法，通过对象调用的方法，内部维护了一个this对象（这是JS中理解函数的难点） apply&amp;call: apply方法是通过函数对象来进行调用，它接收两个参数，第一个参数是指this指针的指向，第二个参数是个array， 表示函数的参数列表;call的第二个参数不是数组 闭包： 这个概念相当重要，也比较晦涩，后续会专门对它进行理解，简单来讲就是返回的函数中带有函数内部的状态（或者说变量值），即闭包实现了从函数外部可以访问到函数内部的状态值 闭包是指在 JavaScript 中，内部函数总是可以访问其所在的外部函数中声明的参数和变量，即使在其外部函数被返回（寿命终结）了之后。 标准对象 Date RegExp /正则表达式/ new RegExp(‘正则表达式’) test：判断正则表达式是否匹配split: 使用正则表达式可以更灵活exec: 通过()分组 JSON 序列化： stringify 反序列化： JSON.parse(); 面向对象编程这章需要单独学习 JS中的面向对象编程是通过原型链(prototype)来实现的，它并不区分类和实例的概念 在查找对象的属性时，会先找当前对象的属性，如果没有，就在它的原型中查找，如果还没有，就一直沿着原型链找到Object.prototype为止 obj.constructor == Student.prototype.constructorobj.constructor.protytype == Student.prototypeobj instanceOf Student new关键字 创建一个空的对象{} 设置空对象的constructor属性 对象继承constructor.prototype的所有属性，这里不是简单地复制，而是代理，意味着后续如果修改了constructor.prototype的属性，已生成的对象也会受影响 原型链的继承 浏览器对象 window: 表示全局域对象，以及浏览器窗口 navigator: 表示浏览器的信息 screen: 代表屏幕 location: 代表当前页面的URL地址 document: 表示当前的页面，DOM的根结点，它还有个属性cookie，是用来存储cookie信息的 history: 浏览器的历史记录，可以通过back和forward跳转 DOM操作在HTML中，node包括element, comment, CDATA_SECTION以及document类型 获取： getElementById, getElementsByTagName, getElementsByClassName， 注意，根据ID查询的是单个节点，根据tag和class名字查询得到的是一组节点 更新： innerHTML 或者innerText 增加/删除: append, insert(before, after) 参考文献： 文献1","categories":[],"tags":[]},{"title":"JAVA-IO学习","slug":"java-io-model","date":"2016-05-07T07:44:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/07/java-io-model/","link":"","permalink":"http://yoursite.com/2016/05/07/java-io-model/","excerpt":"","text":"IO相关概念 阻塞：在发起IO操作之后，线程被阻塞，直到相应的IO操作完成才会返回 非阻塞： 在发起IO操作之后，线程不会被阻塞并且立即返回 同步： 在发起IO操作之后，在没有得到结果之前，调用都不会返回（注意，这里不返回不代表就一定阻塞了，应用也可以处于非阻塞状态），调用一旦返回了，IO操作的结果也就得到了。换句话说，就是由调用者（或应用）主动等待IO操作的结果， Reactor模式就属性这种模式 异步： 在发起IO操作之后，应用程序直接返回，并不等待IO操作的结果，而是由被调用者（通常是系统）在IO操作完成后，通过通知、回调等方式告知应用程序。 Proactor就属性这种模式 从上面的定义可以看出，同步和异步的区别在于IO的调用方是否需要主动地等待数据，在同步操作中，应用需要主动将数据从系统内核空间拷贝到应用空间，并且在这个过程中会出现block状态；而异步操作中，应用调用完IO操作后，就可以执行其它的操作了，系统在将数据拷贝到应用空间完成后，通过回调和通知等方式告知应用，应用再开始对这些数据进行相应的处理. IO模型IO模型大体上可分为以下五类： 阻塞式IO(BIO) 非阻塞式IO(NIO) 多路复用 信息驱动（不常用，略） 异步IO(AIO) 它们之间的比较： IO模型图: 感觉这里的IO多路复用更应该属于“同步阻塞”，但不知道为什么这里被划分为异步阻塞 一个IO操作其实分成了两个步骤：发起IO请求和实际的IO操作。同步IO和异步IO的区别就在于第二个步骤是否阻塞，如果实际的IO读写阻塞请求进程，那么就是同步IO。阻塞IO和非阻塞IO的区别在于第一步，发起IO请求是否会被阻塞，如果阻塞直到完成那么就是传统的阻塞IO，如果不阻塞，那么就是非阻塞IO select，poll，epoll都是IO多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 参考文献 概念比较1： IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇） 概念比较2： 大话同步/异步、阻塞/非阻塞 AIO简介： AIO简介 BIO, NIO和AIO的理解： BIO, NIO和AIO的理解","categories":[],"tags":[{"name":"java AIO BIO NIO","slug":"java-AIO-BIO-NIO","permalink":"http://yoursite.com/tags/java-AIO-BIO-NIO/"}]},{"title":"JAVA-IO学习","slug":"java-nio-2","date":"2016-05-07T05:40:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/07/java-nio-2/","link":"","permalink":"http://yoursite.com/2016/05/07/java-nio-2/","excerpt":"","text":"buffer1.属性: capacity, position, limit, mark capacity: 缓冲区的容量大小 position: 当前缓冲区的游标位置，也就是下一个读或写的位置 limit: 缓冲区读写的上限位置，代表了可读写的最多数据量，在写模式，这个值一般和capacity一样 mark: 缓冲区的标记位置，mark()方法将缓冲区当前的游标位置记录下来，后续可以通过reset()方法将游标重置到这个位置上 2.分类 缓冲区的分类比较简洁，基本上可以按数据类型进行划分，比如ByteBuffer, CharBuffer, DoubleBuffer, IntBuffer等等，但是有一个是特别值得注意的， MappedByteBuffer. 具体的使用场景和方法可以参见TODO. 3.操作 put/get: 两个方法都有两种变体. 一种是基于相对位置的（即不带参数的）,这种方式会改变buffer当前的游标位置，另一种是基于绝对位置的，这种方式不会改变游标的当前位置. allocate, wrap： 获取buffer的两种方式，一种是直接分配相应大小的缓冲区，另一种是将现有的数组进行包装，注意，这种情况下，wrap得到的缓冲区当中的数组即为传给wrap函数的数组，意思就是如果修改了这个数组的值，那么buffer的值也就被修改了. 反之亦然. flip, rewind, clear: 这些操作都将position的值置为0， flip将limit设置成当前position的值，所以它一般用来从写模式切换成读模式; rewind将position的值置为0，但它不会改变limit； clear将position置为0，并且将limit置为capacity，因此它经常用来从读模式切换回写模式. remaining, hasRemaining: 获取和判断从当前位置到缓冲区上界的元素数量. compact: 回收已经读取过的缓冲区数据，保留未被读取的缓冲区数据，也就是保留position~limit之间的数据，将它们复制到数组开始部分，并将position置于下一个能够读写的位置上，将limit置于capacity的位置. mark/reset: 标记和恢复当前游标的位置 4.视图缓冲区 视图缓冲区和原来的缓冲区共享数据元素. duplicate: 复制一份缓冲区，两者拥有独立的position, limit, capacity属性，但共享同一个数组，也就是说，对其中一个缓冲区的修改会反映到另一个缓冲区中 asReadOnlyBuffer: 创建缓冲区的只读视图，除了不能执行写操作，以及readOnly返回true之外，其它的都和原来的缓冲区保持一致. 5.字节顺序 big-endian: 大端字节序，最高字节（most significant byte)存储于低位地址，最低字节（least significant byte)存储于高位地址 little-endian: 小端字节序，最高字节存储于高位地址，最低字节存储于低位地址 channel channel–&gt;buffer, buffer–&gt;channel 多路复用的概念 channel与stream的不同： channel是双向的，而stream是单向的 channel可以是异步的 channel读写的目标都是buffer channel可以通过Files等对象获取得到 channel的类型可以分为两大类： 一类为FileChannel， 一类为SocketChannel, SocketChannel又进一步细分为SocketChannel, ServerSocketChannel, DatagramChannel selector NIO: channel &amp; buffer NIO: non-blocking read &amp; write selector: monitor multiple channels selectionKey: SelectableChannel注册到selector时，将返回相应的selectionKey，它是两者间关系的管理工具 SelectableChannel: 可以被注册到selector中的一种通道，所有的socket通道都属于这种类型， 相反，fileChannel不属于这种类型. 参考文献 JAVA NIO： JAVA NIO O’Reilly%20-%20O’Reilly%20-%20Java%20NIO.pdf) JAVA NIO Turtorial: Jenkov NIO Turtorial","categories":[],"tags":[{"name":"java nio selector","slug":"java-nio-selector","permalink":"http://yoursite.com/tags/java-nio-selector/"}]},{"title":"JAVA-IO学习之NIO.2(一)","slug":"IO/java基础/JAVA-IO学习之NIO.2(一)","date":"2016-05-07T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/07/IO/java基础/JAVA-IO学习之NIO.2(一)/","link":"","permalink":"http://yoursite.com/2016/05/07/IO/java基础/JAVA-IO学习之NIO.2(一)/","excerpt":"","text":"JAVA-IO学习之NIO.2(一）buffer属性 capacity: 缓冲区的容量大小 position: 当前缓冲区的游标位置，也就是下一个读或写的位置 limit: 缓冲区读写的上限位置，代表了可读写的最多数据量，在写模式，这个值一般和capacity一样 mark: 缓冲区的标记位置，mark()方法将缓冲区当前的游标位置记录下来，后续可以通过reset()方法将游标重置到这个位置上 分类缓冲区的分类比较简洁，基本上可以按数据类型进行划分，比如ByteBuffer, CharBuffer, DoubleBuffer, IntBuffer等等，但是有一个是特别值得注意的， MappedByteBuffer. 具体的使用场景和方法可以参见TODO. 操作 put/get: 两个方法都有两种变体. 一种是基于相对位置的（即不带参数的）,这种方式会改变buffer当前的游标位置，另一种是基于绝对位置的，这种方式不会改变游标的当前位置. allocate, wrap： 获取buffer的两种方式，一种是直接分配相应大小的缓冲区，另一种是将现有的数组进行包装，注意，这种情况下，wrap得到的缓冲区当中的数组即为传给wrap函数的数组，意思就是如果修改了这个数组的值，那么buffer的值也就被修改了. 反之亦然. flip, rewind, clear: 这些操作都将position的值置为0， flip将limit设置成当前position的值，所以它一般用来从写模式切换成读模式; rewind将position的值置为0，但它不会改变limit； clear将position置为0，并且将limit置为capacity，因此它经常用来从读模式切换回写模式. remaining, hasRemaining: 获取和判断从当前位置到缓冲区上界的元素数量. compact: 回收已经读取过的缓冲区数据，保留未被读取的缓冲区数据，也就是保留position~limit之间的数据，将它们复制到数组开始部分，并将position置于下一个能够读写的位置上，将limit置于capacity的位置. mark/reset: 标记和恢复当前游标的位置 视图缓冲区视图缓冲区和原来的缓冲区共享数据元素. duplicate: 复制一份缓冲区，两者拥有独立的position, limit, capacity属性，但共享同一个数组，也就是说，对其中一个缓冲区的修改会反映到另一个缓冲区中 asReadOnlyBuffer: 创建缓冲区的只读视图，除了不能执行写操作，以及readOnly返回true之外，其它的都和原来的缓冲区保持一致. 字节顺序 big-endian: 大端字节序，最高字节（most significant byte)存储于低位地址，最低字节（least significant byte)存储于高位地址 little-endian: 小端字节序，最高字节存储于高位地址，最低字节存储于低位地址 channel channel–&gt;buffer, buffer–&gt;channel 多路复用的概念 channel与stream的不同： channel是双向的，而stream是单向的 channel可以是异步的 channel读写的目标都是buffer channel可以通过Files等对象获取得到 channel的类型可以分为两大类： 一类为FileChannel， 一类为SocketChannel, SocketChannel又进一步细分为SocketChannel, ServerSocketChannel, DatagramChannel selector NIO: channel &amp; buffer NIO: non-blocking read &amp; write selector: monitor multiple channels selectionKey: SelectableChannel注册到selector时，将返回相应的selectionKey，它是两者间关系的管理工具 SelectableChannel: 可以被注册到selector中的一种通道，所有的socket通道都属于这种类型， 相反，fileChannel不属于这种类型. 参考文献 JAVA NIO： JAVA NIO O’Reilly%20-%20O’Reilly%20-%20Java%20NIO.pdf) JAVA NIO Turtorial: Jenkov NIO Turtorial","categories":[],"tags":[]},{"title":"自勉","slug":"quota","date":"2016-05-04T10:00:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/04/quota/","link":"","permalink":"http://yoursite.com/2016/05/04/quota/","excerpt":"","text":"When nothing seems to help, I go look at a stonecutter hammering away at his rock perhaps a hundred times without as much as a crack showing in it. Yet at the hundred and first blow it will split into two, and I know it was not that blow that did it, but all that had gone before.","categories":[],"tags":[]},{"title":"JAVA-IO学习之NIO","slug":"IO/java基础/JAVA-IO学习之NIO","date":"2016-05-01T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/01/IO/java基础/JAVA-IO学习之NIO/","link":"","permalink":"http://yoursite.com/2016/05/01/IO/java基础/JAVA-IO学习之NIO/","excerpt":"","text":"JAVA-IO学习之NIONIOJAVA的NIO部分相对来讲比较简单，它把过去那些旧的IO实现进行了新的封装，将与文件相关的操作封装到以下几个类中： Paths: 用于获取Path对象的工具类，可以通过指定文件名，URI等方式来获取Path对象 Path: 文件的语法表示，也就是说，这个对象只是个纯JAVA意义上的路径，它包含了和路径相关的一些操作，比如获取文件名，相对路径转换和解析等操作，但这些操作绝大部分和实际的文件没有关系（除了toRealPath), 具体的操作可以参见javadoc Files: 这是个工具类，它封装了与目录及文件相关的所有操作，它不但提供了文件的增删改查操作，还提供了相应的属性操作以及创建临时文件等操作，除此之外，它还提供了五种读取文件的方式，见下图，从左到右，操作的复杂度逐步上升. 另外，Files类还提供了关于文件夹的操作，包括创建、删除、遍历、临时目录等操作， 具体可以参见javaDoc 最后，JAVA的nio框架还增加了文件夹的监听服务，通过将某个要监视的目录注册到WatchService中，就可以实现对这个目录的增删改操作的监视功能，在整个监听服务中，有几个比较重要的接口： WatchService: 监听服务的核心接口，它用来提供相应的监听服务，所有实现了Watchable接口的类都可以通过这个接口进行注册监听 WatchKey: 注册目录的监听服务后，系统会返回相应的watchKey, 每个watchKey都有三种状态，刚注册完后处于ready状态，当有事件发生时，状态更改为signale， 当被关闭或者取消时，它的状态更改为invalid. 注意：当处理完接收到的事件时，必须将watchKey通过reset方法重新置于ready的状态, 否则它不能继续接收相应的事件. WatchEvent: 代表了监听的事件，每个事件都包括相应的类别信息，上下文信息，以及个数信息 官方文档中推荐使用WatchService的方法如下： 12345678910111213141516171819202122232425Path dir = ...;try &#123; WatchKey key = dir.register(watcher, ENTRY_CREATE, ENTRY_DELETE, ENTRY_MODIFY);&#125; catch (IOException x) &#123; System.err.println(x);&#125; for (;;) &#123; // retrieve key WatchKey key = watcher.take(); // process events for (WatchEvent&lt;?&gt; event: key.pollEvents()) &#123; : &#125; // reset the key boolean valid = key.reset(); if (!valid) &#123; // object no longer registered &#125; &#125; Glob在JAVA的NIO操作中，有些需要用到glob表达式，这里简单地罗列下glob语法的意义： *: match any number of characters(including none) **: works like * but cross directory boundry ?: match exactly one character {sun, moon, star}: collection of subpattern, match sun, moon or star. [0-9, aoei]: convey a set of single character, and when hyphen is used, a range of characters NOTE: use \\ to escape special character 参考文献官方文档：官方文档 示例代码：GitHub","categories":[],"tags":[]},{"title":"JAVA-IO学习","slug":"java-io-nio","date":"2016-05-01T01:48:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/05/01/java-io-nio/","link":"","permalink":"http://yoursite.com/2016/05/01/java-io-nio/","excerpt":"","text":"NIOJAVA的NIO部分相对来讲比较简单，它把过去那些旧的IO实现进行了新的封装，将与文件相关的操作封装到以下几个类中： Paths: 用于获取Path对象的工具类，可以通过指定文件名，URI等方式来获取Path对象 Path: 文件的语法表示，也就是说，这个对象只是个纯JAVA意义上的路径，它包含了和路径相关的一些操作，比如获取文件名，相对路径转换和解析等操作，但这些操作绝大部分和实际的文件没有关系（除了toRealPath), 具体的操作可以参见javadoc Files: 这是个工具类，它封装了与目录及文件相关的所有操作，它不但提供了文件的增删改查操作，还提供了相应的属性操作以及创建临时文件等操作，除此之外，它还提供了五种读取文件的方式，见下图，从左到右，操作的复杂度逐步上升. 另外，Files类还提供了关于文件夹的操作，包括创建、删除、遍历、临时目录等操作， 具体可以参见javaDoc 最后，JAVA的nio框架还增加了文件夹的监听服务，通过将某个要监视的目录注册到WatchService中，就可以实现对这个目录的增删改操作的监视功能，在整个监听服务中，有几个比较重要的接口： WatchService: 监听服务的核心接口，它用来提供相应的监听服务，所有实现了Watchable接口的类都可以通过这个接口进行注册监听 WatchKey: 注册目录的监听服务后，系统会返回相应的watchKey, 每个watchKey都有三种状态，刚注册完后处于ready状态，当有事件发生时，状态更改为signale， 当被关闭或者取消时，它的状态更改为invalid. 注意：当处理完接收到的事件时，必须将watchKey通过reset方法重新置于ready的状态, 否则它不能继续接收相应的事件. WatchEvent: 代表了监听的事件，每个事件都包括相应的类别信息，上下文信息，以及个数信息 官方文档中推荐使用WatchService的方法如下： 12345678910111213141516171819202122232425Path dir = ...;try &#123; WatchKey key = dir.register(watcher, ENTRY_CREATE, ENTRY_DELETE, ENTRY_MODIFY);&#125; catch (IOException x) &#123; System.err.println(x);&#125; for (;;) &#123; // retrieve key WatchKey key = watcher.take(); // process events for (WatchEvent&lt;?&gt; event: key.pollEvents()) &#123; : &#125; // reset the key boolean valid = key.reset(); if (!valid) &#123; // object no longer registered &#125; &#125; Glob在JAVA的NIO操作中，有些需要用到glob表达式，这里简单地罗列下glob语法的意义： *: match any number of characters(including none) **: works like * but cross directory boundry ?: match exactly one character {sun, moon, star}: collection of subpattern, match sun, moon or star. [0-9, aoei]: convey a set of single character, and when hyphen is used, a range of characters NOTE: use \\ to escape special character 参考文献官方文档：官方文档 示例代码：GitHub","categories":[],"tags":[{"name":"java nio","slug":"java-nio","permalink":"http://yoursite.com/tags/java-nio/"}]},{"title":"对称加密、非对称加密及数字证书","slug":"encrption","date":"2016-04-28T13:04:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/28/encrption/","link":"","permalink":"http://yoursite.com/2016/04/28/encrption/","excerpt":"","text":"对称加密对称加密算法是指加密和解密过程使用同一个密钥的算法，或者加密密钥可以由解密密钥计算得到（反过来也一样，解密密钥也可以由加密密钥计算得到），对称加密算法计算量小，因此效率更高，但它的缺点是密钥的管理困难，特别是当需要进行数据传输的参与者很多的时候，密钥的管理将是一个噩梦. 非对称加密顾名思义，非对称加密就是指加密和解密过程使用的不是同一个密钥的算法. 因此，非对称加密的密钥都是以密钥对的形式出现的，发送给别人的密钥称为是公钥(public key), 自己保留的密钥称为是私钥(private key). 根据加密时使用的密钥的不同可以将非对称加密的使用场景分为两种： 加密传输： 如果A想将消息发送给B，但不想被其它人看到，那么A可以使用B的公钥对这段消息进行加密，因为只有B持有他自己的私钥，因此这段被加密过的密文只能由B通过它的私钥解密得到，保证了消息只能由B看到 签名：所谓签名的意思就是某个人声称这段消息是由他发出的，比如说，如果A想向B发送一封邮件，同时他想表明这封邮件是由他发出的，那么A就可以使用他的私钥对这封邮件的内容进行签名，然后将这段密文发给B，B用A的公钥对这段密文进行解密，如果解密成功，说明这封邮件确实是由A发出的. 当然，由于非对称加密算法的效率不高，对一大段内容进行加密可能需要花费比较多的时间，因此在实际的操作中，会先对这段内容通过摘要算法计算得到相应的摘要，再用私钥对这个摘要信息进行加密，从而形成签名，这也是数字签名中实现的内容. 数字签名在通过网络传输信息时，首先使用哈希算法计算得到明文信息的摘要(digest), 然后使用私钥对这段摘要进行加密，并把这段密文和要传输的明文信息一起传输给接收者，这个过程就称为数字签名. 从签名的过程可以看出，它主要包括两个步骤： 计算明文信息的摘要 使用私钥对摘要进行加密 当接收者收到相应的内容和签名信息时，他要做的就是验证签名，首先利用相同的摘要算法对明文信息计算得到摘要信息，再用发送者的公钥解密签名信息得到另一个摘要信息，比较计算得到和解密得到的摘要信息是否一致，以此判断这个签名是由发送者签注的。整个验签的过程也分为三个过程： 计算明文信息的摘要信息 使用公钥对接收内容中的签名进行解密 比较步骤1和2中的摘要信息是否一致 数字证书数字签名解决了消息来源的可靠性问题，但这基于一个前提，就是消息的接收者所持有的公钥确实是属于消息发送者的，如果这个公钥在不知情的情况下被替换了，那么还是存在收到冒充者发送的消息的可能性. 数字证书就是用于解决这个问题的. 现在面临的问题是接收者无法确定自己持有的公钥是否真的来自发送者，数字证书的解决方法就是引入可信任的第三方，首先这个第三方必须是可信任的，再由这个可信任的第三方为发送者的公钥做“担保”，那么这个公钥就是可信任的. 具体的步骤是这样的： 消息发送者将公钥及其它的信息发送给可信任的第三方(CA) CA组织在进行必须的校验后，对发送者给它的消息及公钥进行数字签名，形成数字证书颁发给发送者 发送者将数字证书发给接收者 接收者拿到数字证书后，首先对CA的数字签名进行验签，验签通过后就可以获取到发送者的公钥，这里的公钥就可以认为是由CA组织担保过的，是可信任的 那么问题来了，接收者怎么确定数字证书中的签名所使用的公钥就是可信任的呢？这里就引入了“证书链”的概念，按照上述的思路一步步地进行上溯，直到找到根证书. 根证书就是自签名或者根本没有签名的证书，安装某个根证书就意味着信任来自这个根证书，也就意味着信息由这个根证书签名过的所有证书. 在实际的操作中，浏览器通常会和某些根证书组织合作，安装浏览器的时候会自动安装这些根证书. HTTPS理解了对称加密、非对称加密以及数字证书的机理后，再来看HTTPS就比较简单了. 简单来讲，HTTPS就是在应用层和传输层之间加了SSL/TLS层，在服务端和客户端建立会话前，必须先建立安全通道，之后两者之间所有的数据传输都必须通过这个安全通道来进行. 在建立会话时，服务端和客户端通过非对称加密算法协商密钥，协商完密钥后，后续所有的数据传输都使用这个密钥通过对称加密完成. 整个过程被称为“四次握手”，如下所述： 客户端向服务端发送client_random以及客户端支持的对称加密算法列表 服务端收到客户端的请求后，向客户端发送server_random，选定的对称加密算法，以及服务端的数字证书 客户端首先验证数字证书的真实性，验证通过后，就使用数字证书中的公钥对premaster_num(客户端随机产生)进行加密传输 服务端收到加密的消息后，使用私钥进行解密，得到客户端传来的premaster_num, 并通过client_random, server_random和premaster_num计算得到对称加密中要使用的密钥，到这里为止，服务端和客户端完成了密钥的协商. 在后续的数据传输中，服务端都将使用步骤2中的对称加密算法，以及步骤4中计算得到的密钥对要传输的数据进行加密 客户端同样可以根据client_random, server_random以及premaster_num计算得到对称加密算法的密钥，并用它对服务端发来的加密数据进行解密，并在发送数据给服务端前进行加密 可以看到，在密钥协商的过程中，使用的是非对称加密的方法，最大限度地保证了密钥的随机性和保密性，而在数据传输的时候，使用的是则是对称加密，可以有效地提高加密解密的效率 参考文献 HTTPS详解： HTTPS详解 数字证书的基础知识: 数字证书的基础知识 What’s a digital signature: What’s a digital signature","categories":[],"tags":[{"name":"对称加密 非对称加密","slug":"对称加密-非对称加密","permalink":"http://yoursite.com/tags/对称加密-非对称加密/"}]},{"title":"对称加密、非对称加密及数字证书","slug":"安全/对称加密、非对称加密及数字证书","date":"2016-04-28T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/28/安全/对称加密、非对称加密及数字证书/","link":"","permalink":"http://yoursite.com/2016/04/28/安全/对称加密、非对称加密及数字证书/","excerpt":"","text":"对称加密、非对称加密及数字证书对称加密对称加密算法是指加密和解密过程使用同一个密钥的算法，或者加密密钥可以由解密密钥计算得到（反过来也一样，解密密钥也可以由加密密钥计算得到），对称加密算法计算量小，因此效率更高，但它的缺点是密钥的管理困难，特别是当需要进行数据传输的参与者很多的时候，密钥的管理将是一个噩梦. 非对称加密顾名思义，非对称加密就是指加密和解密过程使用的不是同一个密钥的算法. 因此，非对称加密的密钥都是以密钥对的形式出现的，发送给别人的密钥称为是公钥(public key), 自己保留的密钥称为是私钥(private key). 根据加密时使用的密钥的不同可以将非对称加密的使用场景分为两种： 加密传输： 如果A想将消息发送给B，但不想被其它人看到，那么A可以使用B的公钥对这段消息进行加密，因为只有B持有他自己的私钥，因此这段被加密过的密文只能由B通过它的私钥解密得到，保证了消息只能由B看到 签名：所谓签名的意思就是某个人声称这段消息是由他发出的，比如说，如果A想向B发送一封邮件，同时他想表明这封邮件是由他发出的，那么A就可以使用他的私钥对这封邮件的内容进行签名，然后将这段密文发给B，B用A的公钥对这段密文进行解密，如果解密成功，说明这封邮件确实是由A发出的. 当然，由于非对称加密算法的效率不高，对一大段内容进行加密可能需要花费比较多的时间，因此在实际的操作中，会先对这段内容通过摘要算法计算得到相应的摘要，再用私钥对这个摘要信息进行加密，从而形成签名，这也是数字签名中实现的内容. 数字签名在通过网络传输信息时，首先使用哈希算法计算得到明文信息的摘要(digest), 然后使用私钥对这段摘要进行加密，并把这段密文和要传输的明文信息一起传输给接收者，这个过程就称为数字签名. 从签名的过程可以看出，它主要包括两个步骤： 计算明文信息的摘要 使用私钥对摘要进行加密 当接收者收到相应的内容和签名信息时，他要做的就是验证签名，首先利用相同的摘要算法对明文信息计算得到摘要信息，再用发送者的公钥解密签名信息得到另一个摘要信息，比较计算得到和解密得到的摘要信息是否一致，以此判断这个签名是由发送者签注的。整个验签的过程也分为三个过程： 计算明文信息的摘要信息 使用公钥对接收内容中的签名进行解密 比较步骤1和2中的摘要信息是否一致 数字证书数字签名解决了消息来源的可靠性问题，但这基于一个前提，就是消息的接收者所持有的公钥确实是属于消息发送者的，如果这个公钥在不知情的情况下被替换了，那么还是存在收到冒充者发送的消息的可能性. 数字证书就是用于解决这个问题的. 现在面临的问题是接收者无法确定自己持有的公钥是否真的来自发送者，数字证书的解决方法就是引入可信任的第三方，首先这个第三方必须是可信任的，再由这个可信任的第三方为发送者的公钥做“担保”，那么这个公钥就是可信任的. 具体的步骤是这样的： 消息发送者将公钥及其它的信息发送给可信任的第三方(CA) CA组织在进行必须的校验后，对发送者给它的消息及公钥进行数字签名，形成数字证书颁发给发送者 发送者将数字证书发给接收者 接收者拿到数字证书后，首先对CA的数字签名进行验签，验签通过后就可以获取到发送者的公钥，这里的公钥就可以认为是由CA组织担保过的，是可信任的 那么问题来了，接收者怎么确定数字证书中的签名所使用的公钥就是可信任的呢？这里就引入了“证书链”的概念，按照上述的思路一步步地进行上溯，直到找到根证书. 根证书就是自签名或者根本没有签名的证书，安装某个根证书就意味着信任来自这个根证书，也就意味着信息由这个根证书签名过的所有证书. 在实际的操作中，浏览器通常会和某些根证书组织合作，安装浏览器的时候会自动安装这些根证书. HTTPS理解了对称加密、非对称加密以及数字证书的机理后，再来看HTTPS就比较简单了. 简单来讲，HTTPS就是在应用层和传输层之间加了SSL/TLS层，在服务端和客户端建立会话前，必须先建立安全通道，之后两者之间所有的数据传输都必须通过这个安全通道来进行. 在建立会话时，服务端和客户端通过非对称加密算法协商密钥，协商完密钥后，后续所有的数据传输都使用这个密钥通过对称加密完成. 整个过程被称为“四次握手”，如下所述： 客户端向服务端发送client_random以及客户端支持的对称加密算法列表 服务端收到客户端的请求后，向客户端发送server_random，选定的对称加密算法，以及服务端的数字证书 客户端首先验证数字证书的真实性，验证通过后，就使用数字证书中的公钥对premaster_num(客户端随机产生)进行加密传输 服务端收到加密的消息后，使用私钥进行解密，得到客户端传来的premaster_num, 并通过client_random, server_random和premaster_num计算得到对称加密中要使用的密钥，到这里为止，服务端和客户端完成了密钥的协商. 在后续的数据传输中，服务端都将使用步骤2中的对称加密算法，以及步骤4中计算得到的密钥对要传输的数据进行加密 客户端同样可以根据client_random, server_random以及premaster_num计算得到对称加密算法的密钥，并用它对服务端发来的加密数据进行解密，并在发送数据给服务端前进行加密 可以看到，在密钥协商的过程中，使用的是非对称加密的方法，最大限度地保证了密钥的随机性和保密性，而在数据传输的时候，使用的是则是对称加密，可以有效地提高加密解密的效率 参考文献 HTTPS详解： HTTPS详解 数字证书的基础知识: 数字证书的基础知识 What’s a digital signature: What’s a digital signature","categories":[],"tags":[]},{"title":"java-IO学习","slug":"java-io-basic","date":"2016-04-27T11:44:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/27/java-io-basic/","link":"","permalink":"http://yoursite.com/2016/04/27/java-io-basic/","excerpt":"","text":"JAVA的IO框架可大致分为三个部分：IO(也可以认为是BIO), NIO, NIO2. 这里只介绍IO的部分，其它两部分会在后续的学习中阐述. JAVA的IO部分又可以简单地分为以下几个部分: InputStream/OutputStream: 所有针对二进制字节流的操作都继承于这两个类 Reader/Writer: 这两个类是针对字符的读写操作设计的. Scanner/Formatter: 这两个类分别是用来格式化读取和输出时使用的，可以通过指定相应的格式来获取或输出数据 DataInput/DataOutput: 这两个接口分别是用于读取和输出java基础类型，它可以将基础类型以二进制流的形式输入输出到相应的流中 ObjectInput/ObjectOutput: 这两个接口与上述两个接口类似，不过它输入输出的目标是java对象，使用这两个接口时，要注意的是，输入输出的对象必须实现Serializable接口，否则会抛出异常 关于JAVA基础的IO部分基本上只有这点内容，相对来讲比较简单，但是值得一提的是，在这部分的接口设计中，使用到了装饰器模式，可以结合这部分的源码来理解装饰器模式的原理与应用, 下图是装饰器模式的UML 从上面的UML图中可以看出，装饰器模式的实现关键点有两个： 装饰器与被装饰对象实现了同一个接口 装饰器对象持有被装饰对象的引用 查看InputStream的类图可以发现，有个叫FilterInputStream的实现类，它继承了InputStream，并且在构造函数中持有一个inputStream的实例对象，可以看出，这里这个FilterInputStream的角色就是装饰器，而被持有的inputStream的实例对象就是被装饰对象，它们共同实例了装饰器模式. 进一步查看FilterInputStream的类图可以发现，许多带有“装饰”功能的inputStream类都继承自它，比如GzipInputStream, BufferedInputStream等等. 同样地，对于OutputStream, Reader, Writer等等接口也都可以看到相应的设计思路 练习代码可参见： 练习代码","categories":[],"tags":[{"name":"java io decorator","slug":"java-io-decorator","permalink":"http://yoursite.com/tags/java-io-decorator/"}]},{"title":"Git学习备忘","slug":"git-learning-memo","date":"2016-04-27T04:38:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/27/git-learning-memo/","link":"","permalink":"http://yoursite.com/2016/04/27/git-learning-memo/","excerpt":"","text":"写在前面这篇文档不是git命令的学习文档，git的基础命令及相应的语法和使用可以通过git help subcommand很方便地查到. 这里主要是将自己在学习git过程中遇到的一些问题记录下来，以备后续备忘使用. 名词解释 HEAD: git内部使用HEAD来指示当前所在分支的最新提交 master: 从远程仓库拉取代码时，git会默认在本地创建一个master分支，并用它来跟踪远程仓库中origin/master分支的内容 origin: 从远程创建拉取代码时，git会默认将远程仓库的地址设置为origin，也可以通过git remote add手工加入其它的远程仓库地址 Rebase在git内部中使用分支是个很常见的做法，使用分支必然意味着需要将不同的分支内容进行合并. 在git中提供了两种合并的方法，一种是merge， 一种是rebase. merge的使用方法比较简单，这里不做阐述，这是主要是简单记录下rebase的使用方法. 查阅rebase的文档可以发现，它的使用方法有下面三种: rebase –onto A B C(rebase –onto newBase Upstream Branch)这是最完整的命令形式，它的作用是切换到branch分支，并找到这个分支中不存在于upstream中的那些提交，将这些提交一一地在newBase中重放. 举个例子，可以看到，在进行rebase操作之前，分支b2是提交是在b1提交的基础上，在执行了rebase操作后，b2分支中的5b9686以及16f966两次被rebase到master分支的f71fe18的提交上, 并且HEAD指针指向b1分支. 1234567891011121314151617181920&gt; git lg* 5b9686c1fb2dbb1ef95fb36b47254bbf66225b83 (HEAD -&gt; b2) 7* 16f9663f77c6c58447e2f7d7f2b4f129fc1f1b05 6* 9947edd2cbafc34cc24d0182a8118e900ee043d9 (b1) 5* 479e44cf3d5e0e20f82ecd84f597d73c47b69919 3| * f71fe18e70ea2e2b2ae04ec405ea2e57b03be496 (master) 4|/ * 88a3b40bea4b9a4c0971ba4e08efb92491c2c4a3 2* 95f16a83315b86a4b0228310626ddcab69aefa96 1&gt; git rebase --onto master b1 b2&gt; git lg* c87c8e21df58e064b5b6e23def4efc9ead57bec6 (HEAD -&gt; b2) 7* aa84807e80971f8d51930a5c6d9dbf055bceff2a 6* f71fe18e70ea2e2b2ae04ec405ea2e57b03be496 (master) 4| * 9947edd2cbafc34cc24d0182a8118e900ee043d9 (b1) 5| * 479e44cf3d5e0e20f82ecd84f597d73c47b69919 3|/ * 88a3b40bea4b9a4c0971ba4e08efb92491c2c4a3 2* 95f16a83315b86a4b0228310626ddcab69aefa96 1 rebase B C这个命令省略了–onto参数，它默认将Upstream分支当作rebase分支, 从下面的例子可以看出，这个命令首先将分支切换到b1，并找到不在b2中的所有提交, 这里即20c108和f6a00b两次提交，然后把这两个提交在b2分支上进行重放. 最后得到一条线性的提交树 12345678&gt; git rebase b2 b1* 20c108c57a31783bdd9b794d9ad9549bbdc9a5e3 (HEAD -&gt; b1) 5* f6a00b0ee7905b937843747ee95ab73b891aef32 3* c87c8e21df58e064b5b6e23def4efc9ead57bec6 (b2) 7* aa84807e80971f8d51930a5c6d9dbf055bceff2a 6* f71fe18e70ea2e2b2ae04ec405ea2e57b03be496 (master) 4* 88a3b40bea4b9a4c0971ba4e08efb92491c2c4a3 2* 95f16a83315b86a4b0228310626ddcab69aefa96 1 rebase B这个命令格式省略了branch参数, 也就是说它在进行rebase操作之前，并不会进行分支切换，而是直接在当前分支进行操作. 最后的结果就是，计算出当前分支中没有在upstream分支中的提交对象，并将它们一一在upstream分支中回放. 使用rebase要注意的事项使用rebase可以使提交历史更加清晰明了，但使用它有个前提，就是不能对已经推到远程仓库的提交信息进行rebase, 否则其它的合作者在重新拉取仓库时会遇到很难搞的一些问题. Git的内部存储机制git除了对外提供简洁友好的用户命令之外，对内部它还有一套底层的接口来实现对象的存储. git内部存储的所有信息都放在.git目录下，目录的结构如下所示, 其中branches在高版本的git中已弃用，config中保存了项目相关的一些配置, description记录了gitWeb需要使用的一些信息，hooks记录了客户端以及服务端的一些hook配置，info中记录了全局配置的忽略跟踪的一些文件(.gitignore),除此之外，就剩下四个比较重要的文件和目录，分别是HEAD, index, objects和refs. 其中HEAD记录了当前分支的最新提交信息，index中记录了暂存区的信息，objects目录中记录了库中所有的对象，具体的存储形式在后续中会提到，而refs中又有heads, tags，以及remotes三个子目录，分别对应记录了分支，标签以及远程仓库的提交信息. object在git中,所有的对象都被存储为blob的形式，具体的存储过程如下： 获取相应的内容(content) 增加相应的头信息(header) 获取内容和头信息的sha1(sha1) 使用zlib压缩内容(zlib_content) 存储 1234567891011121314151617181920212223242526272829#引用库require 'digest/sha1'require 'zlib'require 'fileutils'#blob内容content = \"hello world!\"#blob头信息header = \"blob #&#123;content.length&#125;\\0\"#blob存储的消息store = header + content#blob消息的sha1sha1 = Digest::SHA1.hexdigest(store)#blob消息的zlibzlib_content = Zlib::Deflate.deflate(store)#写到文件中path = '.git/objects/' + sha1[0, 2] + '/' + sha1[2, 38]FileUtils.mkdir_p(File.dirname(path))File.open(path, 'w')&#123;|f|f.write zlib_content&#125;#校验git cat-file -p bc7774a7b18deb1d7bd0212d34246a9b1260ae17 hash-object上面的ruby脚本简单地展示了git内部创建blob对象的过程，事实上，git也提供了hash-object命令用于很方便地创建git节点，在git内部也是使用这个命令创建各个对象的blob 12345&gt; echo &quot;hello world!&quot; &gt;&gt; test.txt&gt; git hash-object -w test.txta0423896973644771497bdc03eb99d5281615b51&gt; git cat-file -p a0423896973644771497bdc03eb99d5281615b51hello world! cat-file这个命令号称是git用来检测内部对象的“瑞士军刀”. 它可以用来查询对象的存储信息，具体的输出根据不同的类型会有不同的信息，当然它也可以用来查询存储对象的类型信息, 在git中存储的对象类型分为四种，分别是blob, tree, commit和tag. 12345&gt; git cat-file -p a0423896973644771497bdc03eb99d5281615b51hello world!&gt; git cat-file -t a0423896973644771497bdc03eb99d5281615b51blob update-index这个命令是将相应的文件增加到暂存区，它还可以更新暂存里的文件内容，在执行完这条命令后，文件的blob对象就已经存储到git的objects中了. 1&gt; git update-index --add test.txt write-tree在将文件加到git的暂存区后，就可以使用write-tree命令了。write-tree命令是将暂存区中存储的对象写成一个树对象，并返回相应的SHA1标识. 同样地，生成的tree对象也存储在.git的objects中 通过cat-file查询树对象可以发现，树对象中存储了相应对象的类型，地址信息，及其文件名和读写权限信息(100644) 12345&gt; git write-tree5d56cf9b9843c20d7b29bf6374501f4f48841210&gt; git cat-file -p 5d56cf9b9843c20d7b29bf6374501f4f48841210100644 blob a0423896973644771497bdc03eb99d5281615b51 test.txt commit-tree在使用write-tree生成相应的树对象之后，就可以使用commit-tree命令来生成相应的commit对象了.顾名思义，这里的commit对象就是使用git commit时会生成的对象. 在使用commit-tree时，必要指定要提交的树对象的标识，即由write-tree命令返回的sha1标识，另外，它还可以通过-p参数指定父提交对象，这样就形成了使用git log时显示的提交历史树. 通过cat-file来查询commit-tree对象，可以看到对象中存储了提交信息，作者信息，提交者信息以及相应的树对象地址. 123456789&gt; echo &apos;first commit&apos; | git commit-tree 5d56cf9b9843c20d7b29bf6374501f4f488412104f69a44b7b55cd5dd4dcb872af5b0ac5e793de9b&gt; git cat-file -p 4f69a44b7b55cd5dd4dcb872af5b0ac5e793de9btree 5d56cf9b9843c20d7b29bf6374501f4f48841210author essviv &lt;514912821@qq.com&gt; 1461726403 +0800committer essviv &lt;514912821@qq.com&gt; 1461726403 +0800first commit update-ref在学习git的过程，经常会提到分支是git有别于其它VCS的“杀手锏”，因为在git中创建分支是个非常简单快速的过程, 而update-ref就是用来完成这个功能的. 在.git/refs目录下可以看到两个子目录，分别是heads和tags, 其中heads目录记录了各个分支的头结点的信息, 而tags则记录了tag信息，以下的例子就通过update-ref很简单地创建了个test分支，可以看到，在git中创建个分支简单到只需要在refs/heads/中创建个相应的文件，并在文件中写入41个字符（40个sha1标识+1个换行符). 1&gt; echo &apos;4f69a44b7b55cd5dd4dcb872af5b0ac5e793de9b&apos; &gt;&gt; test symbolic-ref除了分支信息外，git中还通过HEAD指针来指示当前所在分支的最新提交对象，这个信息就存储在.git/HEAD中，可以从HEAD文件的内容中看到，这个文件里存储的就是到refs/heads中某个文件的引用而已. 可以想像，在git中使用git checkout进行分支切换的时候，只需要将HEAD的中的内容指定相应的refs/heads中的分支结点，然后更新工作区的内容即可. 12&gt; cat HEADref: refs/heads/test All-in-one这里通过上述的命令简单地构造git的提交信息，完成通过git用户命令就可以完成的内容，进一步加深对git内部存储的理解. 创建文件test，并产生一些文件内容 使用update-index –add将它加入到暂存区 使用write-tree获取相应的树对象 使用commit-tree获取相应的提交对象C1 修改文件test内容，并增加新的文件new，加上一些内容 使用update-index将它们加入到暂存区 使用write-tree获取相应的树对象 使用commit-tree获取相应的提交对象C2，并将它的父提交对象设置为C1 重复前面四个步骤，获取提交对象C3，并将它的父对象设置为C2 使用update-ref在refs/heads中创建master分支，并将它指向C1 使用symbolic-ref将HEAD对象指定master 使用git log来查看刚刚创建的提交记录树 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&gt; echo “first commit” &gt;&gt; test&gt; git update-index --add test&gt; git write-treebff0827a0da713487c9fca9235263772db9a3b0e&gt; echo &quot;first commit&quot; | git commit-tree bff0827a0da713487c9fca9235263772db9a3b0ed6fac28b22fffdcd4d54126ec44ec86a712d6fde&gt; echo &quot;second commit&quot; &gt;&gt; test&gt; echo &quot;new file&quot; &gt;&gt; new&gt; git update-index test&gt; git update-index --add new&gt; git write-treebde4f04d1037685b47deeb515984cdb994888d07&gt; echo &quot;second commit&quot; | git commit-tree bde4f04d1037685b47deeb515984cdb994888d07 -p d6fac28b22fffdcd4d54126ec44ec86a712d6fde0f33097c08b7827224083238dc9294f59aa7948a&gt; echo &quot;third commit&quot; &gt;&gt; test&gt; echo &quot;third commit&quot; &gt;&gt; new&gt; git update-index test&gt; git update-index new&gt; git write-treef4c0f72be323c47af0dd2bbddfbaf6f548b11926&gt; echo &quot;third commit&quot; | git commit-tree f4c0f72be323c47af0dd2bbddfbaf6f548b11926 -p 0f33097c08b7827224083238dc9294f59aa7948a25eee6f70d508567abe72969d3524f557bc159d8&gt; git update-ref refs/heads/master 25eee6f70d508567abe72969d3524f557bc159d8&gt; git update-ref refs/heads/test 0f33097c08b7827224083238dc9294f59aa7948a&gt; git log --pretty=onelie --graph --decorate=short&gt; git symbolic-ref HEAD refs/heads/test&gt; git log --pretty=onelie --graph --decorate=short 其它从上述的描述中可以看到，git内部维护了许多对象，包括blob, blob, tree以及tags信息，并且每次更新后重新提交又会创建新的blob对象，这样，随着时间的推移，git内部的对象将越来越多，如果不对这些对象进行一些处理，可以想像，这些对象占据的空间将逐步增大，因此，git内部使用了压缩的方法, 将一批文件压缩将形成pack文件，pack文件还有相应的idx信息，方便外部快速地对pack文件进行读取,这部分内容具体可以git官方文档. 参考文档ProGit Book: ProGit","categories":[],"tags":[{"name":"git internal","slug":"git-internal","permalink":"http://yoursite.com/tags/git-internal/"}]},{"title":"JAVA-IO学习之IO","slug":"IO/java基础/JAVA-IO学习之IO","date":"2016-04-27T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/27/IO/java基础/JAVA-IO学习之IO/","link":"","permalink":"http://yoursite.com/2016/04/27/IO/java基础/JAVA-IO学习之IO/","excerpt":"","text":"JAVA-IO学习之IOJAVA的IO框架可大致分为三个部分：IO(也可以认为是BIO), NIO, NIO2. 这里只介绍IO的部分，其它两部分会在后续的学习中阐述. JAVA的IO部分又可以简单地分为以下几个部分: InputStream/OutputStream: 所有针对二进制字节流的操作都继承于这两个类 Reader/Writer: 这两个类是针对字符的读写操作设计的. Scanner/Formatter: 这两个类分别是用来格式化读取和输出时使用的，可以通过指定相应的格式来获取或输出数据 DataInput/DataOutput: 这两个接口分别是用于读取和输出java基础类型，它可以将基础类型以二进制流的形式输入输出到相应的流中 ObjectInput/ObjectOutput: 这两个接口与上述两个接口类似，不过它输入输出的目标是java对象，使用这两个接口时，要注意的是，输入输出的对象必须实现Serializable接口，否则会抛出异常 关于JAVA基础的IO部分基本上只有这点内容，相对来讲比较简单，但是值得一提的是，在这部分的接口设计中，使用到了装饰器模式，可以结合这部分的源码来理解装饰器模式的原理与应用, 下图是装饰器模式的UML 从上面的UML图中可以看出，装饰器模式的实现关键点有两个： 装饰器与被装饰对象实现了同一个接口 装饰器对象持有被装饰对象的引用 查看InputStream的类图可以发现，有个叫FilterInputStream的实现类，它继承了InputStream，并且在构造函数中持有一个inputStream的实例对象，可以看出，这里这个FilterInputStream的角色就是装饰器，而被持有的inputStream的实例对象就是被装饰对象，它们共同实例了装饰器模式. 进一步查看FilterInputStream的类图可以发现，许多带有“装饰”功能的inputStream类都继承自它，比如GzipInputStream, BufferedInputStream等等. 同样地，对于OutputStream, Reader, Writer等等接口也都可以看到相应的设计思路 练习代码可参见： 练习代码","categories":[],"tags":[]},{"title":"git","slug":"工具/git","date":"2016-04-27T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/27/工具/git/","link":"","permalink":"http://yoursite.com/2016/04/27/工具/git/","excerpt":"","text":"#写在前面 这篇文档不是git命令的学习文档，git的基础命令及相应的语法和使用可以通过git help subcommand很方便地查到. 这里主要是将自己在学习git过程中遇到的一些问题记录下来，以备后续备忘使用. 名词解释 HEAD: git内部使用HEAD来指示当前所在分支的最新提交 master: 从远程仓库拉取代码时，git会默认在本地创建一个master分支，并用它来跟踪远程仓库中origin/master分支的内容 origin: 从远程创建拉取代码时，git会默认将远程仓库的地址设置为origin，也可以通过git remote add手工加入其它的远程仓库地址 Rebase在git内部中使用分支是个很常见的做法，使用分支必然意味着需要将不同的分支内容进行合并. 在git中提供了两种合并的方法，一种是merge， 一种是rebase. merge的使用方法比较简单，这里不做阐述，这是主要是简单记录下rebase的使用方法. 查阅rebase的文档可以发现，它的使用方法有下面三种: &gt; rebase --onto newBase Upstream Branch rebase –onto A B C这是最完整的命令形式，它的作用是切换到branch分支，并找到这个分支中不存在于upstream中的那些提交，将这些提交一一地在newBase中重放. 举个例子，可以看到，在进行rebase操作之前，分支b2是提交是在b1提交的基础上，在执行了rebase操作后，b2分支中的5b9686以及16f966两次被rebase到master分支的f71fe18的提交上, 并且HEAD指针指向b1分支. 1234567891011121314151617181920&gt; git lg* 5b9686c1fb2dbb1ef95fb36b47254bbf66225b83 (HEAD -&gt; b2) 7* 16f9663f77c6c58447e2f7d7f2b4f129fc1f1b05 6* 9947edd2cbafc34cc24d0182a8118e900ee043d9 (b1) 5* 479e44cf3d5e0e20f82ecd84f597d73c47b69919 3| * f71fe18e70ea2e2b2ae04ec405ea2e57b03be496 (master) 4|/ * 88a3b40bea4b9a4c0971ba4e08efb92491c2c4a3 2* 95f16a83315b86a4b0228310626ddcab69aefa96 1&gt; git rebase --onto master b1 b2&gt; git lg* c87c8e21df58e064b5b6e23def4efc9ead57bec6 (HEAD -&gt; b2) 7* aa84807e80971f8d51930a5c6d9dbf055bceff2a 6* f71fe18e70ea2e2b2ae04ec405ea2e57b03be496 (master) 4| * 9947edd2cbafc34cc24d0182a8118e900ee043d9 (b1) 5| * 479e44cf3d5e0e20f82ecd84f597d73c47b69919 3|/ * 88a3b40bea4b9a4c0971ba4e08efb92491c2c4a3 2* 95f16a83315b86a4b0228310626ddcab69aefa96 1 rebase B C这个命令省略了–onto参数，它默认将Upstream分支当作rebase分支, 从下面的例子可以看出，这个命令首先将分支切换到b1，并找到不在b2中的所有提交, 这里即20c108和f6a00b两次提交，然后把这两个提交在b2分支上进行重放. 最后得到一条线性的提交树 12345678&gt; git rebase b2 b1* 20c108c57a31783bdd9b794d9ad9549bbdc9a5e3 (HEAD -&gt; b1) 5* f6a00b0ee7905b937843747ee95ab73b891aef32 3* c87c8e21df58e064b5b6e23def4efc9ead57bec6 (b2) 7* aa84807e80971f8d51930a5c6d9dbf055bceff2a 6* f71fe18e70ea2e2b2ae04ec405ea2e57b03be496 (master) 4* 88a3b40bea4b9a4c0971ba4e08efb92491c2c4a3 2* 95f16a83315b86a4b0228310626ddcab69aefa96 1 rebase B这个命令格式省略了branch参数, 也就是说它在进行rebase操作之前，并不会进行分支切换，而是直接在当前分支进行操作. 最后的结果就是，计算出当前分支中没有在upstream分支中的提交对象，并将它们一一在upstream分支中回放. 使用rebase要注意的事项使用rebase可以使提交历史更加清晰明了，但使用它有个前提，就是不能对已经推到远程仓库的提交信息进行rebase, 否则其它的合作者在重新拉取仓库时会遇到很难搞的一些问题. Git的内部存储机制git除了对外提供简洁友好的用户命令之外，对内部它还有一套底层的接口来实现对象的存储. git内部存储的所有信息都放在.git目录下，目录的结构如下所示, 其中branches在高版本的git中已弃用，config中保存了项目相关的一些配置, description记录了gitWeb需要使用的一些信息，hooks记录了客户端以及服务端的一些hook配置，info中记录了全局配置的忽略跟踪的一些文件(.gitignore),除此之外，就剩下四个比较重要的文件和目录，分别是HEAD, index, objects和refs. 其中HEAD记录了当前分支的最新提交信息，index中记录了暂存区的信息，objects目录中记录了库中所有的对象，具体的存储形式在后续中会提到，而refs中又有heads, tags，以及remotes三个子目录，分别对应记录了分支，标签以及远程仓库的提交信息. object在git中,所有的对象都被存储为blob的形式，具体的存储过程如下： 获取相应的内容(content) 增加相应的头信息(header) 获取内容和头信息的sha1(sha1) 使用zlib压缩内容(zlib_content) 存储 1234567891011121314151617181920212223242526272829#引用库require 'digest/sha1'require 'zlib'require 'fileutils'#blob内容content = \"hello world!\"#blob头信息header = \"blob #&#123;content.length&#125;\\0\"#blob存储的消息store = header + content#blob消息的sha1sha1 = Digest::SHA1.hexdigest(store)#blob消息的zlibzlib_content = Zlib::Deflate.deflate(store)#写到文件中path = '.git/objects/' + sha1[0, 2] + '/' + sha1[2, 38]FileUtils.mkdir_p(File.dirname(path))File.open(path, 'w')&#123;|f|f.write zlib_content&#125;#校验git cat-file -p bc7774a7b18deb1d7bd0212d34246a9b1260ae17 hash-object上面的ruby脚本简单地展示了git内部创建blob对象的过程，事实上，git也提供了hash-object命令用于很方便地创建git节点，在git内部也是使用这个命令创建各个对象的blob 12345&gt; echo &quot;hello world!&quot; &gt;&gt; test.txt&gt; git hash-object -w test.txta0423896973644771497bdc03eb99d5281615b51&gt; git cat-file -p a0423896973644771497bdc03eb99d5281615b51hello world! cat-file这个命令号称是git用来检测内部对象的“瑞士军刀”. 它可以用来查询对象的存储信息，具体的输出根据不同的类型会有不同的信息，当然它也可以用来查询存储对象的类型信息, 在git中存储的对象类型分为四种，分别是blob, tree, commit和tag. 12345&gt; git cat-file -p a0423896973644771497bdc03eb99d5281615b51hello world!&gt; git cat-file -t a0423896973644771497bdc03eb99d5281615b51blob update-index这个命令是将相应的文件增加到暂存区，它还可以更新暂存里的文件内容，在执行完这条命令后，文件的blob对象就已经存储到git的objects中了. 1&gt; git update-index --add test.txt write-tree在将文件加到git的暂存区后，就可以使用write-tree命令了。write-tree命令是将暂存区中存储的对象写成一个树对象，并返回相应的SHA1标识. 同样地，生成的tree对象也存储在.git的objects中 通过cat-file查询树对象可以发现，树对象中存储了相应对象的类型，地址信息，及其文件名和读写权限信息(100644) 12345&gt; git write-tree5d56cf9b9843c20d7b29bf6374501f4f48841210&gt; git cat-file -p 5d56cf9b9843c20d7b29bf6374501f4f48841210100644 blob a0423896973644771497bdc03eb99d5281615b51 test.txt commit-tree在使用write-tree生成相应的树对象之后，就可以使用commit-tree命令来生成相应的commit对象了.顾名思义，这里的commit对象就是使用git commit时会生成的对象. 在使用commit-tree时，必要指定要提交的树对象的标识，即由write-tree命令返回的sha1标识，另外，它还可以通过-p参数指定父提交对象，这样就形成了使用git log时显示的提交历史树. 通过cat-file来查询commit-tree对象，可以看到对象中存储了提交信息，作者信息，提交者信息以及相应的树对象地址. 123456789&gt; echo &apos;first commit&apos; | git commit-tree 5d56cf9b9843c20d7b29bf6374501f4f488412104f69a44b7b55cd5dd4dcb872af5b0ac5e793de9b&gt; git cat-file -p 4f69a44b7b55cd5dd4dcb872af5b0ac5e793de9btree 5d56cf9b9843c20d7b29bf6374501f4f48841210author essviv &lt;514912821@qq.com&gt; 1461726403 +0800committer essviv &lt;514912821@qq.com&gt; 1461726403 +0800first commit update-ref在学习git的过程，经常会提到分支是git有别于其它VCS的“杀手锏”，因为在git中创建分支是个非常简单快速的过程, 而update-ref就是用来完成这个功能的. 在.git/refs目录下可以看到两个子目录，分别是heads和tags, 其中heads目录记录了各个分支的头结点的信息, 而tags则记录了tag信息，以下的例子就通过update-ref很简单地创建了个test分支，可以看到，在git中创建个分支简单到只需要在refs/heads/中创建个相应的文件，并在文件中写入41个字符（40个sha1标识+1个换行符). 1&gt; echo &apos;4f69a44b7b55cd5dd4dcb872af5b0ac5e793de9b&apos; &gt;&gt; test symbolic-ref除了分支信息外，git中还通过HEAD指针来指示当前所在分支的最新提交对象，这个信息就存储在.git/HEAD中，可以从HEAD文件的内容中看到，这个文件里存储的就是到refs/heads中某个文件的引用而已. 可以想像，在git中使用git checkout进行分支切换的时候，只需要将HEAD的中的内容指定相应的refs/heads中的分支结点，然后更新工作区的内容即可. 12&gt; cat HEADref: refs/heads/test All-in-one这里通过上述的命令简单地构造git的提交信息，完成通过git用户命令就可以完成的内容，进一步加深对git内部存储的理解. 创建文件test，并产生一些文件内容使用update-index –add将它加入到暂存区使用write-tree获取相应的树对象使用commit-tree获取相应的提交对象C1修改文件test内容，并增加新的文件new，加上一些内容使用update-index将它们加入到暂存区使用write-tree获取相应的树对象使用commit-tree获取相应的提交对象C2，并将它的父提交对象设置为C1重复前面四个步骤，获取提交对象C3，并将它的父对象设置为C2使用update-ref在refs/heads中创建master分支，并将它指向C1使用symbolic-ref将HEAD对象指定master使用git log来查看刚刚创建的提交记录树 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&gt; echo “first commit” &gt;&gt; test&gt; git update-index --add test&gt; git write-treebff0827a0da713487c9fca9235263772db9a3b0e&gt; echo &quot;first commit&quot; | git commit-tree bff0827a0da713487c9fca9235263772db9a3b0ed6fac28b22fffdcd4d54126ec44ec86a712d6fde&gt; echo &quot;second commit&quot; &gt;&gt; test&gt; echo &quot;new file&quot; &gt;&gt; new&gt; git update-index test&gt; git update-index --add new&gt; git write-treebde4f04d1037685b47deeb515984cdb994888d07&gt; echo &quot;second commit&quot; | git commit-tree bde4f04d1037685b47deeb515984cdb994888d07 -p d6fac28b22fffdcd4d54126ec44ec86a712d6fde0f33097c08b7827224083238dc9294f59aa7948a&gt; echo &quot;third commit&quot; &gt;&gt; test&gt; echo &quot;third commit&quot; &gt;&gt; new&gt; git update-index test&gt; git update-index new&gt; git write-treef4c0f72be323c47af0dd2bbddfbaf6f548b11926&gt; echo &quot;third commit&quot; | git commit-tree f4c0f72be323c47af0dd2bbddfbaf6f548b11926 -p 0f33097c08b7827224083238dc9294f59aa7948a25eee6f70d508567abe72969d3524f557bc159d8&gt; git update-ref refs/heads/master 25eee6f70d508567abe72969d3524f557bc159d8&gt; git update-ref refs/heads/test 0f33097c08b7827224083238dc9294f59aa7948a&gt; git log --pretty=onelie --graph --decorate=short&gt; git symbolic-ref HEAD refs/heads/test&gt; git log --pretty=onelie --graph --decorate=short 其它从上述的描述中可以看到，git内部维护了许多对象，包括blob, blob, tree以及tags信息，并且每次更新后重新提交又会创建新的blob对象，这样，随着时间的推移，git内部的对象将越来越多，如果不对这些对象进行一些处理，可以想像，这些对象占据的空间将逐步增大，因此，git内部使用了压缩的方法, 将一批文件压缩将形成pack文件，pack文件还有相应的idx信息，方便外部快速地对pack文件进行读取,这部分内容具体可以git官方文档. 参考文档ProGit Book: https://progit2.s3.amazonaws.com/en/2016-03-22-f3531/progit-en.1084.pdf","categories":[],"tags":[]},{"title":"java集合学习之源码分析1","slug":"集合/java集合学习之源码分析1","date":"2016-04-20T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/20/集合/java集合学习之源码分析1/","link":"","permalink":"http://yoursite.com/2016/04/20/集合/java集合学习之源码分析1/","excerpt":"","text":"#HashMap 在之前的学习中我们也提到过HashMap的一些特点，这里再简单地总结下： 非线程安全的实现方式 作为一种通用实现，它的key和value都可以是null 迭代器实现了fast-fail机制，即在迭代过程中如果发生了并发修改操作，会抛出异常 数据存储我们都知道map中的数据是以键值对的形式存储的，那么在hashMap中，具体存储的数据格式是怎么样的，当我们执行put或者get操作时，底层到底是如何实现的呢？一起先来看看HashMap的存储结构，如下图所示，hashMap事实上是数组和链表的组合体，每个键值对被封装成entry存储在这个结构中，每个entry除了提供的key和value之外，还有个很重要的元素是hash，它的值是由key经过hash计算得来的. PUT操作put操作的源码如下所示： 首先先根据提供的key值计算相应的hash值, 这个hash值决定了这个键值对会被存储在map中的哪个bucket中，即决定了图中数组的下标;而hash值的计算是根据Key的hashCode方法计算后取其高位16位得到，注意这里如果提供的key为null值，默认得到的哈希值为0，即key为null的键值对会被默认存储于0号桶中 得到hash, key, value后（暂时忽略onlyIfAbsent以及evict参数，它对hashMap的影响不是本质性的），会调用 putVal方法，这个方法首先检查数组的长度,如果为空或者数组长度为0，则进入resize操作(代码块第13行）(resize操作的细节请参见resize一章）.紧接着就会根据提供的hash值计算这个键值对所在的bucket的位置（在代码中为变量i的值)，这里分两种情况 如果数组的当前索引位置上没有元素，那么可以直接把这个元素放到数组的这个索引位置上(代码块第15行） 如果数组的当前索引位置上有元素，说明当前键值对的哈希值和Map中存储的另一键值对的哈希值发生了碰撞，在这种情况下，hashMap引入了链表的结构来进行存储。通过遍历链表上的元素，并通过key.equals()方法来比较当前的键值对和链表上的元素是否相等，如果相等那么直接进行替换，如果直到链表尾部也没有找到相等的元素，那么就在链表尾部插入新的键值对(代码块16~43行) 在完成元素的插入操作后，hashMap还会进行一次判断，比较当前map里存储的元素个数和预先设定的threshold值，这个阈值是由数组初始化长度和loadFactor相乘得来 ，如果超过这个阈值，说明当前Map中的负载已经较高，那么就进行一次resize操作，将数组的长度扩大一倍 除此之外，hashMap还做了一些优化，比如当某个bucket的链表超过一定数量时，它会把链表转化成树来存储，当少于一定数量时，又会转化成链表来存储；另外，通过modCount这个参数来实现了迭代器的fast-fail机制.（代码块第28行） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; ###GET操作 理解了put操作的原理后，再来理解get操作就非常简单了. 首先还是根据提供的key值计算相应的hash，计算的方法和put时使用的方法是一样的，否则可能会造成取不到值了 得到相应的hash值之后，相当于就知道这个元素所在的bucket的位置了，这个bucket在前面讲过，就是个链表结构，所以剩下要做的就是遍历这个链表，取出相应的值即可. 在遍历链表的时候，比较的是提供的key值和链表中的key值是否相等，通过key.equals()方法来判断，如果相等，说明找到了相应的值，直接返回；如果直到链表结尾都没有找到相等的元素，那么直接返回null即可. 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; RESIZE操作resize操作是在map中存储的键值对个数超过预先设置的阈值时对hashMap进行扩容的实现，预设的阈值由capacity和load_factor两个参数共同决定, capacity决定了数组的初始长度，而load_factory决定了当数组的负载达到多少的时候，进入resize操作. 首先判断旧的数组长度，如果长度不为0，则直接将capacity和threshold扩大为原来的两倍 如果旧数组长度为0， 说明这是个初始化操作, 初始化的数组长度保存于threshold中(代码块15行） 如果capacity和threshold都为0，新的数组长度和阈值均采用默认值 在计算好新的数组长度和阈值之后，要做的就是遍历原来的元素并将迁移到现在的数组中（代码块31~69行）. 注意这里的每个数组元素可能都是一个链表，所以每次遍历的是一个链表，而不是一个数组元素。在遍历的时候，还要注意的是这里将hash和原来的数组长度按位并操作，如果得到的结果为0，说明这个元素在新的数组中还是应该存储在原来的索引位置上，如果得到的值不是0，说明这个键值对在新的数组中应该放在另外的位置，这个位置就是原先位置偏移oldCap个索引的位置. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125;","categories":[],"tags":[]},{"title":"java集合学习之源码分析","slug":"java-collection-hashmap","date":"2016-04-19T11:22:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/19/java-collection-hashmap/","link":"","permalink":"http://yoursite.com/2016/04/19/java-collection-hashmap/","excerpt":"","text":"HashMap在之前的学习中我们也提到过HashMap的一些特点，这里再简单地总结下： 非线程安全的实现方式 作为一种通用实现，它的key和value都可以是null 迭代器实现了fast-fail机制，即在迭代过程中如果发生了并发修改操作，会抛出异常 数据存储我们都知道map中的数据是以键值对的形式存储的，那么在hashMap中，具体存储的数据格式是怎么样的，当我们执行put或者get操作时，底层到底是如何实现的呢？一起先来看看HashMap的存储结构，如下图所示，hashMap事实上是数组和链表的组合体，每个键值对被封装成entry存储在这个结构中，每个entry除了提供的key和value之外，还有个很重要的元素是hash，它的值是由key经过hash计算得来的. PUT操作put操作的源码如下所示： 首先先根据提供的key值计算相应的hash值, 这个hash值决定了这个键值对会被存储在map中的哪个bucket中，即决定了图中数组的下标;而hash值的计算是根据Key的hashCode方法计算后取其高位16位得到，注意这里如果提供的key为null值，默认得到的哈希值为0，即key为null的键值对会被默认存储于0号桶中 得到hash, key, value后（暂时忽略onlyIfAbsent以及evict参数，它对hashMap的影响不是本质性的），会调用 putVal方法，这个方法首先检查数组的长度,如果为空或者数组长度为0，则进入resize操作(resize操作的细节请参见resize一章）.紧接着就会根据提供的hash值计算这个键值对所在的bucket的位置（在代码中为变量i的值)，这里分两种情况 如果数组的当前索引位置上没有元素，那么可以直接把这个元素放到数组的这个索引位置上 如果数组的当前索引位置上有元素，说明当前键值对的哈希值和Map中存储的另一键值对的哈希值发生了碰撞，在这种情况下，hashMap引入了链表的结构来进行存储。通过遍历链表上的元素，并通过key.equals()方法来比较当前的键值对和链表上的元素是否相等，如果相等那么直接进行替换，如果直到链表尾部也没有找到相等的元素，那么就在链表尾部插入新的键值对 在完成元素的插入操作后，hashMap还会进行一次判断，比较当前map里存储的元素个数和预先设定的threshold值，这个阈值是由数组初始化长度和loadFactor相乘得来 ，如果超过这个阈值，说明当前Map中的负载已经较高，那么就进行一次resize操作，将数组的长度扩大一倍 除此之外，hashMap还做了一些优化，比如当某个bucket的链表超过一定数量时，它会把链表转化成树来存储，当少于一定数量时，又会转化成链表来存储；另外，通过modCount这个参数来实现了迭代器的fast-fail机制. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; GET操作理解了put操作的原理后，再来理解get操作就非常简单了. 首先还是根据提供的key值计算相应的hash，计算的方法和put时使用的方法是一样的，否则可能会造成取不到值了 得到相应的hash值之后，相当于就知道这个元素所在的bucket的位置了，这个bucket在前面讲过，就是个链表结构，所以剩下要做的就是遍历这个链表，取出相应的值即可. 在遍历链表的时候，比较的是提供的key值和链表中的key值是否相等，通过key.equals()方法来判断，如果相等，说明找到了相应的值，直接返回；如果直到链表结尾都没有找到相等的元素，那么直接返回null即可. 123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; RESIZE操作resize操作是在map中存储的键值对个数超过预先设置的阈值时对hashMap进行扩容的实现，预设的阈值由capacity和load_factor两个参数共同决定, capacity决定了数组的初始长度，而load_factory决定了当数组的负载达到多少的时候，进入resize操作. 首先判断旧的数组长度，如果长度不为0，则直接将capacity和threshold扩大为原来的两倍 如果旧数组长度为0， 说明这是个初始化操作, 初始化的数组长度保存于threshold中 如果capacity和threshold都为0，新的数组长度和阈值均采用默认值 在计算好新的数组长度和阈值之后，要做的就是遍历原来的元素并将迁移到现在的数组中. 注意这里的每个数组元素可能都是一个链表，所以每次遍历的是一个链表，而不是一个数组元素。在遍历的时候，还要注意的是这里将hash和原来的数组长度按位并操作，如果得到的结果为0，说明这个元素在新的数组中还是应该存储在原来的索引位置上，如果得到的值不是0，说明这个键值对在新的数组中应该放在另外的位置，这个位置就是原先位置偏移oldCap个索引的位置. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125;","categories":[],"tags":[{"name":"java collection hashMap","slug":"java-collection-hashMap","permalink":"http://yoursite.com/tags/java-collection-hashMap/"}]},{"title":"java集合学习","slug":"java-collection-concurrent-implements","date":"2016-04-18T13:55:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/18/java-collection-concurrent-implements/","link":"","permalink":"http://yoursite.com/2016/04/18/java-collection-concurrent-implements/","excerpt":"","text":"JAVA的集合框架中提供了很多通用的实现，但这些实现基本上都不是线程安全的，因此，JAVA框架还提供了这些集合类的并发实现，这篇文章专门来讨论这些并发的实现 BlockingQueue接口这个接口定义了线程安全的QUEUE接口，在从队列里读取或者插入操作时，如果队列为空，或者已经到达队列上限，那么操作会被阻塞. 按操作无法立即执行时方法执行的操作可以将BlockingQueue的方法分为四类： 抛出异常: add, remove, element 返回特殊值: offer, poll, peek 阻塞直到可执行: put, take 阻塞一定时间: offer, poll BlockingQueue的实现有五种： ArrayBlockingQueue: BlockingQueue的数组实现，这种实现的元素在创建时就定好了，如果达到数组的上限，那么offer和put操作将不能得到立即实现; LinkedBlockingQueue: 阻塞队列的链表实现. 这种实现默认是是没有元素个数限制的, 当然如果希望有上限的话，也可以设置 DelayQueue: 延迟队列，它的元素必须实现了Delay接口，返回延迟的时间，在这个时间前，元素不能被消费,其底层是通过PriorityQueue来实现的. PriorityBlockingQueue: 这种实现可以是PriorityQueue的并发实现, 它的所有元素必须实现Comparable，或者在构造对象时提供相应的comparator接口的实现，这点和PriorityQueue是一致的. SynchronizedQueue: 这是一种特殊的阻塞队列的实现，它只有一个元素，并且插入的线程一直阻塞到这个元素被消费了为止；同样地，如果一个线程试图从空队列中取元素，那么它也会一直阻塞到有其它线程往这个队列里插入新的元素为止. BlockingDeque接口这个接口是双向队列的阻塞接口，它和BlockingQueue的关系就像Queue和Deque的关系是一样的，所有的BlockingQueue的操作在双向扩展后即是BlockingDeque的方法，这里就不做具体的叙述，实现这个接口的类只有一个LinkedBlockingDeque,从名字可以看出，这是一种链表实现. ConcurrentMap接口这是Map的并发实现接口, 它提供了线程安全的访问Map的方法，它的实现有ConcurrentHashMap, 这个实现和HashTable十分相似，但有几点不同： 在进行读操作时，ConcurrentHashMap不会对map加锁，而hashTable会对所有的操作进行同步 在进行写操作时，concurrentHashMap不会对整个map加锁，它将整个map分为几个部分（默认为16）,每次写操作时只会对一个部分进行加锁，而hashTable会对整个map进行同步 在迭代过程, 如果map的内容发生了变化，concurrentHashMap不会抛出异常，虽然它的迭代器并不是为并发设计的 参考文献Sample代码： 这里 参考文献: 参考文献","categories":[],"tags":[{"name":"java collection implement concurrent","slug":"java-collection-implement-concurrent","permalink":"http://yoursite.com/tags/java-collection-implement-concurrent/"}]},{"title":"java集合学习之并发实现","slug":"集合/java集合学习之并发实现","date":"2016-04-18T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/18/集合/java集合学习之并发实现/","link":"","permalink":"http://yoursite.com/2016/04/18/集合/java集合学习之并发实现/","excerpt":"","text":"JAVA的集合框架中提供了很多通用的实现，但这些实现基本上都不是线程安全的，因此，JAVA框架还提供了这些集合类的并发实现，这篇文章专门来讨论这些并发的实现 BlockingQueue接口这个接口定义了线程安全的QUEUE接口，在从队列里读取或者插入操作时，如果队列为空，或者已经到达队列上限，那么操作会被阻塞. 按操作无法立即执行时方法执行的操作可以将BlockingQueue的方法分为四类： 抛出异常: add, remove, element 返回特殊值: offer, poll, peek 阻塞直到可执行: put, take 阻塞一定时间: offer, poll BlockingQueue的实现有五种： ArrayBlockingQueue: BlockingQueue的数组实现，这种实现的元素在创建时就定好了，如果达到数组的上限，那么offer和put操作将不能得到立即实现; LinkedBlockingQueue: 阻塞队列的链表实现. 这种实现默认是是没有元素个数限制的, 当然如果希望有上限的话，也可以设置 DelayQueue: 延迟队列，它的元素必须实现了Delay接口，返回延迟的时间，在这个时间前，元素不能被消费,其底层是通过PriorityQueue来实现的. PriorityBlockingQueue: 这种实现可以是PriorityQueue的并发实现, 它的所有元素必须实现Comparable，或者在构造对象时提供相应的comparator接口的实现，这点和PriorityQueue是一致的. SynchronizedQueue: 这是一种特殊的阻塞队列的实现，它只有一个元素，并且插入的线程一直阻塞到这个元素被消费了为止；同样地，如果一个线程试图从空队列中取元素，那么它也会一直阻塞到有其它线程往这个队列里插入新的元素为止. BlockingDeque接口这个接口是双向队列的阻塞接口，它和BlockingQueue的关系就像Queue和Deque的关系是一样的，所有的BlockingQueue的操作在双向扩展后即是BlockingDeque的方法，这里就不做具体的叙述，实现这个接口的类只有一个LinkedBlockingDeque,从名字可以看出，这是一种链表实现. ConcurrentMap接口这是Map的并发实现接口, 它提供了线程安全的访问Map的方法，它的实现有ConcurrentHashMap, 这个实现和HashTable十分相似，但有几点不同： 在进行读操作时，ConcurrentHashMap不会对map加锁，而hashTable会对所有的操作进行同步 在进行写操作时，concurrentHashMap不会对整个map加锁，它将整个map分为几个部分（默认为16）,每次写操作时只会对一个部分进行加锁，而hashTable会对整个map进行同步 在迭代过程, 如果map的内容发生了变化，concurrentHashMap不会抛出异常，虽然它的迭代器并不是为并发设计的 参考文献Sample代码： 这里参考文献: 参考文献","categories":[],"tags":[]},{"title":"Java集合学习","slug":"java-collection-implements","date":"2016-04-17T03:14:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/17/java-collection-implements/","link":"","permalink":"http://yoursite.com/2016/04/17/java-collection-implements/","excerpt":"","text":"JAVA集合框架的接口部分定义了各种集合类的功能，具体的实现则在各种具体的实现中完成。按照实现的目的不同，JAVA集合接口的实现可分为以下几种： 通用目的的实现： 这些实现都是日常开发中经常用到的实现，具体的实现类包括： 特殊目的的实现： 这些实现都是为了一些特殊目的而进行的实现，可能会有某些限制 并发实现（也称为线程安全的实现）: 这些实现由java.util.concurret包提供，主要是提供相应接口的高并发实现，在性能上要比相应单线程实现稍差 包装实现：这些实现通常要与通用的实现一起，通过对通用实现的封装，实现或增加某些功能，这是一种装饰器模式的实现，装饰器模式的结构图如下所示： 工具类实现： 这类实现通常是利用静态工厂方法来很方便地提供通用实现的实例 从通用实现的表中可以看到，set, list和map都提供了相应的通用实现，sortedSet和sortedMap都只提供了一种实现，即TreeSet和TreeMap，Queue也提供了LinkedList和PriorityQueue的实现，它们的语义是不同的，LinkedList提供了FIFO的实现，而priorityQueue则是按照元素的值进行排序. 所有通用实现的类都提供了接口所有的可选操作，并且都允许它们的元素，键和值为null，并且它们都不是线程安全的实现，而且它们的迭代器都采用了fast-fail^1机制 Set接口的实现Set接口提供了三种默认的实现: HashSet, TreeSet和LinkedHashSet. HashSet: 这是三种实现中效率最高的实现，基本上大部分的操作可以在常量时间内完成，但它没有对元素的顺序提供任何保证，如果对元素的顺序没有特殊要求，基本上可以考虑使用这种实现注意 HashSet的迭代效率同时取决于它的元素个数和capacity的大小，因此，如果将capacity的值设置得过大，那么将造成时间和空间的浪费，但如果设置得太小，也会导致元素不停地被复制。如果这个值没有被设置，那么默认为16. TreeSet: 如果需要使用SortedSet，或者需要迭代时需要根据元素的值进行排序时，则需要使用这种实现 LinkedHashSet: 这种实现可以认为是介于HashSet和TreeSet的一种实现，它在内部维护了一份双向链表，实现的效率与HashSet相差无几，但遍历的时候会按照元素插入的顺序进行 同时，Java框架也提供了两种特殊目的实现，EnumSet和CopyOnWriteSet EnumSet: 这种set实现了对枚举类型的某些方便操作，比如获取某种枚举类型的所有元素，补集等操作，总得来讲，比较简单. CopyOnWriteSet: 内部是维护了一份CopyOnWriteList对象，它们唯一的区别是CopyOnWriteList允许有重复元素,而CopyOnWriteSet不允许, 事实上, COWSet的所有操作都是通过COWList来完成的(除了add)。这种实现的机制是所有的写操作(add, remove, set等等)都会复制一份原来的数据，然后进行写操作，然后再把数据赋值给原来的引用, 当然在整个写的过程中，需要使用锁机制完成。 另外，它的迭代器不支持写操作，同时在迭代的过程中，由其它线程更新的数据也不会被迭代器看到，也就是说，在迭代器创建的时候，它就维护了那个时刻集合中元素的一个快照，因此它的迭代过程中不会有线程安全问题. CopyOnWrite的实现决定了它只适合于那种读操作远远超过写操作的场合. List接口的实现在JAVA框架中，提供了List接口的两种通用实现，一种是ArrayList， 一种是LinkedList， 顾名思义，这两种实现分别是基于数组和链接的方式实现的，它们的优缺点也比较明显： 数组方式的实现能够在常量时间内提供位置访问，但插入和删除操作需要对元素进行拷贝；而链表方式的实现，插入操作和删除操作不需要进行元素拷贝，只需要修改下元素的前后指标即可，但访问某个元素需要通过遍历的方式进行，需要线性时间来完成。 JAVA框架中还提供了一种特殊的实现，CopyOnWriteArrayList, 这种实现的机制就像COWSet中阐述的那样，是一种线程安全的实现，每次的写操作都会导致内部数据的复制，因此特别适合于大量写少量写的场合，比如用它来维护一个eventHandler的列表 Map接口的实现Map的三种实现是和Set的三种实现一一对应的，分别是HashMap， TreeMap和LinkedHashMap. 使用它们的场景也各不相同： 如果需要执行SortedMap的操作或者需要按照Key值顺序对整个集合进行遍历时，使用TreeMap 如果希望获得最好的运行效率而不关心遍历顺序时，使用HashMap 如果希望按照插入的顺序进行遍历时，使用LinkedHashMap Map的特殊实现还包括: EnumMap, WeakHashMap, IdentityHashMap以及CocurrentMap. 其中CocurrentMap是由JAVA的并发框架提供的map的并发实现. EnuMap: 这种实现底层是通过数组来实现的，是以枚举类的对象为key值，并提供快速访问数组元素的方法，如果希望将枚举类型映射成值，应该考虑使用这种实现 WeakHashMap: 这种实现的key值使用了对象的弱引用（weak reference，具体含义及说明请见这里), 也就是说，当集合中的Key对象不再被外界引用的时候，它会被GC回收，导致相应的entry不存在. IdentityHashMap: 这种类型的实现是使用引用相等性来替代对象相等性，也就是说，它可以允许存在重复的key值，在identityHashMap中，判断两个key值相等的条件是k1 == k2. 注意这里比较的是引用相等， 而在一般的map实现中，Key值的相等条件是(k1==null)?k2==null:k1.equals(k2). CocurrentMap：这是由java的并发框架提供的一种线程安全的map实现，CocurrentHashMap是它的一种实现，和hashtable不同的是，它并不是简单地对所有方法加上synchronized关键字，它是通过将bucket分成几个部分（默认为16）,每次操作的时候只获取相应部分的锁，从而达到并发操作的目的. Queue接口的实现Queue接口提供了两种通用实现： Queue的poll, remove, element以及peek等方法都是取出相应的头元素，而所谓的头元素是指排序上排得最前面的那个元素，具体排序的规则由相应的实现规定. 在遍历Queue的时候，要使用poll方法来获取相应的元素，这些元素才会按照相应的顺序进行排序输出，如果使用iterator， 会直接按照底层的数组顺序进行输出，这点可以从PriorityQueue的测试代码中很明显的看出来。 LinkedList: 这是一种FIFO的队列，它的元素顺序是指插入的时间顺序 PriorityQueue: 这是一种优先级队列.它的顺序是指元素的自然顺序或者是在构建对象时指定的comparator指定的比较顺序. Queue接口还提供了相应的并发实现，通过定义BlockingQueue来完成，它拓展了queue的功能，在获取元素时可以一直等待直到队列非空，在插入元素时会一直等待直接队列可用。BlockingQueue有几下几种实现: ArrayBlockingQueue: 一种FIFO的阻塞队列实现，底层是通过数组来实现 LinkedBlockingQueue: 这是一种通过链表实现的FIFO阻塞队列 PriorityBlockingQueue: 具有优先级的阻塞队列 DelayQueue: 基于定时器的阻塞队列 Deque接口的实现Deque接口定义可以从两端进行存储的容器类，它提供了两种通用实现和一种并发实现： ArrayDeque: 它在队列两端进行增删的效率要高于LinkedList LinkedList: 双向队列的链表实现，它在队列两端进行增删的不如ArrayDeque, 但它在迭代过程中可以删除相应的元素，并且效率要好于ArrayDeque. LinkedBlockingDeque: 这是deque接口的阻塞式链表实现，在获取元素时，如果队列为空，它会一直阻塞直到队列中出现了新的元素才返回 包装实现JAVA的集合框架中提供了相应接口的包装实现，这是一种装饰器模式的应用 ，它可以在原有的接口功能上加上一些特殊功能和限制，比如同步机制，不可修改的限制等等. 这些实现通常不是通过提供公有的类定义来实现，而是通过collections这个类的静态方法来提供，大体上，包装类可分为两大类： 同步实现： 这种实现对于每种接口都有一个相应的方法，把原来通用实现中线程不安全的集合接口封装成线程安全的实现，底层是通过对每个方法都加上synchronized关键字来实现的，可想而知，效率并不高，但在一些场合保证了并发操作的安全性. Collections中所有以synchronized开头的方法都是这种实现. 增加不可变限制： 目前所有的接口实现都不要求它的元素是不可变的，在生成相应的容器类及元素后，可以对容器当中的元素进行任意地修改，通过这种装饰器模式，容器中的元素就具备了只读属性，当然，这里有个前提，在得到包装后的容器类后，就不应该再引用原来的容器类，更不应该对原来的容器类进行修改，从而保证包装后的容器中的元素不会被修改。这种实现是通过重载所有会对容器中元素进行修改操作的方法来完成，它会抛出UnsupportedOperation异常. Collections中所有以unmodifiable开头的方法都是这种实现. 参考文献 参考文献: JAVA官方文档 示例代码：集合实现sample代码","categories":[],"tags":[{"name":"java collection","slug":"java-collection","permalink":"http://yoursite.com/tags/java-collection/"}]},{"title":"java集合学习之实现","slug":"集合/java集合学习之实现","date":"2016-04-17T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/17/集合/java集合学习之实现/","link":"","permalink":"http://yoursite.com/2016/04/17/集合/java集合学习之实现/","excerpt":"","text":"JAVA集合框架的接口部分定义了各种集合类的功能，具体的实现则在各种具体的实现中完成。按照实现的目的不同，JAVA集合接口的实现可分为以下几种： 通用目的的实现： 这些实现都是日常开发中经常用到的实现，具体的实现类包括： 特殊目的的实现： 这些实现都是为了一些特殊目的而进行的实现，可能会有某些限制 并发实现（也称为线程安全的实现）: 这些实现由java.util.concurret包提供，主要是提供相应接口的高并发实现，在性能上要比相应单线程实现稍差 包装实现：这些实现通常要与通用的实现一起，通过对通用实现的封装，实现或增加某些功能，这是一种装饰器模式的实现，装饰器模式的结构图如下所示： 工具类实现： 这类实现通常是利用静态工厂方法来很方便地提供通用实现的实例 从通用实现的表中可以看到，set, list和map都提供了相应的通用实现，sortedSet和sortedMap都只提供了一种实现，即TreeSet和TreeMap，Queue也提供了LinkedList和PriorityQueue的实现，它们的语义是不同的，LinkedList提供了FIFO的实现，而priorityQueue则是按照元素的值进行排序. 所有通用实现的类都提供了接口所有的可选操作，并且都允许它们的元素，键和值为null，并且它们都不是线程安全的实现，而且它们的迭代器都采用了fast-fail^1机制 Set接口的实现Set接口提供了三种默认的实现: HashSet, TreeSet和LinkedHashSet. HashSet: 这是三种实现中效率最高的实现，基本上大部分的操作可以在常量时间内完成，但它没有对元素的顺序提供任何保证，如果对元素的顺序没有特殊要求，基本上可以考虑使用这种实现注意 HashSet的迭代效率同时取决于它的元素个数和capacity的大小，因此，如果将capacity的值设置得过大，那么将造成时间和空间的浪费，但如果设置得太小，也会导致元素不停地被复制。如果这个值没有被设置，那么默认为16. TreeSet: 如果需要使用SortedSet，或者需要迭代时需要根据元素的值进行排序时，则需要使用这种实现 LinkedHashSet: 这种实现可以认为是介于HashSet和TreeSet的一种实现，它在内部维护了一份双向链表，实现的效率与HashSet相差无几，但遍历的时候会按照元素插入的顺序进行 同时，Java框架也提供了两种特殊目的实现，EnumSet和CopyOnWriteSet EnumSet: 这种set实现了对枚举类型的某些方便操作，比如获取某种枚举类型的所有元素，补集等操作，总得来讲，比较简单. CopyOnWriteSet: 内部是维护了一份CopyOnWriteList对象，它们唯一的区别是CopyOnWriteList允许有重复元素,而CopyOnWriteSet不允许, 事实上, COWSet的所有操作都是通过COWList来完成的(除了add)。这种实现的机制是所有的写操作(add, remove, set等等)都会复制一份原来的数据，然后进行写操作，然后再把数据赋值给原来的引用, 当然在整个写的过程中，需要使用锁机制完成。 另外，它的迭代器不支持写操作，同时在迭代的过程中，由其它线程更新的数据也不会被迭代器看到，也就是说，在迭代器创建的时候，它就维护了那个时刻集合中元素的一个快照，因此它的迭代过程中不会有线程安全问题. CopyOnWrite的实现决定了它只适合于那种读操作远远超过写操作的场合. List接口的实现在JAVA框架中，提供了List接口的两种通用实现，一种是ArrayList， 一种是LinkedList， 顾名思义，这两种实现分别是基于数组和链接的方式实现的，它们的优缺点也比较明显： 数组方式的实现能够在常量时间内提供位置访问，但插入和删除操作需要对元素进行拷贝；而链表方式的实现，插入操作和删除操作不需要进行元素拷贝，只需要修改下元素的前后指标即可，但访问某个元素需要通过遍历的方式进行，需要线性时间来完成。 JAVA框架中还提供了一种特殊的实现，CopyOnWriteArrayList, 这种实现的机制就像COWSet中阐述的那样，是一种线程安全的实现，每次的写操作都会导致内部数据的复制，因此特别适合于大量写少量写的场合，比如用它来维护一个eventHandler的列表 Map接口的实现Map的三种实现是和Set的三种实现一一对应的，分别是HashMap， TreeMap和LinkedHashMap. 使用它们的场景也各不相同： 如果需要执行SortedMap的操作或者需要按照Key值顺序对整个集合进行遍历时，使用TreeMap 如果希望获得最好的运行效率而不关心遍历顺序时，使用HashMap 如果希望按照插入的顺序进行遍历时，使用LinkedHashMap Map的特殊实现还包括: EnumMap, WeakHashMap, IdentityHashMap以及CocurrentMap. 其中CocurrentMap是由JAVA的并发框架提供的map的并发实现. EnuMap: 这种实现底层是通过数组来实现的，是以枚举类的对象为key值，并提供快速访问数组元素的方法，如果希望将枚举类型映射成值，应该考虑使用这种实现 WeakHashMap: 这种实现的key值使用了对象的弱引用（weak reference，具体含义及说明请见这里), 也就是说，当集合中的Key对象不再被外界引用的时候，它会被GC回收，导致相应的entry不存在. IdentityHashMap: 这种类型的实现是使用引用相等性来替代对象相等性，也就是说，它可以允许存在重复的key值，在identityHashMap中，判断两个key值相等的条件是k1 == k2. 注意这里比较的是引用相等， 而在一般的map实现中，Key值的相等条件是(k1==null)?k2==null:k1.equals(k2). CocurrentMap：这是由java的并发框架提供的一种线程安全的map实现，CocurrentHashMap是它的一种实现，和hashtable不同的是，它并不是简单地对所有方法加上synchronized关键字，它是通过将bucket分成几个部分（默认为16）,每次操作的时候只获取相应部分的锁，从而达到并发操作的目的. Queue接口的实现Queue接口提供了两种通用实现： Queue的poll, remove, element以及peek等方法都是取出相应的头元素，而所谓的头元素是指排序上排得最前面的那个元素，具体排序的规则由相应的实现规定. 在遍历Queue的时候，要使用poll方法来获取相应的元素，这些元素才会按照相应的顺序进行排序输出，如果使用iterator， 会直接按照底层的数组顺序进行输出，这点可以从PriorityQueue的测试代码中很明显的看出来。 LinkedList: 这是一种FIFO的队列，它的元素顺序是指插入的时间顺序 PriorityQueue: 这是一种优先级队列.它的顺序是指元素的自然顺序或者是在构建对象时指定的comparator指定的比较顺序. Queue接口还提供了相应的并发实现，通过定义BlockingQueue来完成，它拓展了queue的功能，在获取元素时可以一直等待直到队列非空，在插入元素时会一直等待直接队列可用。BlockingQueue有几下几种实现: ArrayBlockingQueue: 一种FIFO的阻塞队列实现，底层是通过数组来实现 LinkedBlockingQueue: 这是一种通过链表实现的FIFO阻塞队列 PriorityBlockingQueue: 具有优先级的阻塞队列 DelayQueue: 基于定时器的阻塞队列 Deque接口的实现Deque接口定义可以从两端进行存储的容器类，它提供了两种通用实现和一种并发实现： ArrayDeque: 它在队列两端进行增删的效率要高于LinkedList LinkedList: 双向队列的链表实现，它在队列两端进行增删的不如ArrayDeque, 但它在迭代过程中可以删除相应的元素，并且效率要好于ArrayDeque. LinkedBlockingDeque: 这是deque接口的阻塞式链表实现，在获取元素时，如果队列为空，它会一直阻塞直到队列中出现了新的元素才返回 包装实现JAVA的集合框架中提供了相应接口的包装实现，这是一种装饰器模式的应用 ，它可以在原有的接口功能上加上一些特殊功能和限制，比如同步机制，不可修改的限制等等. 这些实现通常不是通过提供公有的类定义来实现，而是通过collections这个类的静态方法来提供，大体上，包装类可分为两大类： 同步实现： 这种实现对于每种接口都有一个相应的方法，把原来通用实现中线程不安全的集合接口封装成线程安全的实现，底层是通过对每个方法都加上synchronized关键字来实现的，可想而知，效率并不高，但在一些场合保证了并发操作的安全性. Collections中所有以synchronized开头的方法都是这种实现. 增加不可变限制： 目前所有的接口实现都不要求它的元素是不可变的，在生成相应的容器类及元素后，可以对容器当中的元素进行任意地修改，通过这种装饰器模式，容器中的元素就具备了只读属性，当然，这里有个前提，在得到包装后的容器类后，就不应该再引用原来的容器类，更不应该对原来的容器类进行修改，从而保证包装后的容器中的元素不会被修改。这种实现是通过重载所有会对容器中元素进行修改操作的方法来完成，它会抛出UnsupportedOperation异常. Collections中所有以unmodifiable开头的方法都是这种实现. 参考文献 参考文献: JAVA官方文档 示例代码：集合实现sample代码","categories":[],"tags":[]},{"title":"强引用，软引用，弱引用以及虚引用","slug":"reference-types-in-java","date":"2016-04-16T09:55:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/16/reference-types-in-java/","link":"","permalink":"http://yoursite.com/2016/04/16/reference-types-in-java/","excerpt":"","text":"强引用(Strong Reference)是指平时经常用到的引用类型，如果某个对象存在强引用，那么它将不会被GC回收 软引用(Soft Reference)是指那些有用但不是必需的对象，它经常被用作缓存，当JVM内存充足时，它不会被GC回收，但如果内存不足时，它会被回收；它可以和引用队列(ReferenceQueue)进行关联，当软引用被回收时，它就进入关联的引用队列 弱引用(Weak Reference)是指不是必需的引用，在GC开始的时候，不管内存是否充足，它都将被回收；它可以和引用队列（ReferenceQueue)相关联，当弱引用被回收时，它就被加入到相关联的引用队列中 虚引用(Phantom Reference) 并不影响对象的生命周期，如果一个对象和虚引用关联，那么就跟没有和引用关联一样，它随时可能被GC回收，它必须和引用队列相关联，当GC某个对象时，如果发现它还有虚引用，则会把它加入到相应的引用队列中，可以通过判断虚引用是否出现在这个引用队列中，来确定该对象是否被回收 参考文献：参考文献","categories":[],"tags":[{"name":"引用类型，GC","slug":"引用类型，GC","permalink":"http://yoursite.com/tags/引用类型，GC/"}]},{"title":"JAVA集合学习","slug":"java-collection-interface","date":"2016-04-14T13:47:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/14/java-collection-interface/","link":"","permalink":"http://yoursite.com/2016/04/14/java-collection-interface/","excerpt":"","text":"java集合框架的内容包括三个部分， 分别为接口，实现，算法. 本章只详述接口部分的内容。 接口： 接口可从宏观上分为两类， 分别为map和collection， collection又可细分为set, list, queue, deque.在理解接口的时候，可以从以下几个方面来理解： 接口提供了哪些功能 接口有哪些具体的实现，以及它们之间的区别 接口可以有哪些操作 Collection接口collection接口中包括了集合最基本的操作，比如size, add, remove, iterator, isEmpty, contains等等操作，不同的collection实现类可以通过构造函数很方便地进行类型转换.如下: List&lt;String&gt; strings = new LinkedList&lt;String&gt;(); Set&lt;String&gt; stringSets = new HashSet&lt;String&gt;(strings); 遍历collection的方式 在JDK1.8中，可以通过聚合操作来完成 collection.stream().filter(e –&gt; e.getColor == Colors.RED).forEach(e –&gt; doSth(e)) 使用forEach操作 for(String s : collection){ doSth(s); } 使用iterator操作: 这里的iterator使用了迭代器模式，可以借此机会复习下迭代器模式，在遍历的过程中，使用迭代器的remove方法也是唯一能够安全地操作collection中的元素的方法，如果在遍历的过程中使用其它的方式改变了collection中的元素，其行为是不可预知的. Iterator iter = collection.iterator(); while(iter.hasNext()){ doSth(iter.next()); } Set接口Set接口定义了元素不能重复的集合类，它可以认为是数学意义上的集合，在JAVA的集合框架中有三种不同的实现： HashSet: 底层使用HashMap进行存储，事实上HashMap的keySet就是这个hashSet, 它不保证集合遍历的顺序，但这是效率最好的实现 TreeSet: 底层使用红黑树存储，根据它们的值进行排序，效率比hashSet略慢 LinkedHashSet: 底层使用hashMap以及双向链接进行存储，它能够保证元素遍历的顺序与插入的顺序一致 既然把set接口认为是数学意义上的集合，很显然就可以对它进行交集、并集、求差等操作，事实上set接口中的retailAll和removeAll方法就是用来实现这些功能的 List接口List接口定义了一组有序集合，集合中的元素可以重复，它提供了顺序访问以及搜寻功能，同时也提供了遍历和局部视图功能，可以取出集合中的某部分元素集合进行操作，在JAVA集合框架中，提供了两种实现： ArrayList: 底层使用数组来实现元素的存储，在绝大部分情况下，这种实现的性能是比较好的。 LinkedList: List的链表实现, 底层使用双向链表进行存储，这种实现在增删元素时性能更佳，但顺序访问时性能不好，因为需要遍历整个链表 Queue接口Queue是一系列准备用于处理的元素的集合，除了collection提供的方法之外，它还提供了额外的增改查操作，对于所有的增改查操作，queue接口都提供了两种实现方式，在操作失败的时候，一种是抛出异常，另一种是返回特定的值（如null或者false， 依不同的操作而定), 具体的操作如下： 其中，add， remove和element在操作失败的时候会抛出异常, 而与它们一一对应的offer, poll和peek在操作失败时则会返回false，另外remove/poll和element/peek的区别在于，前者从queue中取到元素后，会把元素从queue中删除，而后者则不会 Deque接口deque接口是可以从头尾进行增删改的队列接口，应该说它是queue接口的扩展，因为它同时实现了queue(FIFO）以及堆(LIFO)的功能，从它的提供的操作来看，也可以很清楚地看到这点，所有在queue中的六个操作(add, offer, remove, poll, element, peek)在deque都有头元素以及尾元素的实现，具体操作如下： 可以看到，所有的六个操作都有了两种针对头元素和尾元素的实现，但语义不变 Map接口map接口提供了将Key映射成value的对象，它可以认为是数学意义上的函数。它提供了基本的增删改查的操作以及相应的视图操作(put, remove, get, contains, size, empty, entrySet, keySet, values)等等 注意在map接口提供的三个视图中，keySet和entrySet都是set类型的，也就是说它们是不能重复的，但是values只是collection类型，这意味着它的值是可以重复的（这也是数学意义上函数的定义). 另外，在视图上的一些操作（如removeAll, retailAll等)都会影响到原来的map的内容,具体可以查阅HashMap的keySet方法的实现源码 在JAVA集合框架中也提供了三种实现： HashMap, TreeMap和LinkedHashMap.它们的语义及特点正如这些名字所指示的那样，和对应的三个Set(HashSet, TreeSet, LinkedHashSet)相同，事实上，对应的set在实现的时候，内部就是借助了Map的key不能重复的特点，直接将map的keySet进行使用 multimap的语义是它的每个Key值可以指向多个value, 在java的集合框架中并没有这种类型，事实上，这种主义的Map完全可以通过将值的类型设置为某种集合来实现，如Map&lt;String, List&lt;String&gt;&gt;. 因此，这种类型的map将不再被特殊讨论和对待. Comparable接口在进一步学习容器接口之前，有必要先了解下Comparable接口，顾名思义，这个接口定义了对象的比较属性，实现了这个接口的类就具备了可比较性，比较的语义由实现决定，在JAVA的实现中有很多类都实现了这个接口，比如String，按照字母顺序进行比较；Date类会按照时间顺序进行比较 Comparator接口这是另一个用来实现对象比较的接口，在一些集合类算法中，如果某些类对象没有实现comparable接口，在使用排序算法时，可以额外提供一个comparator接口来实现排序功能，事实上，comparator接口是一种策略模式的实现，策略模式的结构图如下: SortedSet接口SortedSet是一种set接口，但是它把元素按照升序进行排列，排序的规则由元素本身提供(自然排序)，前提是元素实现了Comparable接口，否则将返回类型转换错误; 如果元素没有实现comparable接口，也可以提供comparator接口，通过使用策略模式来进行排序SortedSet接口提供了几类操作：视图操作，端点操作以及获取内部使用的comparator的接口的操作 视图操作情况下，如果原来的集合中的元素被修改了，视图中的元素也会发生相应的变化，反过来也一样，也就是说，可以把视图当作集合的一个窗口，视图操作获取的元素只不过通过这个窗口能看到的原来的集合中的部分元素 端点操作默认是左闭右开区间，即包含头节点，但不包含尾节点，但可以通过在头节点或者尾节点后增加“\\0”来改变这种行为，如果头节点增加了这个标识，意味着头节点将不被包括在返回的集合中(左开区间），如果尾节点增加了这个标识，意味着它将会被包括在返回的集合中（右闭区间) SortedMap接口这个接口所有的操作和属性都和SortedSet一致，因此这里就略过不讲 在JAVA的集合框架中，提供了TreeSet来实现SortedSet接口. 备注 具体的Sample代码可以参见这里 参考文档： 集合说明文档","categories":[],"tags":[]},{"title":"数据库事务的隔离级别","slug":"数据库/数据库事务的隔离级别","date":"2016-04-12T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/12/数据库/数据库事务的隔离级别/","link":"","permalink":"http://yoursite.com/2016/04/12/数据库/数据库事务的隔离级别/","excerpt":"","text":"数据库事务的隔离级别隔离级别 数据库事务的隔离级别分为四个： 未提交读， 提交读，重复读以及序列化，不同隔离级别能解决的问题以及不能解决的问题如下： 重复读会对读的范围加锁，也就是在第一次读操作开始的时候，事务会对它所读的范围进行加锁，其它的事务就不能对这个范围内的记录进行修改和删除操作，以此来保证它的重复读；但它不能保证选择范围外的记录不被修改或增加，因此无法避免幻读的情况(select) 幻读更多的是针对增加或修改范围外的记录的情况而言的 提交读的读锁是在读完之后就被立即释放，而写锁是在事务提交时释放，当事务A的读锁释放后，这部分数据就可能被其它事务修改，因此 它会导致不可重复读的问题； 重复读获取的也是读锁，但读锁直到事务结束才会被释放， 因此它能够解决重复读的问题，但是因为它没有范围锁，所以无法避免幻读的情况； 隔离级别和锁的联系封锁（Locking）封锁是实现并发控制的一个非常重要的技术。所谓封锁就是事务T在对某个数据对象例如表、记录等操作之前，先向系统发出请求，对其加锁。加锁后事务T就对该 数据对象有了一定的控制，在事务T释放它的锁之前，其它的事务不能更新此数据对象。 基本的封锁类型有两种：排它锁（Exclusive locks 简记为X锁）和共享锁（Share locks 简记为S锁）。 排它锁又称为写锁。若事务T对数据对象A加上X锁，则只允许T读取和修改A，其它任何事务都不能再对A加任何类型的锁，直到T释放A上的锁。这就保证了其它事务在T释放A上的锁之前不能再读取和修改A。 共享锁又称为读锁。若事务T对数据对象A加上S锁，则其它事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这就保证了其它事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 封锁协议在 运用X锁和S锁这两种基本封锁，对数据对象加锁时，还需要约定一些规则，例如应何时申请X锁或S锁、持锁时间、何时释放等。我们称这些规则为封锁协议 （Locking Protocol）。对封锁方式规定不同的规则，就形成了各种不同的封锁协议。下面介绍三级封锁协议。三级封锁协议分别在不同程度上解决了丢失的修改、不 可重复读和读”脏”数据等不一致性问题，为并发操作的正确调度提供一定的保证。下面只给出三级封锁协议的定义，不再做过多探讨。 1级封锁协议：事务T在修改数据R之前必须先对其加X锁，直到事务结束才释放。事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。 1级封锁协议可防止丢失修改，并保证事务T是可恢复的。在1级封锁协议中，如果仅仅是读数据不对其进行修改，是不需要加锁的，所以它不能保证可重复读和不读”脏”数据。 2级封锁协议：1级封锁协议加上事务T在读取数据R之前必须先对其加S锁，读完后即可释放S锁。2级封锁协议除防止了丢失修改，还可进一步防止读”脏”数据。 一旦释放了S锁之后，其它的事务就可以对这部分数据进行修改，因此它无法避免重复读的问题。 3级封锁协议：1级封锁协议加上事务T在读取数据R之前必须先对其加S锁，直到事务结束才释放。3级封锁协议除防止了丢失修改和不读’脏’数据外，还进一步防止了不可重复读。 参考文献 文献1 文献2 隔离级别和锁的联系 数据库并发的五个问题及四级封锁协议","categories":[],"tags":[]},{"title":"RabbitMQ的集群方案","slug":"消息队列/rabbitMQ/RabbitMQ的集群方案","date":"2016-04-10T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/10/消息队列/rabbitMQ/RabbitMQ的集群方案/","link":"","permalink":"http://yoursite.com/2016/04/10/消息队列/rabbitMQ/RabbitMQ的集群方案/","excerpt":"","text":"RabbitMQ的集群方案1. RMQ的集群组建RMQ集群可以动态的变化，集群中的每个节点可以先单独创建，然后再加入到集群中；也可以随时从集群中退出 在RMQ集群中，所有的数据和状态都是共享的，包括用户，虚拟主机，队列，exchange, 绑定以及运行时参数，这些对象在集群中基本上都是有备份的，除了queue，在默认配置下,queue只存在于被声明的那个节点中，如果需要对它进行镜像，需要参考“RabbitMQ的HA方案”进行配置 通过rabbitmqctl手工创建 通过config file列出所有的集群节点 通过rabbitmq-autocluster插件来组建 通过rabbitmq-clusterer插件来组建 2. 集群模式RMQ的节点可以有两种模式，一种是disk模式，一种是RAM模式。RAM节点具有更高的性能表现，但在处理持久化消息的时候，仍然会把消息持久化到硬盘上 3. RMQ集群的组建步骤 集群中各个节点（包括命令行工具）的通讯是通过cookie文件来实现的，各个节点必须拥有相同的cookie才可以进行通讯。Erlang虚拟机在启动时，会自动生成一个cookie文件，因此，最简单的方式就是让集群中的某个节点先生成cookie文件，然后将这个cookie文件复制给集群中的其它节点即可 在集群中的每个节点上分别启动rabbitmq-server， 此时可以通过rabbitmqctl来分别查看三个节点的集群状态，可以看到，它们现在还是三个独立运行的节点 假设要将A节点加入到B节点中，可以通过在A节点上执行以下的操作来实现: 123rabbitmqctl stop_apprabbitmqctl join_cluster B（这里是指B节点的名字)rabbitmqctl start_app 这时可以查看AB两个节点的集群状态 ，可以看到它们和之前的状态已经是不一样的；另外值得注意的是，将A节点加入到B节点中会导致A节点原来的数据都被重置 如果要将集群中的某个节点下线或者退出集群，可以通过在该节点上执行以下的操作即可： 123rabbitmqctl stop_apprabbitmqctl resetrabbitmqctl start_app 当整个集群下线时，最后下线的节点必须是最先上线的那个节点，否则其它的节点会等待30秒然后报错，如果想从集群中移除这个节点，可以通过设置forget_cluster_node 来完成 如果整个集群由于断电等原因同时下线，就会导致这样的一种情况，所有的节点都认为有其它的节点比自己晚下线，这时可以使用force_boot参数来强制重启集群 集群中所有的节点使用的erlang版本或者rabbitmq版本必须一致，但补丁版本可以不一样(x.y.z中的z) 参考文献 官方文档 参考文献","categories":[],"tags":[]},{"title":"Redis","slug":"缓存/Redis","date":"2016-04-10T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/10/缓存/Redis/","link":"","permalink":"http://yoursite.com/2016/04/10/缓存/Redis/","excerpt":"","text":"Redis数据类型 字符串 列表 集合 散列 有序集合 说明: 具体的redis命令可以通过中文网站进行参考, 每个类型的命令均可以按照“CURD”的分组来帮助记忆 键空间事件通知 通知可分为两类 键空间通知：键空间中的键发生变化的时候会发送通知，通知内容为事件的名称 键事件通知：当某些特定的操作被执行时，会触发相应的键事件,通知内容为操作的键名称:123对 0 号数据库的键 mykey 执行 DEL 命令时， 系统将分发两条消息,相当于执行以下两个 PUBLISH 命令： PUBLISH __keyspace@0__:mykey del PUBLISH __keyevent@0__:del mykey 键事件的具体分类可参考键空间通知 可以结合psubscribe的功能实现订阅空间中所有键的通知或所有事件的通知 psubscribe \\_\\_key\\*\\_\\_:\\* 事务中的错误事务中的错误也可以分为两类 事务执行前的错误，例如语法错误、内存不足等错误 事务执行之后的错误，有些操作虽然语法正确，但是操作的内容可能出错，比如把一个列表命令用于操作一个zset类型的键等对于事务执行前的错误，客户端一般会记录这些错误，在事务执行的时候，拒绝执行并取消这个事务；而对于事务执行后发生的错误，redis只是简单地将它们忽略，不影响事务中其它命令的执行 订阅和发布Redis中通过subscribe、unsubscribe、psubscribe, punsubscribe，publish来实现相应的功能，具体的消息格式见消息格式. psubscribe支持glob风格的通配符： *： 0或任意多个字符 ?: 任意单个字符 […]: 方括号中的任意一个字符 [!…]: 不在方括号中的任意一个字符 主从备份集群部署持久化策略管道技术管道技术可以将多个命令批量地发送给服务器进行执行，它可以和事务进行结合使用但管道技术和事务是不一样的，管道技术是在传输层的一种优化，它将多个命令打包成一次命令进行传送，以此来减少网络的回路时间；而事务主要的功能是保证了事务中所有操作的原子性，并且保证这些命令在执行过程中不会有其它的命令被执行 其它 Redis事务中的watch/unwatch关键字，可以用来实现乐观锁（CAS)的功能 事务执行之后，不论事务的执行是成功还是失败，之前watch的内容都会被取消 参考文献 http://redis.io/topics/pipelining http://stackoverflow.com/questions/29327544/pipelining-vs-transaction-in-redis","categories":[],"tags":[]},{"title":"RabbitMQ的基本概念","slug":"消息队列/rabbitMQ/RabbitMQ的基本概念","date":"2016-04-09T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/09/消息队列/rabbitMQ/RabbitMQ的基本概念/","link":"","permalink":"http://yoursite.com/2016/04/09/消息队列/rabbitMQ/RabbitMQ的基本概念/","excerpt":"","text":"RMQ的基本概念： 基本概念 生产者将消息投递给broker中的exchange模块，exchange根据“绑定”规则将这条消息投递给queue， 而queue再根据订阅的情况主动地push给消费者，或者由消费者主动向broker进行pull操作。 queue， exchange以及binding三者一起组成了broker的实体，这是个可编程的模型，应用开发者可以自行定义实体内容，并在需要的时候删除或修改实体的内容。 Exchange的类型 默认的exchange，它是directExchange的一种 ，但它的名字是空的，它有一个很重要的特性，每一个新创建的队列都会自动绑定到它这里，使用的routingKey和名称相同，这在一些简单的程序里很有用，因为它让程序从外部看上去就好像是直接将消息投递到queue中，虽然事实并不是这样。 direct exchange: 它是根据routingKey进行路由的一种exchange， 假如queueA使用routingKey = K绑定到这个exchange，那么所有routingKey = K的消息都将被投递给这个queueA. fanout exchange: 这种exchange将忽略消息中的routingKey, 它会直接将消息发送给和这个exchange绑定的所有queue，因此这种exchange特别适合于广播，比如游戏中的广播或者公告栏等功能 topic exchange: 多播，这种exchange分发消息的策略是根据消息中的routingKey和queue绑定到这个exchange时所使用的routing pattern进行匹配决定的。比如说，如果某个queueA使用stocks.update.*绑定到这种类型的exchange， 那么如果某个消息的routingKey为stocks.update.IBM，那么这条消息将会被投递给这个queueA header exchange: 这种类型的exchange不是根据routingKey来投递消息的，而是根据消息的头信息的匹配来进行投递，如果某个消息的头信息和queue在绑定到exchange时使用的头信息相同，那么消息就会被投递给这个queue，它可以认为是direct exchange的一种补充，因为direct exchange要求routingKey必须是字符串，而header exchange的头信息可以是数字，也可以是其它类型。header类型的exchange不支持通配符匹配。 队列队列的属性包括： 名字，持久化，排它性，自动删除，其它属性 队列的名称：长度不超过255个UTF-8的字符串，也可以使用空字符串，这时broker会自动创建名称，在后续需要使用队列名称的地方，继续传入空字符串即可，因为channel会记住broker为它创建的队列名称 队列的持久化： 持久化支持的队列在broker重启之后会被自动创建，但是持久化的队列并不能保证其中的消息也是持久化的，也就是说，如果broker重启了，那么持久化的队列会被自动创建，但是只能那些持久化的消息才会恢复，而没有设置持久化的消息则会丢失。 排它性是指当创建它的连接(connection)关闭时，RMQ将自动将这个队列删除，因此，当这个属性设置为true时，持久化和自动删除两个参数都将被忽略，此时持久化和自动删除都没有意义 持久化是指当broker重启的时候，这个消息队列是否能继续存活 自动删除是指当这个队列不需要时，将会被自动删除 绑定绑定是将消息从exchange路由到queue的规则，其本质是路由规则。在一些exchange中，可能需要用到routingKey(比如direct exchange, topic exchange, default exchange) 消费者消费者可以消费队列中的消息，有两种方式， 一种是broker主动推送(push)，一种是消费者主动拉取(pull)。每个消费者都有个tag，这个tag可以用在取消订阅的时候。 消费者的回执： 有两种方式，一种是broker在发送消息后自动获得ACK操作并将消息删除，一种是消费者显式地进行ACK确认操作，如果某个消息没有得到相应的ACK回执，那么broker将会择机再发送这条消息 消费者也可以通过拒绝某个消息，使得这条消息被重新放到队列中或者被丢弃。 消息消息通常包括两个部分： 属性和负载，属性部分可以认为是消息的元数据，它描述了消息的特性，比如是否需要持久化，routingKey, 有效期, 优先级等等信息，而负载则代表了消息的内容，broker不会对消息的负载进行任何的修改，它会直接透传这部分内容。 AMQP中的方法在AMQP协议， 方法即是操作，不同的操作按照组别被分成多个组，每个组实现相应的功能。比如， exchange.declare用于客户端向broker进行exchange的声明，如果broker声明并创建了相应的exchange之后，就会调用exchange.declare_ok来进行响应。 连接AMQP中的连接通常是长连接，它使用TCP来保证消息投递的可靠性，也可以使用TLS来进行加密，当客户端不需要再连接到broker时，必须正常地关闭相应的连接，而不是突然间中断连接 通道在RMQ中，通常情况下会需要很多的连接，如果每个连接都开启一个TCP连接的话，对于客户端来讲，系统的资源会承受很大的压力。因此可以通过多路复用的技术，在一个连接中保持多个通道，每个通道维持一个到broker的连接，通过共享连接的方式来减小占用系统的资源。通常情况下，各个通道的处理是通过单独开启相应的线程来处理的，换句话说，通道内的数据处理是线程独立的。","categories":[],"tags":[]},{"title":"RabbitMQ基础教程","slug":"消息队列/rabbitMQ/RabbitMQ基础教程","date":"2016-04-08T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/08/消息队列/rabbitMQ/RabbitMQ基础教程/","link":"","permalink":"http://yoursite.com/2016/04/08/消息队列/rabbitMQ/RabbitMQ基础教程/","excerpt":"","text":"RabbitMQ基础教程 生产者-消费者 生产者的流程：ConnectionFactory —&gt; Connection —&gt; Channel —&gt; ExchangeDeclare –&gt; QueueDeclare —&gt; BasicPublish 消费者的流程: ConnectionFactory –&gt; Connection –&gt; Channel –&gt; ExchangeDeclare –&gt; QueueDeclare –&gt; BasicConsume 注: 每个connection都是个TCP连接， 因此为了节省系统资源， 没必要为每个消费者都创建一个连接，但是可以为每个消费者都创建一个channel， 让这些channel共享这个连接，通过这种多路复用的技术，节省系统在连接上所耗费的资源 工作队列 当有多个消费者时，RMQ将会使用round-robin的方式进行分发(轮询分发) 如果需要设置消息的持久化属性，那么必须满足两个条件，一个是消息分发到的队列是持久化的，另一个则是消息本身必须是持久化的 消息确认回执在RMQ中，消息确认回执有两种方式，一种是自动回执(autoAck = true) ，在这种模式下，消息只要一到达队列，队列就自动返回相应的回执给broker， 还有一种是手动回执，队列可以选择发送回执的时机，可以是刚收到消息的时候，也可以是收到消息并处理完后再发送，这种情况下，如果忘记给broker发送回执，那么broker会再次发送这条消息 消息预获取(prefetch)prefetch是指消费者可以预获取的消息条数，prefetch设置的数值越高，消费者用于等待消息到达的时间就越短，因此就可以获得更高的吞吐值，当消费者的prefetch达到上限时，这个消费者将不会再收到broker发来的消息，直到它对之前的消息进行了ack操作之后。这个参数可以用来做负载均衡，比如，可以把消费者的prefetch的值设置为1，并且让每个消费者手工的发送消息回执，这样当某个消费者的处理任务很重时，它将不会再收到broker发来的消息，而那些处理任务很轻的消费者，就可以处理更多的消息，以此来达到负载均衡的目的。 rabbitmqctl的使用rabbitmqctl是用来管理rabbitmq的后台工具，提供了用户管理，连接管理，集群管理，虚拟主机管理 参考链接 参考文档 RMQ系列教程—网易 在线文档","categories":[],"tags":[]},{"title":"AMQP学习","slug":"消息队列/rabbitMQ/AMQP学习","date":"2016-04-07T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/07/消息队列/rabbitMQ/AMQP学习/","link":"","permalink":"http://yoursite.com/2016/04/07/消息队列/rabbitMQ/AMQP学习/","excerpt":"","text":"AMQP学习Topic Exchange的匹配 *: 匹配一个单词 #: 匹配零或多个单词 1e.g. *.stocks.# 能够匹配usd.stocks以及eur.stocks.db，但不能匹配stocks.nasdaq header exchange的匹配 这种类型的exchange在匹配时会忽略routingKey, 它会使用消息的头信息来进行匹配 在头信息中，x-match是个特殊的头,它的值决定了其它头信息的匹配算法，它可以有两种取值， all: 意味着其它的头信息必须全部匹配(and) any: 意味着只要任意头信息匹配了就可以(or) 头信息中某个属性匹配的判定规则： 在绑定的时候，这个属性没有设置值， 在消息的头信息中存在这个属性 在绑定的时候，这个属性设定了相应的值，在消息的头消息中存在这个属性，并且这个属性的值和绑定值相等 所有以x-（除了x-match）开始的头信息是保留字段，都将会被忽略 注意： headers类型的exchange中的头信息不支持通配符！ vhost虚拟主机在RMQ中起到了命名空间的作用，它将不同的用户进行隔离，因此不同的应用可以使用在同一个broker中的不同虚拟主机","categories":[],"tags":[]},{"title":"activeMQ","slug":"消息队列/activeMQ/activeMQ","date":"2016-04-04T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/04/消息队列/activeMQ/activeMQ/","link":"","permalink":"http://yoursite.com/2016/04/04/消息队列/activeMQ/activeMQ/","excerpt":"","text":"AcitveMQ 多路复用: 每个连接可以创建多个会话，只要保证每个会话之间是线程独立的即可，多个会话共享一个连接， 这样可以节省连接的数量 广播: AMQ自带的根据topic进行广播的机制可以相应的功能 监听消息: 消息监听器能够实现异步的消费，同步的消费者通过receive方法来等待消息的到达，而异步的消费者通过注册监听器，当消息到达的时候进行处理，而不需要进行监听 持久化: 持久化分为两个部分，一个是消息的持久化（persistent)，一个是消费者的持久化(durable) 关于persistent和durable语义的说明和比较可以参考： 文章1 文章2 理解: 消息的persistent属性决定了这条消息在broker重启后还否继续存活，而消费者的durable属性决定了当它能否收到处于inactive状态下发送的消息 对于队列而言，消费者的持久化只有一种，也就是durable， 意思是队列的消费者总能收到所有的消息，不管是在它处于active还是inactive状态下的消息; 而对于主题(topic)而言，如果消费者是durable的，那么它就能收到它处于inactive状态下的消息，如果是non-durable， 那么它就只能收到上线后广播的消息，而之前的消息就收不到 消息默认情况下都是persistent状态的，可以通过生产者设置deliveryMode来设置， 也可以在发送的时候来设置，但是不能通过设置消息本身的deliveryMode来实现 主题广播的消费者默认情况下都是non-durable的，可以通过createDurableSubcriber来创建durable的消费者 重发机制： 重发的条件请见： 重发条件 重发属性 查看当前队列里的状态: 只能对队列里的消息进行监听， 通过创建queueBrowser对象，获取到迭代对象，然后就可以遍历队列中的消息，在创建浏览对象的时候，可以设置相应的选择器 massageSelector的作用: 根据内容来进行消息路由，通过给message设置属性，同时给消费者设置选择器，如果两者匹配，则进行路由 Consumer, noLocal: 如果noLocal设置为true, 并且destination为主题模式，那么由同一个连接（注意这里是连接，不是会话）发布的消息不会被路由到这个消费者 Request-Response模式: 感觉这种模式的运用和MOM的设计理念有冲突，因为MOM要实现的目标有两个，一个是应用间的解耦，一个是异步请求，如果做成请求-应答模式，和这两个都有所违背。应答-响应模式通常不能在一个队列中完成，可能会造成自己生产的数据被自己消费的情况， 不过可以使用消息选择器来进行筛选。 durable consumer： 对于队列而言，它的消费者都是durable的，而对于主题广播来讲，可以通过createConsumer或者createDurableSubscriber来分别创建non-durable和durable的消费者durable消费者的作用是能够接收下线期间的广播消息，而non-durable则不行另外，在activeMQ中，创建durable消费者时，必须指定连接的名字和订阅者的名称，订阅者的标识由连接名字和订阅者标识唯一确定 事务机制: 在创建会话的时候，可以选择事务性会话或者是非事务性会话。 过期: activeMQ中使用DLQ(Dead Letter Queue) 来处理过期或者失败的消息，当一个消息被重发超过六次的时候，就会给broker发送poison ack， 这个信号被认为是poison pill， 然后broker就会把这个消息扔到DLQ中，默认情况下，DLQ只会保存持久化的消息，而不会保存非持久化的消息，但是可以通过配置来修改。 消息确认模式：具体的说明可以参见： 消息确认模式 auto_acknowledge: 自动确认，即当消息到达消费者的时候，会话自动对消息进行确认 client_acknowlege: 手动确认，客户端需要显式地对消息进行确认，注意，这里的确认会对之前所有的消息进行确认，而不仅仅是当前这条消息 dups_ok_acknowledge: 这也是一种自动确认模式，只不过它会存在一定的延迟，如果在这期间provider宕机了，可能会造成某些消息重复发送，因此这种确认模式适用于那些可以接受重复发送消息的场景（duplication is ok) session_acknowleged: 这种模式下，消息的确认是通过事务的提交和回滚来进行的，在事务提交时自动进行消息的确认；在事务回滚时立即重新发送 123456注意： 当会话是事务性会话时(transacted = true)，消息确认模式将被自动设置为session_acknowlege，即事务型确认模式，这点在文档中没有很好的说明，但从源码中可以看到&lt;br&gt;在以下两种情况下，在会话结束前，消息都不会被重新发送，消息的redelivery状态仍然为false，从broker的角度来看，这条消息处于Inflight的状态， 只有当会话结束后，如果消息还未被确认，那么这条消息的redelivery才会被标识为true, 意味着这条消息需要被重新发送。 * 在事务型会话中，没有进行commit也没有进行rollback操作 * 在非事务型会话中，且消息确认模式为client_acknowlege，并且没有对消息进行确认 prefetch: 负载均衡 集群 通配符参阅http://activemq.apache.org/wildcards.html . ： 点号用于分隔路径 * ：冒号用于匹配路径中的一节 > ： 大于号用于匹配任意节的路径 与spring的集成 桥接 自定义分发策略 延迟和定时投递 消费者的特性具体的文档说明参见： 特性说明 异步分发： 默认情况下，AMQ采用的是异步分发的策略，这种策略在有低速消费者的情况下十分有用，因为它不会造成生产者的阻塞；如果想要追求更高的吞吐量，并且出现低速消费者的可能性是比较低的，也可以将broker的分发策略修改成同步分发，从而避免在增加新的队列时所需要的同步以及上下文切换的时间成本 。 设置自启动目标(destination)， 可以通过XML配置文件来进行配置，需要AMQ的版本&gt;=4.1 删除不活跃的目标，当某个目标为空的时间超过配置文件预设的时间长度时，broker将自动将这个目标进行删除。 schedulePeriodForDestinationPurge： broker检查不活跃目标的间隔时间 gcInactiveDestinations： 是否删除不活跃目标的标识 inactiveTimoutBeforeGC： 判断目标是否为不活跃目标的时间长度 优先级： 当某个队列的消费者中有高优先级的消费者时，那么broker会优先将消息分发给高优先级的消费者，直到达到它的prefetch的上限时，才会分发给低优先级的消费者(要通过目标属性进行设置） 持久化的主题订阅者： 当某个主题的订阅者当中有持久化需求时，broker必须保存所有在它们离线过程中发来的消息，如果这个订阅者迟迟没有上线，就会导致broker中存储的消息越来越多，最终超过上限值或者导致系统性能下降，因此，必须有一定的机制能够避免出现这种情况 避免囤积大量的消息： 给每个消息设置一定的有效期，过期后消息自动被删除 超过一定时间长度没有上线的订阅者将会被自动删除，通过设置broker的属性即可实现 消息组: 可以通过设置消息JMSXGroup属性来设置该消息的所属组，对于同一个消息组的消息，总是会发送给同一个消费者进行处理，只要这个消费者处于活跃状态，但是如果这个消费者断线了，那么消息将会被另一个消费者消费。通过消息组的机制，可以很方便地实现负载均衡，错误转移以及顺序处理等功能。 AMQ特性的说明 ： AMQ的特性 虚拟目标和组合目标 不同的持久化策略： 文件， 数据库等","categories":[],"tags":[]},{"title":"First Post","slug":"first-post","date":"2016-04-01T08:00:00.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/04/01/first-post/","link":"","permalink":"http://yoursite.com/2016/04/01/first-post/","excerpt":"","text":"”Congratulations!“","categories":[],"tags":[]},{"title":"Java_Concurrency_In_Practise笔记(3) - Composing objects","slug":"多线程/Java_Concurrency_In_Practise笔记(3) - Composing objects","date":"2016-03-24T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/03/24/多线程/Java_Concurrency_In_Practise笔记(3) - Composing objects/","link":"","permalink":"http://yoursite.com/2016/03/24/多线程/Java_Concurrency_In_Practise笔记(3) - Composing objects/","excerpt":"","text":"Java_Concurrency_In_Practise笔记(3) - Composing objects 设计线程安全的类 明确类的状态 明确类状态之间的限制 明确访问类状态时 每个类的所有成员变量组成了这个类的状态，这个类的所有状态组成了相应的状态空间，在状态空间中，如果存在某些无效的状态，那么就要求类有一定的封装来隐藏这些状态空间，否则外部的代码有可能使这个类对象牌处于无效的状态中；另外，如果几个成员变量间有某些不变量，那么对于这些成员变量的操作就必须被原子化，否则可能会破坏这种隐藏的不变量。例如，如果在某个类中一个自然数以及这个自然数的因子，那么隐含的不变量就是这个自然数必须和这些因子匹配，那么针对这个自然数或者因子的变量的操作就必须是原子化的。 类状态变量之间的不变量以及方法的后置条件（比如操作依赖于当前值等）限制了类状态的转移以及有效状态的范围。同样地，方法的前置条件也增加了某些限制。比如，无法从某个空队列中删除元素。在单线程程序中，如果无法满足程序的某些前置条件，那么只要简单地返回错误就可以了，但是在多线程环境下，线程可以等待这些前置条件被满足（其它线程的某些操作使前置条件成立），因此，在多线程编程中提供了wait和notify关键字来实现等待和通知的功能，但是，在使用这两个关键字时，请优先考虑使用已有的框架。比如，阻塞队列，信息量等 实例绑定是最简单地实现线程安全类的方法，具体的做法是使用封装，将一个非线程安全的对象封装到包装对象中，并通过包装对象来控制非线程安全对象的访问，比如HashMap不是个线程安全的集合类，但可以使用平台提供的Collections.synchronizedMap来封装它，从而得到线程安全的hashMap（具体是使用了装饰器模式），只要保证对hashMap对象的访问都是通过封装对象来完成的，那么线程安全就是可以保证的。","categories":[],"tags":[]},{"title":"Java_Concurrency_In_Practise笔记(2)-Sharing Objects","slug":"多线程/Java_Concurrency_In_Practise笔记(2)-Sharing Objects","date":"2016-03-23T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/03/23/多线程/Java_Concurrency_In_Practise笔记(2)-Sharing Objects/","link":"","permalink":"http://yoursite.com/2016/03/23/多线程/Java_Concurrency_In_Practise笔记(2)-Sharing Objects/","excerpt":"","text":"Java_Concurrency_In_Practise笔记(2)-Sharing Objects 锁机制有两方面的作用， 一个是通过定义临界区，提供原子化操作；另一个则是提供了可见性保证。在单线程中，如果对某个变量进行赋值，随后马上进行读取，并且在上次写操作之后没有再进行写操作，那么可以预见的是，读取到的值就是上次赋给它的值，但是在多线程编程中，这种可见性是不能保证的，除非使用了锁，产生这种情况的原因主要是由于指令的重排序。 在JAVA中，对没有声明成volatile的64位数值型变量来讲，JVM允许将对它们的赋值操作转化成两次32位的读写操作，因此，如果需要保证double或者long类型变量的线程安全，就必须把它们声明成volatile或者使用锁机制 volatile类型的变量提供了可见性保证，所有对它的写操作都立即对其它线程可见。如果线程A对某个volatile变量进行赋值，紧接着线程B对这个变量进行读取，那么这个变量的值以及在为这个变量赋值前的所有操作，对线程B都是可见的，可以认为，volatile变量的写操作与随后对它的读操作之间，有happen-before的关系。volatile类型使用起来比锁要方便，但有它的局限性（不提供锁机制），因此通常被用来当作标志变量进行使用。在java中，锁机制同时提供了原子化和可见性保证，但volatile只提供了可见性保证。 对象的发布，比较重要的就一点，不要在构造函数中发布this指针，因为这可能会导致程序获取到不完整的对象。 在多线程编程中，有一种方式能够很好地避免线程安全问题，即每个链接或者每个请求的处理被限制在一个线程中，即线程绑定，也就是说，在单个请求的生命周期中，只会用到一个线程。这种模式在数据库的连接池以及SWING编程中经常被用到。这种方法最常见的是使用ThreadLocal类型的变量，声明为threadLocal类型的变量后，对于每个线程而言，会保留一份该变量的副本，彼此之间不共享数据ThreadLocal和synchronized提供了两种不同的方法来解决多线程的数据安全问题，synchronized是通过锁机制，保证同一时间只有一个线程能获得相应的对象锁，从而实现数据在线程间的共享，而threadlocal恰好相反，它为每个线程保留了一份数据的副本，彼此独立，也就是说并不存在线程间的数据共享的问题，它通过数据的隔离来解决多线程的数据冲突问题 不可变对象 解决多线程编程中数据安全问题的另一个思路。因为几乎所有的多线程数据安全问题，总会涉及到多个线程同时对某个对象进行读写操作，尤其是写操作，如果一个对象在构造函数之后，它的状态就不会发生变化，那么就不会存在数据的冲突问题。另外，把一个类的所有变量都声明成final，并不能保证这个类就是个不可变对象，比如，当某个变量是个引用类型时就无法保证，如果要声明一个类成不可变对象，那么它必须满足以下几个条件： 所有的成员变量都声明成private final， private是为了防止从外部访问修改，final是为了防止从内部进行修改 不提供任何set方法 对于引用类型，在构造函数时要使用深拷贝技术；同样，当向外部返回引用类型的成员变量时，也应该使用深拷贝技术 将类声明成final, 以防止子类继承后重写某些方法 参考资料 http://www.cnblogs.com/dolphin0520/p/3920407.html","categories":[],"tags":[]},{"title":"Spring security的原理","slug":"安全/Spring security的原理","date":"2016-03-22T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/03/22/安全/Spring security的原理/","link":"","permalink":"http://yoursite.com/2016/03/22/安全/Spring security的原理/","excerpt":"","text":"Spring security的原理在使用SS的时候，需要在web.xml里定义DelegatingFilterProxy，这个对象的作用是将托管在spring容器的对象当成filter来使用，它是个代理，所有对这个代理的调用最终都会调用它所代理的对象; 在SS中，delegatingFilterProxy代理的对象就是容器中和filter-name名称一样的那个对象，默认为springSecurityFilterChain, 这个对象的类型是FilterChainProxy，是一堆过滤器的组合 由此可以看出，SS框架通过DelegatingFilterProxy这个对象引入了整个过滤器链，认证和授权过程中需要用到的所有东西都由这个过滤器链完成，并且，这个过滤器链也是可以自定义的 当访问系统中某个受保护的资源时，会依次通过这个过滤器链，从而引发整个权限操作。 1. 认证认证操作会在两种情况下被触发，一种是直接进入到认证页面，一种是访问受保护的资源但用户本身没有被认证的时候。认证的过程大体上有以下几个步骤： AuthenticationManager —&gt; ProviderManager —&gt; AuthenticationProvider —&gt; DaoAuthenticationProvider(UserDetailsService) —&gt; UserDetailsService —&gt; UserDetails 2. 授权首先通过过滤器链进入到FilterSecurityInterceptor, 在FilterSecurityInterceptor中的整个工作过程可以参考: 这里 过滤器链中默认的过滤器有：这里， 它们的详细说明如下： 另外， 在SS的过滤器链中，过滤器之间的顺序也是很关键的，如下如述： The order that filters are defined in the chain is very important. Irrespective of which filters you are actually using, the order should be as follows: ChannelProcessingFilter, because it might need to redirect to a different protocol SecurityContextPersistenceFilter, so a SecurityContext can be set up in the SecurityContextHolder at the beginning of a web request, and any changes to the SecurityContextcan be copied to the HttpSession when the web request ends (ready for use with the next web request) ConcurrentSessionFilter, because it uses the SecurityContextHolder functionality but needs to update the SessionRegistry to reflect ongoing requests from the principal Authentication processing mechanisms - UsernamePasswordAuthenticationFilter, CasAuthenticationFilter, BasicAuthenticationFilter etc - so that the SecurityContextHolder can be modified to contain a valid Authentication request token The SecurityContextHolderAwareRequestFilter, if you are using it to install a Spring Security aware HttpServletRequestWrapper into your servlet container RememberMeAuthenticationFilter, so that if no earlier authentication processing mechanism updated the SecurityContextHolder, and the request presents a cookie that enables remember-me services to take place, a suitable remembered Authentication object will be put there AnonymousAuthenticationFilter, so that if no earlier authentication processing mechanism updated the SecurityContextHolder, an anonymous Authentication object will be put there ExceptionTranslationFilter, to catch any Spring Security exceptions so that either an HTTP error response can be returned or an appropriate AuthenticationEntryPoint can be launched FilterSecurityInterceptor, to protect web URIs and raise exceptions when access is denied 3. 命名空间的配置 http: 配置URL级别的权限信息 global-method-security: 配置方法级别的权限信息 authentication-manager: 配置认证信息 4. 参考资料 https://docs.spring.io/spring-security/site/docs/3.0.x/reference/el-access.html http://docs.spring.io/spring-security/site/docs/3.0.x/reference/appendix-namespace.html 使用Spring Security实现Resource Based Access Control的方法 URL级别的权限控制是通过标签来控制的，可以通过自定义voter来实现，注意要将http元素的use-expression设置为false(默认为true)，否则系统将使用默认的表达式解析，导致解析失败 方法级别的权限控制通过global-method-security来控制， 需要通过自定义PermissonEvaluator来重新实现hasPermission方法，并将它设置DefaultMethodSecurityExpressionHandler的属性，然后设置到global-method-security中，以此来实现自定义的权限管理 在平台中引入权限控制的时候，需要先定义平台的资源以及能对资源执行的操作，也可以定义一组角色，然后逐一确定每个角色对各个资源的权限项，并将它配置到DB中或者代码中","categories":[],"tags":[]},{"title":"Java_Concurrency_In_Practise笔记(1)-Thread safety","slug":"多线程/Java_Concurrency_In_Practise笔记(1)-Thread safety","date":"2016-03-22T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/03/22/多线程/Java_Concurrency_In_Practise笔记(1)-Thread safety/","link":"","permalink":"http://yoursite.com/2016/03/22/多线程/Java_Concurrency_In_Practise笔记(1)-Thread safety/","excerpt":"","text":"Java_Concurrency_In_Practise笔记(1)-Thread safety使用多线程编程时会遇到的问题 数据一致性问题: nothing bad ever happen Liveness Failure: something good eventually happens, deadlock, starvation, livelock 性能问题：多线程时，CPU需要在不同的上下文之间进行切换，线程间的同步操作等也会耗费一些CPU时间 数据一致性编写线程安全的代码的本质是：管理好对那些可共享，可变状态的读写操作 任何情况下，如果多个线程会同时对某个状态访问，并且其中至少一个线程会对状态进行修改，那么它们就必须通过某些机制来保证数据的一致性，这也是判断某个对象是否是线程安全的依据 1. Race condition出现数据一致性的情况基本上都具备了上述的两个条件：同时有多个线程对对象的状态进行访问，并且其中至少一个线程会对状态进行修改 竞争条件指的是两个或多个线程读写某些共享数据，而最后的结果取决于线程的精确执行时序，这种情况在多线程中非常常见。 check-and-act：执行的操作取决于某个观察到的先决条件，但当开始act(act并不一定是写操作，也可以是读操作）的时候，之前观察到的先决条件已经失效 123e.g. 检查到文件系统中没有文件X，于是开始创建X，但是当开始创建X时，另一个线程已经在文件系统中创建了Xe.g. lazy initialization: 单例模式 read-modify-write: 由于这种操作的结果依赖于之前的操作，当写操作开始的时候，可能之前读取到的状态值已经发生变化 1e.g. i++ 2. 解决办法解决数据的一致性问题也可以通过两个方面来进行：控制同一时间访问状态的线程数量，或者让状态变为只读变量，甚至可以将类实现为无状态类，无状态类天生就是线程安全的 在JAVA中，同步的机制包括：synchronized， volatile，原子化变量，显式锁（这些方法本质上都是限制了同一时间只能有一个线程能操作对象的状态） 2.1 原子化变量对原子化变量的所有操作都是原子化的，从内存的角度来看，基本上同操作volatile变量是一样的（对volatile变量的写操作与随后对该变量的读操作之间有happen-before关系），因此这些操作都是线程安全的 2.2 synchronizedsynchronized由两个部分组成，一个是获取到的锁对象，另一个是锁的代码块 synchronized方法是synchronized代码块的一种简略表达，它包含方法中所有的代码块，锁的对象是调用该方法的那个对象；如果是对static方法加synchronized关键字，那么锁的对象的是该方法所在的类 JAVA中的每个对象都有内置锁，当线程进入synchronized区域时，自动得到某个对象（synchronized后的对象）的内置锁，并在退出synchronized代码块时自动释放该对象锁，不管这种退出是正常的执行结束还是由于代码出现异常 2.3 重入锁当一个线程试图获取某个锁时，如果这个锁已经被其它的线程所占有，那么该线程会阻塞，但如果拥有这个锁的线程就是它本身，那么它可以重复地获取锁，这种机制被称为是重入锁，这种重入锁是通过记录拥有锁的线程对象以及获取锁的次数来实现的。 2.4 利用锁实现状态的统一这里的状态并不一定是指某个变量，也可能是几个变量组合成的状态，在读写这些状态的时候，需要通过锁机制（不管是synchronized关键字还是Lock机制）保证同一时间只有一个线程在对它们进行操作 另外，对某个类中所有的方法加上synchronized关键字并不能保证这个类的使用中就是线程安全的，如 123if(!a.containsElement(b))&#123; a.addElement(b);&#125; 这是个典型的check-and-act代码；在这段代码中，即使这两个方法都是原子操作，也不能保证这两个操作是原子的。 Liveness","categories":[],"tags":[]},{"title":"JS基础 - 高阶函数","slug":"前端/eloquent_js/JS基础---高阶函数","date":"2016-02-05T12:12:00.000Z","updated":"2017-02-06T00:20:12.000Z","comments":true,"path":"2016/02/05/前端/eloquent_js/JS基础---高阶函数/","link":"","permalink":"http://yoursite.com/2016/02/05/前端/eloquent_js/JS基础---高阶函数/","excerpt":"","text":"","categories":[],"tags":[{"name":"js","slug":"js","permalink":"http://yoursite.com/tags/js/"},{"name":"function","slug":"function","permalink":"http://yoursite.com/tags/function/"}]},{"title":"IO & NIO & NIO2","slug":"IO/java基础/IO & NIO & NIO2","date":"2016-02-04T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/02/04/IO/java基础/IO & NIO & NIO2/","link":"","permalink":"http://yoursite.com/2016/02/04/IO/java基础/IO & NIO & NIO2/","excerpt":"","text":"IO &amp; NIO &amp; NIO2IO Inputstream/OutputStream: 流对象，用于字节读写 Reader/Writer: 用于字符读写 DataInput/DataOutput: 用于二进制读写 ObjectInput/ObjectOutput: 用于对象读写 Scanner/Formatter: 用于读取格式化的字符 NIO Path: 从语法上对系统中的文件和文件夹的抽象 Files: 封装对文件的操作，比如对文件的增删改查，拷贝，重命名，读取文件属性等操作 Paths: 获取path对象的工具类 FileAttribute: 文件属性 WatchService: 监听文件/文件夹增删改查 FileSystem/FileStore: 代表文件系统和磁盘 NIO2 ByteBuffer: 字节缓冲区 Channel: 通道，用于从缓冲区读取数据或者向缓冲区写入数据 Selector: 多路复用选择器 练习代码 git@github.com:Essviv/nio.git","categories":[],"tags":[]},{"title":"css","slug":"doc/css","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/css/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/css/","excerpt":"","text":"#Selector ###简单选择器 元素选择器 h1 类选择器 .className h1.className ID选择器 #idName h1#idName 属性选择器 [propName]: 拥有某个属性的对象 h1[propName][propName]: 同时拥有两个属性 简单选择器分组 h1, p h2, li ###复合选择器 兄弟选择器 p + p li + li 后代选择器(包括多代子元素） p em h1 em 子元素（直接子元素） p &gt; h1 p &gt; em","categories":[],"tags":[]},{"title":"java_collection","slug":"doc/java_collection","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/java_collection/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/java_collection/","excerpt":"","text":"#Java collection frameworkThe core collection interfaces are the foundation of the Java Collections Framework. The Java Collections Framework hierarchy consists of two distinct interface trees: The first tree starts with the Collection interface, which provides for the basic functionality used by all collections, such as add and remove methods. Its subinterfaces — Set, List, and Queue — provide for more specialized collections. The Set interface does not allow duplicate elements. This can be useful for storing collections such as a deck of cards or student records. The Set interface has a subinterface, SortedSet, that provides for ordering of elements in the set. The List interface provides for an ordered collection, for situations in which you need precise control over where each element is inserted. You can retrieve elements from a List by their exact position. The Queue interface enables additional insertion, extraction, and inspection operations. Elements in a Queue are typically ordered in on a FIFO basis. The Deque interface enables insertion, deletion, and inspection operations at both the ends. Elements in a Deque can be used in both LIFO and FIFO. The second tree starts with the Map interface, which maps keys and values similar to a Hashtable. Map’s subinterface, SortedMap, maintains its key-value pairs in ascending order or in an order specified by a Comparator. These interfaces allow collections to be manipulated independently of the details of their representation. ##Collection ###List ArrayList: 内部使用动态数组来实现，当调用add/remove等方法时，会拷贝数组 LinkedList: 内部使用双向链表实现，当调用set/get等方法时，需要遍历 ###Set HashSet: 内部使用hashMap来实现, 利用其key值不能重复的特点 LinkedHashSet: 内部使用LinkedHashMap来实现，它的特点是可以按插入顺序遍历 TreeSet: 内部使用TreeMap来实现，它保证set处于排序状态 SortedSet: 集合内部的元素可以按照一定的顺序进行排序，内部通常借助sortedMap实现 可以看出，set的实现大多依赖于Map, 不同的set实现主要是由于内部使用的map类型不同 ###Queue BlockingQueue: 阻塞式队列，包括ArrayBlockingQueue和LinkedBlockingQueue Deque: 双向队列，可以用来实现栈, ArrayDeque, LinkedList BlockingDeque: 阻塞式双向队列, LinkedBlockingDeque ##Map HashMap: 使用key-value的值进行存储， 这是非同步的(Hashtable是同步的) LinkedHashMap: 使用链表的方式进行存储，它可以按插入的顺序进行输出 TreeMap: 它可以按照指定的比较器进行排序输出 SortedMap: 按照一定的比较器对结果进行输出，TreeMap是它的一种实现 ConcurrentMap: map的多线程实现，它将map分成多个（默认是16个）hashtable，在每个hashtable中可同时进行操作 ###Arrays asList: 将数组转化成List对象 binarySearch: 二分查找 sort: 排序 fill: 填充 copyOf: 拷贝 ###Collections Sorting shuffling routing data searching composition","categories":[],"tags":[]},{"title":"id_auto_increment_accross_tables","slug":"doc/id_auto_increment_accross_tables","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/id_auto_increment_accross_tables/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/id_auto_increment_accross_tables/","excerpt":"","text":"#分表情况下的用户ID自增策略 总得来讲，在数据库中记录当前已经生成的用户ID数，缓存启动时，读取这个值并将它增加一定的数量（100，1000),然后在缓存中预先生成相应数量的ID值，当应用层请求生成用户ID时，直接从缓存中取出预先生成的值，若缓存里的值被取完，继续更新数据库，生成新的ID值 ##几点考虑 缓存系统崩溃：此时只需要重新启动即可，缓存会根据数据库中记录的值生成新一批ID，不会出现重复的情况，不过这种情况下，整个系统会丢失缓存崩溃时遗留在缓存系统中的那批ID 效率考虑：由于缓存系统每次总是从数据库里更新并生成一定数量的ID，当应用层请求生成ID时，是从缓存里直接读取预先生成好的ID，比起每次都读取再更新数据库，效率要高出很多 每次生成ID的数量：如果每次生成的数量很多，那么后续取ID时效率会很高，但相应地，如果缓存系统崩溃，丢失大量未使用ID的概率也更大，因此更适合于短时间内生成大量ID的场景;相反，如果每次生成的数量很少，会导致请求数据库的次数增加（极端情况，将每次生成的ID数量设成1，那么就和直接操作数据库没有区别），影响效率，但可以有效地避免丢失ID的情况。 注意： 更新数据库的时候，可以考虑采用乐观锁，防止多线程情况下更新数据库导致相同的ID值出现 Jedis jedis = newJedis(); String value = jedis.spop(KEY); while (StringUtils.isBlank(value)) { updateCacheAndDB(jedis); value = jedis.spop(KEY); } try { return NumberUtils.toLong(value); } finally { if (jedis != null) { jedis.close(); } }","categories":[],"tags":[]},{"title":"CXF","slug":"SOA/CXF","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/SOA/CXF/","link":"","permalink":"http://yoursite.com/2016/01/25/SOA/CXF/","excerpt":"","text":"CXF根据WSDL生成客户端的方式有以下几种： wsdl2java自动生成代码 jax-ws代理： 通过service.create来创建相应的客户端代码. CXF与spring的集成 在beans中定义server和client，如下, 其中id为spring中标识，implementor为服务的实现类， address为定义的服务地址 123&lt;jaxws:endpoint id=\"helloWorld\" implementor=\"com.cmcc.syw.cxf.HelloWorldImpl\" address=\"/helloWorld\"/&gt;&lt;jaxws:client id=\"helloWorldClient\" serviceClass=\"com.cmcc.syw.cxf.HelloWorld\" address=\"http://localhost:8080/spring/ws/helloWorld\"/&gt; 在web.xml中定义cxf-servlet，如下： 123456789&lt;servlet&gt; &lt;servlet-name&gt;cxf&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.cxf.transport.servlet.CXFServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;cxf&lt;/servlet-name&gt; &lt;url-pattern&gt;/ws/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; HelloWorld服务的类定义为： 12345678910111213@WebService@Features(features = \"org.apache.cxf.feature.LoggingFeature\")public interface HelloWorld &#123; String sayHi(@WebParam(name = \"name\") String name);&#125;@WebService(endpointInterface = \"com.cmcc.syw.cxf.HelloWorld\")public class HelloWorldImpl implements HelloWorld &#123; @Override public String sayHi(String name) &#123; return \"Hi, \" + name; &#125;&#125; ​ 对于只有wsdl文档，或者得不到服务源码的场景，可以通过工具自动生成客户端代码，然后再和spring进行集成. 注解 @Features: 提供了类似于过滤器的功能, CXF框架提供的feature列表参见http://cxf.apache.org/docs/featureslist.html, 包括故障转移、日志等等 @InInterceptors, @OutInterceptors, @InFaultInterceptors, @OutFaultInterceptors: 拦截器功能在进行消息处理时，CXF会自动创建相应的拦截器链，对于从客户端发往服务端的请求，CXF会自动在客户端创建一个输出的拦截器链，同时在服务端创建一个输入的拦截器链. 每个拦截器链又可以进一步分为多个phase(相位）, 每个拦截器属于哪个phase由程序在构造它的时候指定. 输入输出拦截器链的具体phase可以参阅http://cxf.apache.org/docs/interceptors.html.在cxf, 拦截器功能主要是由InterceptorProvider接口提供的. 实现这个接口的组件包括Client, Service, EndPoint, Bus以及Binding. @DataBinding: 指定数据绑定的方法，默认情况下是jaxb @Logging: 打开端点的日志开关，可以设置上限值和输出日志的位置. @GZip: 指定将输入输出的信息进去压缩，可以指定超过一定大小的流进行压缩或者强制压缩. 参考文档 中文文档 官方文档","categories":[],"tags":[]},{"title":"hibernate","slug":"doc/hibernate","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/hibernate/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/hibernate/","excerpt":"","text":"[TOC] #Hibernate ##OverviewHibernate在应用程序和数据库之间增加了一层，用于管理POJO与DB之间的映射关系, 如下图所示 ##Bootstrap progress ###Hibernate native bootstrap ServiceRegistryBuilder –&gt; ServiceRegistry –&gt; MetaSources –&gt; MetadataBuilder –&gt; Metadata –&gt; SessionFactoryBuilder –&gt; SessionFactory –&gt; Session –&gt; Transaction ——&gt; transaction save/commit The native bootstrapping API is quite flexible, but in most cases it makes the most sense to think of it as a 3 step process: Build the StandardServiceRegistry Build the Metadata Use those 2 things to build the SessionFactory ###JPA-compliant bootstrap container-bootstrapping For compliant container-bootstrapping, the container will build an EntityManagerFactory for each persistent-unit defined in the deployment’s META-INF/persistence.xml and make that available to the application for injection via the javax.persistence.PersistenceUnit annotation or via JNDI lookup. application-bootstrapping For compliant application-bootstrapping, rather than the container building the EntityManagerFactory for the application, the application builds the EntityManagerFactory itself using the javax.persistence.Persistence bootstrap class. The application creates an entity manager factory by calling the createEntityManagerFactory method. Persistence contextEntity states transient - the entity has just been instantiated and is not associated with a persistence context. It has no persistent transient - the entity has just been instantiated and is not associated with a persistence context. It has no persistent representation in the database and typically no identifier value has been assigned. managed, or persistent - the entity has an associated identifier and is associated with a persistence context. It may or may not physically exist in the database yet. detached - the entity has an associated identifier, but is no longer associated with a persistence context (usually because the persistence context was closed or the instance was evicted from the context) removed - the entity has an associated identifier and is associated with a persistence context, however it is scheduled for removal from the database. ###Change states save/persist delete/remove getReference, load refresh(database –&gt; context) flush saveOrUpdate merge ##Database Access ###Dialect Although SQL is relatively standardized, each database vendor uses a subset and superset of ANSI SQL defined syntax. This is referred to as the database’s dialect. ##锁机制 ###乐观锁 @Version int, or Integer short, or Short long, or Long java.sql.Timestamp @OptimisticLocking NONE VERSION(default) DIRTY ALL ###悲观锁通过JDBC的锁机制来实现，用户只需要指定锁的级别即可 ##拦截器与事件 ###拦截器Interceptor: 可以在session域，也可以在sessionFactory域，不过在sessionFactory域的拦截器需要考虑线程安全问题 ###事件所有定义的事件都有相应的监听器，业务层通过实现这些监听器接口来实现监听的功能，注意监听器的实现必须是无状态 ###回调暂时没看明白…. ##HQL ###select select_statement :: = [select_clause] from_clause [where_clause] [groupby_clause] [having_clause] [orderby_clause] e.g. from User ###update update_statement ::= update_clause [where_clause] update_clause ::= UPDATE entity_name [[AS] identification_variable] SET update_item {, update_item}* update_item ::= [identification_variable.]{state_field | single_valued_object_field} = new_value new_value ::= scalar_expression | simple_entity_expression | NULL e.g. update Customer c set c.name = &quot;sunyiwei&quot;, c.age = 27 where c.id = 27 update versioned Customer c set c.name = &quot;patrick&quot; where c.id = 27 //version ###delete delete_statement ::= delete_clause [where_clause]delete_clause ::= DELETE FROM entity_name [[AS] identification_variable]e.g. delete from Custom c where c.id = 27 ###insert(HQL only) insert_statement ::= insert_clause select_statementinsert_clause ::= INSERT INTO entity_name (attribute_list)attribute_list ::= state_field[, state_field ]* e.g.insert into Custom c (name, age) select name, age from Custom where id=27 ###from ####1. identification variables(alias) ####2. root entity reference ####3. explicit join: [left join, inner join] fetch sth with someCondition ####4. implicit join e.g.select cfrom Customer cwhere c.chiefExecutive.age &lt; 25 // same asselect cfrom Customer c inner join c.chiefExecutive ceowhere ceo.age &lt; 25 ===========&gt; 13.4 ##类型映射 ###基本类型 @Basic: 基本上可以不用 @Column: 当默认的列名不符合需要时，可以通过这个注解来进行修改 ###枚举类型 @Enumerated ORDINAL: 整型数据，按enum中的ordinal来映射 NAME: 字符串类型，按enum中的name来映射 AttributeConverter + @Converter ###复合类型 @Embeddable @Embedded 多个复合类型时，会遇到类型匹配的问题，此时需要引入@AttributeOverride来解决，具体可以查看AttributeOverride的注解 ###容器类型文档中没有做详细的描述，这块还需要进一步阅读其它文档来补充 ###标识符 简单类型 According to JPA only the following types should be used as identifier attribute types: any Java primitive type any primitive wrapper type java.lang.String java.util.Date (TemporalType#DATE) java.sql.Date java.math.BigDecimal java.math.BigInteger 复杂类型: 说明 @EmbeddedId @IdClass 自动产生ID的策略: GenerationType AUTO (顺序为: SequenctGenerator –&gt; TableGenerator –&gt; GenericGenerator) IDENTITY SEQUENCE TABLE","categories":[],"tags":[]},{"title":"memo","slug":"doc/memo","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/memo/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/memo/","excerpt":"","text":"#Memo 按天查询收入（开始时间-结束时间) 通过手机号查询用户信息 流量充值 创建APP接口 JIRA上的问题 白条","categories":[],"tags":[]},{"title":"useful_website","slug":"doc/useful_website","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/useful_website/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/useful_website/","excerpt":"","text":"#Some useful websites www.owasp.org: open web application security project.","categories":[],"tags":[]},{"title":"easyMock","slug":"测试/easyMock","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/测试/easyMock/","link":"","permalink":"http://yoursite.com/2016/01/25/测试/easyMock/","excerpt":"","text":"EasyMock参考文献 EasyMock测试入门文章 EasyMock讲解文章 http://www.ibm.com/developerworks/java/library/j-easymock/index.html https://effectivejavatesting.wordpress.com/2008/11/25/beware-easymock/ 官方的user-guide partial class mock EasyMock测试大体分为: record, replay, verify三个步骤","categories":[],"tags":[]},{"title":"memcached","slug":"doc/memcached","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/memcached/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/memcached/","excerpt":"","text":"#memcached ##commandsset, add, replace, append, prepend, cas, get, gets, flush_all, delete, incr, decr, touch ##References官方文档 命令说明 turtorialPoint","categories":[],"tags":[]},{"title":"SQL","slug":"doc/SQL","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/SQL/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/SQL/","excerpt":"","text":"#SQL 字符类型的长度的意义 浮点数的精度问题 计算机中浮点数的实现大都遵从 IEEE754 标准，IEEE754 规定了单精度浮点数和双精度 浮点数两种规格，单精度浮点数用4字节（32bit）表示浮点数，格式是： 1位符号位，8位表示指数，23位表示尾数 双精度浮点数8字节（64bit）表示实数，格式是： 1位符号位 11位表示指数 52位表示尾数 同时，IEEE754标准还对尾数的格式做了规范：d.dddddd...，小数点左面只有1位且不能为零，计算机内部是二进制，因此，尾数小数点左面部分总是1。显然，这个1可以省去，以提高尾数的精度。由上可知，单精度浮点数的尾数是用24bit表示的，双精度浮点数的尾数是用53bit表示的，转换成十进制： 2^24 - 1 = 16777215； 2^53 - 1 = 9007199254740991 由上可见，IEEE754单精度浮点数的有效数字二进制是24位，按十进制来说，是8位；双精度浮点数的有效数字二进制是53位，按十进制来说，是16 位。 日期类型: date, time, datetime, timestamp(自动更新时间) 数据库表设计的三范式： 1st NF(原子性) Columns with similar content must be eliminated A table must be created for a group of associated data Each data record must be identifiable by means of a primary key 2nd NF(非主属性非部分依赖于主属性） Whenever the contents of columns repeat themselves, this means that the table must be divided into subtables. These tables must be linked by foreign keys 3rd NF(属性不依赖于其它非主属性） Columns that are not directly related to primary key must be eliminated(that is, must be translated into a table of their own)","categories":[],"tags":[]},{"title":"Spring resource","slug":"安全/Spring resource","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/安全/Spring resource/","link":"","permalink":"http://yoursite.com/2016/01/25/安全/Spring resource/","excerpt":"","text":"Spring Resource##1. URL与URI的区别 URIs identify and URLs locate. 参考1 参考2 URI是个纯粹的句法结构，用于指定标识Web资源的字符串的各个不同部分。URL是URI的一个特例，它包含了定位Web资源的足够信息。其他URI，比如： mailto: cay@horstman.com 则不属于定位符，因为根据该标识符无法定位任何资源。像这样的URI我们称之为URN(统一资源名称)。 在Java类库中，URI类不包含任何访问资源的方法，它唯一的作用就是解析。相反的是，URL类可以打开一个到达资源的流。因此URL类只能作用于那些Java类库知道该如何处理的模式，例如：http：，https：，ftp:，本地文件系统(file:)，和Jar文件(jar:)。 2. Resource接口的具体实现 URLResource: 用于访问文件系统(file)、http文件(http)以及ftp文件(ftp) ClassPathResource: 用于加载classpath中的文件 FileSystemResource: 用于访问文件系统和url文件 ServletContenxtResource: 访问相对当前servletContext根路径的文件 3. ResourceLoader和ResourceLoaderAware接口4. 加载applicationContext时指定resource的路径的注意事项参考文献链接","categories":[],"tags":[]},{"title":"常见的线程模型","slug":"多线程/常见的线程模型","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/多线程/常见的线程模型/","link":"","permalink":"http://yoursite.com/2016/01/25/多线程/常见的线程模型/","excerpt":"","text":"常见的线程模型参考文献 http://blog.csdn.net/solstice/article/details/5307710","categories":[],"tags":[]},{"title":"RESTful Spring","slug":"doc/RESTful Spring","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/doc/RESTful Spring/","link":"","permalink":"http://yoursite.com/2016/01/25/doc/RESTful Spring/","excerpt":"","text":"#Spring中的REST视图 在spring中实现RESTful风格的视图有两种方法： ContentNegotiatingViewResolver: 可以同时支持RESTful风格和视图风格 RequestBody/ResponseBody+HttpMessageConverter： 只能支持RESTful格式 ##ContentNegotiatingViewResolverViewResolver接口的作用就是将返回的视图名称转化成某一个特定的View类(ViewResolver List)，而ContentNegotiatingViewResolver就是其中一种 ContentNegotiatingViewResolver首先通过ContentNegotiationManager来确定请求的mediaType(后者其实也是通过一系列的ContentNegotiationStrategy来确定的), 然后请求每个代理ViewResolver获取View对象，在这个过程中，viewResolver必须支持请求中的mediaType,如果所有viewResolver都没有返回view，可以通过defaultViews来替代代理VR返回的View The ContentNegotiatingViewResolver does not resolve views itself, but delegates to other ViewResolvers. By default, these other view resolvers are picked up automatically from the application context, though they can also be set explicitly by using the viewResolvers property. Note that in order for this view resolver to work properly, the order property needs to be set to a higher precedence than the others (the default is Ordered.HIGHEST_PRECEDENCE). ContentNegotiatingViewResolver -(1)-&gt; ContentNegotiationManager -(2)-&gt; ContentNegotiationStrategy -(3)-&gt; MediaType -(4)-&gt; ViewResolvers -(5)-&gt; View -(6)-&gt; defaultViews &lt;!-- contentNegotiatingViewResolver --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.ContentNegotiatingViewResolver&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;1&quot;/&gt; &lt;!-- default views --&gt; &lt;property name=&quot;defaultViews&quot;&gt; &lt;list&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.xml.MappingJackson2XmlView&quot;/&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.json.MappingJackson2JsonView&quot;/&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- contentNegotiationManager --&gt; &lt;property name=&quot;contentNegotiationManager&quot;&gt; &lt;bean class=&quot;org.springframework.web.accept.ContentNegotiationManagerFactoryBean&quot;&gt; &lt;property name=&quot;mediaTypes&quot;&gt; &lt;map&gt; &lt;entry key=&quot;xml&quot; value=&quot;application/xml&quot;/&gt; &lt;entry key=&quot;json&quot; value=&quot;application/json&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- velocity view resolver --&gt; &lt;bean class=&quot;org.springframework.web.servlet.view.velocity.VelocityConfigurer&quot;&gt; &lt;property name=&quot;resourceLoaderPath&quot; value=&quot;/WEB-INF/views&quot;/&gt; &lt;property name=&quot;velocityProperties&quot;&gt; &lt;props&gt; &lt;prop key=&quot;input.encoding&quot;&gt;UTF-8&lt;/prop&gt; &lt;prop key=&quot;output.encoding&quot;&gt;UTF-8&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;velocityLayoutVR&quot; class=&quot;org.springframework.web.servlet.view.velocity.VelocityViewResolver&quot;&gt; &lt;property name=&quot;order&quot; value=&quot;2&quot;/&gt; &lt;property name=&quot;suffix&quot; value=&quot;.vm&quot;/&gt; &lt;property name=&quot;cache&quot; value=&quot;false&quot;/&gt; &lt;property name=&quot;contentType&quot; value=&quot;text/html;charset=UTF-8&quot;/&gt; &lt;property name=&quot;requestContextAttribute&quot; value=&quot;rc&quot;/&gt; &lt;property name=&quot;exposeRequestAttributes&quot; value=&quot;true&quot;/&gt; &lt;/bean&gt; (1)CNVR将解析请求的MediaType的操作委托给CNM (2)CNM又将解析请求的MediaType的操作委托给CNS (3)CNS解析MediaType (4)(5)CNVR委托ViewResolver利用方法返回的ViewName和解析得来的MediaType获取View (6)如果所有的ViewResovler都没有解析到view，则使用CNVR中的defaultViews设置（前提是这些views能够支持相应的mediaTypes) ##RequestBody/ResponseBody + HttpMessageConverterRequestBody注解用于说明方法的参数来自请求的信息体，类似地，ResponseBody注解用于说明返回的对象被存放于回复的消息体中，HttpMessageConverter接口是专门设计来将请求信息和回复信息转化成对象的接口 HttpMessageConverter is responsible for converting from the HTTP request message to an object and converting from an object to the HTTP response body The @RequestBody method parameter annotation indicates that a method parameter should be bound to the value of the HTTP request body. The @ResponseBody annotation is similar to @RequestBody. This annotation can be put on a method and indicates that the return type should be written straight to the HTTP response body (and not placed in a Model, or interpreted as a view name). 具体的实现逻辑可以查看RequestResponseBodyMethodProcessor的源码，大体思路如下： request --&gt; requestMediaTypes --&gt; httpConverter supported mediaTypes --&gt; compatible mediaTypes --&gt; selectedMediaTypes --&gt; (第一个支持这种返回类型的)httpMessageConverter.write &lt;!-- marshaller --&gt; &lt;bean id=&quot;marshaller&quot; class=&quot;org.springframework.oxm.jaxb.Jaxb2Marshaller&quot;&gt; &lt;property name=&quot;packagesToScan&quot; value=&quot;com.cmcc.zypt.model&quot;/&gt; &lt;/bean&gt; &lt;!-- httpMessageConverter --&gt; &lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;org.springframework.http.converter.json.GsonHttpMessageConverter&quot;/&gt; &lt;bean class=&quot;org.springframework.http.converter.xml.MarshallingHttpMessageConverter&quot;&gt; &lt;property name=&quot;marshaller&quot; ref=&quot;marshaller&quot;/&gt; &lt;property name=&quot;unmarshaller&quot; ref=&quot;marshaller&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.http.converter.StringHttpMessageConverter&quot;/&gt; &lt;/mvc:message-converters&gt; &lt;/mvc:annotation-driven&gt; ##说明 使用时，默认支持XML和JSON，前提是必须提供相应的JSON包(jackson)和XML包(jaxb)","categories":[],"tags":[]},{"title":"shiro学习笔记","slug":"安全/shiro学习笔记","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/安全/shiro学习笔记/","link":"","permalink":"http://yoursite.com/2016/01/25/安全/shiro学习笔记/","excerpt":"","text":"Shiro##Terminology参考1 参考2注意realm, principal, credential的理解 ##Architecture ##SecurityManager ###GraphAuthenticator, Authorizator, SessionManagement, SessionDAO, CacheManagement ###Programmic configurationsetter/getter method of securityManager ###INI configurationload file from file, url and classpath with prefix “file”, “url” and “classpath”, respectively. ###AuthenticatorAuthenticationToken, AuthenticationException, RememberMe, AuthencationStrategy, Realm ###Authorization RBAC: implicit or explicit Permission-based: Permission, String Annotation-based: Requires AOP support. @RequiresAuthentication @RequiesGuest @RequirePermissions @RequiresRoles @RequiresUser JSP-based Global permission resolver: convert string to implicit permission RolePermissionResolver: convert role to implicit permission ###Permission syntax Simple usage queryPrinter printPrinter Multipart: domain: action1, action2 printer:query printer:print printer: query, print printer:* instance-level access control: domain, action, instance being acted upon printer:query:lp7200 printer:print:canon printer:*:lp7200 print:query, print: canon missing part: missing parts imply that the user has access to all values corresponding to that part. printer:print &lt;==&gt; printer:print:* printer &lt;==&gt; printer:*:* BUT printer:lp7200 IS NOT EQUIVALENT TO printer:*:lp7200 ###Realm supports getAuthenticationInfo GetAuthenticationInfo的实现逻辑: Inspects the token for the identifying principal (account identifying information) Based on the principal, looks up corresponding account data in the data source Ensures that the token’s supplied credentials matches those stored in the data store If the credentials match, an AuthenticationInfo instance is returned that encapsulates the account data in a format Shiro understands If the credentials DO NOT match, an AuthenticationException is thrown CredentialMatcher SaltedAuthenticationInfo ###Session and SessionManager SessionListener: listen to important session events SessionManager, SessionDAO: session store CacheManager, Cache, Session SessionValidationScheduler ##Web support ###Configuration org.apache.shiro.web.env.EnvironmentLoaderListener &lt;filter&gt; &lt;filter-name&gt;ShiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.shiro.web.servlet.ShiroFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;ShiroFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt; &lt;dispatcher&gt;INCLUDE&lt;/dispatcher&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt; &lt;/filter-mapping&gt; ###INI configuration[main], [users], [roles], [urls] [urls] URL_Ant_Path_Expression = Path_Specific_Filter_Chain filter_chain: filter1[optional_config1], filter2[optional_config2] This line below states that &quot;Any request to my application&apos;s path of /account or any of it&apos;s sub paths (/account/foo, /account/bar/baz, etc) will trigger the &apos;ssl, authc&apos; filter chain&quot;. /account/** = ssl, authc ###Integration with SpringRef ##CryptographyHashService, PasswordService, CredentialsMatcher ##权限注解 @ RequiresAuthentication可以用户类/属性/方法，用于表明当前用户需是经过认证的用户。 @ RequiresGuest表明该用户需为”guest”用户 @ RequiresPermissions当前用户需拥有制定权限 @RequiresRoles当前用户需拥有制定角色 @RequiresUser当前用户需为已认证用户或已记住用户 ###过滤器ShiroFilterFactoryBean, filters, filterChainDefinition, FilterChainManager ##网络资源 跟着开源学shiro: 实例教程 Shiro官方: 官方教程， 比较权威 APIdocs 注： shiro的源码比较简洁，架构设计清晰，用到了很多设计模式，可以用来作为源码学习的材料","categories":[],"tags":[]},{"title":"mybatis migration","slug":"工具/mybatis migration","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/工具/mybatis migration/","link":"","permalink":"http://yoursite.com/2016/01/25/工具/mybatis migration/","excerpt":"","text":"mybatis migrationmybatis migration(MM)的目的也是对数据库升级进行管理， 它的命令主要包括： init: 初始化项目目录，运行完这条命令后，MM会在指定的目录下生成项目所需要的目录结构，包括 drivers: 用于存放数据库驱动 environments: 环境变量参数，默认有development.properties，代表开发环境，可以通过这里设置多个环境 scripts: 这里就是存储sql脚本的地方， 通过new命令会自动在这里生成以时间戳为前缀的脚本文件，每个文件分为两个区域，一个是升级脚本，一个是回退脚本. bootstrap: 初始化脚本，当需要对一个已经存在数据和结构的DB引入MM时，这里就存放着初始化的数据 new: 创建一个新的sql升级脚本. 它会自动在scripts目录下生成一个以时间戳为前缀的sql文件 status: 查看当前scripts目录下所有脚本的情况， 当脚本没有被执行时，状态栏会显示pending， 否则会显示执行的时间 up/down/version: 这三个命令分别代表升级、降级以及恢复到指定版本 up默认升级到最新的版本，但可以通过参数指定升级的版本个数 down默认只回退一个版本，但可以通过参数指定回退的版本个数 version后面跟具体的版本号, 表示需要回退到这个版本 pending: 在多人合作的场景中，可能会出现这样一个情况： A生成了sqlA脚本，B生成了sqlB脚本，sqlA的时间早于sqlB, 但提交时间晚于sqlB, 于是sqlB已经被执行了，但sqlA还处于pending状态，如下所示： 这时候有两种方案可以解决这个问题： 先降级，再升级： 这也是推荐的方式 直接使用pending： 这个命令是强制执行某个脚本，适合于这个升级脚本相对独立的情况 script: 这个命令有两个参数，一个是起始版本，一个是终止版本，输出一个delta脚本，这个脚本的作用是将数据库从起始版本变更到终止版本 如果起始版本低于终止版本，那么生成的delta脚本就是个升级脚本 如果起始版本高于终止版本，那么生成的delta脚本就是个降级脚本.","categories":[],"tags":[]},{"title":"junit","slug":"测试/junit","date":"2016-01-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2016/01/25/测试/junit/","link":"","permalink":"http://yoursite.com/2016/01/25/测试/junit/","excerpt":"","text":"JUnit Test(timeout, expected)： 表示该方法为单元测试方法，timeout表示执行的最长时间，expected表示预期方法会抛出的异常 Before: 每个测试方法执行前被执行一次 After: 每个测试方法执行后被执行一次 BeforeClass: 方法必须为static，在类初始化时被执行一次 AfterClass： 方法必须为static, 在所有测试方法执行完后被执行一次 Ignore： 表示忽略该测试方法 parameters: 用于参数化测试， 方法必须返回collection，作为构造函数的参数feeder，每套参数都会被所有的测试方法执行一次 参考资料 http://www.ibm.com/developerworks/cn/java/j-lo-junit4/","categories":[],"tags":[]},{"title":"SOAP","slug":"SOA/SOAP","date":"2015-11-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2015/11/25/SOA/SOAP/","link":"","permalink":"http://yoursite.com/2015/11/25/SOA/SOAP/","excerpt":"","text":"SOAPSOAP消息包括以下几个部分： Envelope元素： 标识此XML文档为SOAP消息， 必需 Heaher元素： 头部信息，可选 Body元素： 包含所有的请求和响应信息， 必需 Fault元素： 包含针对此消息出错时的错误信息，可选 例如： 12345678910111213&lt;?xml version=&quot;1.0&quot; ?&gt;&lt;soap:Envelope&gt; &lt;soap: Header /&gt; &lt;soap: Body&gt; &lt;soap:Fault&gt;&lt;/soap:Fault&gt; &lt;/soap:Body&gt;&lt;/soap:Envelope&gt; HTTP+XML=SOAP","categories":[],"tags":[]},{"title":"data_access","slug":"doc/data_access","date":"2015-10-25T02:20:54.000Z","updated":"2017-03-08T08:38:11.000Z","comments":true,"path":"2015/10/25/doc/data_access/","link":"","permalink":"http://yoursite.com/2015/10/25/doc/data_access/","excerpt":"","text":"#Data Access ##Transaction Management ###Concept diagram ###Propagation参考1 参考2 ###Isolation参考1 Read Uncommitted —&gt; Dirty Read —&gt; Read Committed —&gt; Unrepeatable Read —&gt; Repeatable Read —&gt; Phatom Read —&gt; Serialization Dirty Read: a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed. Unrepeatable Read: during the course of a transaction, a row is retrieved twice and the values within the row differ between reads. During the course of transaction 1, transaction 2 commits successfully, which means that its changes to the row with id 1 should become visible. However, Transaction 1 has already seen a different value for age in that row. Phatom Read: in the course of a transaction, two identical queries are executed, and the collection of rows returned by the second query is different from the first. The phantom reads anomaly is a special case of Non-repeatable reads when Transaction 1 repeats a ranged SELECT … WHERE query and, between both operations, Transaction 2 creates (i.e. INSERT) new rows (in the target table) which fulfill that WHERE clause. ###Transaction abstraction参考1 ###Core interfacePlatformTransactionManager, TransactionException, TransactionStatus, TransactionDefinition, TransactionTemplate ###Declarative transaction management ####XML-Based &lt;!-- other methods use the default transaction settings (see below) --&gt; &lt;tx:method name=&quot;*&quot;/&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;!-- ensure that the above transactional advice runs for any execution of an operation defined by the FooService interface --&gt; &lt;aop:config&gt; &lt;aop:pointcut id=&quot;fooServiceOperation&quot; expression=&quot;execution(* x.y.service.FooService.*(..))&quot;/&gt; &lt;aop:advisor advice-ref=&quot;txAdvice&quot; pointcut-ref=&quot;fooServiceOperation&quot;/&gt; &lt;/aop:config&gt; ####Annotation-Based@Transactional ####Note In proxy mode (which is the default), only external method calls coming in through the proxy are intercepted. This means that self-invocation, in effect, a method within the target object calling another method of the target object, will not lead to an actual transaction at runtime even if the invoked method is marked with @Transactional. Also, the proxy must be fully initialized to provide the expected behaviour so you should not rely on this feature in your initialization code, i.e. @PostConstruct. The @Transactional annotation is simply metadata that can be consumed by some runtime infrastructure that is @Transactional-aware and that can use the metadata to configure the appropriate beans with transactional behavior. In the preceding example, the element switches on the transactional behavior. When using proxies, you should apply the @Transactional annotation only to methods with public visibility. If you do annotate protected, private or package-visible methods with the @Transactional annotation, no error is raised, but the annotated method does not exhibit the configured transactional settings. @EnableTransactionManagement and only looks for @Transactional on beans in the same application context they are defined in. This means that, if you put annotation driven configuration in a WebApplicationContext for a DispatcherServlet, it only checks for @Transactional beans in your controllers, and not your services ##DAO SupportDaoImpl —&gt; JdbcTemplate —&gt; dataSource —&gt; execute SQLNamedParameterJdbcTemplate, MapSqlParameterSource","categories":[],"tags":[]}]}